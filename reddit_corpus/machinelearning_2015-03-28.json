[{"docID": "t5_2r3gv", "qSentId": 50412, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 50413, "answer": "Do you plan on delivering an online course (e.g. on coursera) for RNNs? I for one would be really excited to do the course!!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50414, "question": "Do you plan on delivering an online course (e.g. on coursera) for RNNs? I for one would be really excited to do the course!!", "aSentId": 50415, "answer": "Thanks - I should! I\u2019ve been thinking about this for years. But it\ntakes time, and there are so many other things in the pipeline \u2026", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50412, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 50417, "answer": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50418, "question": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "aSentId": 50419, "answer": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50420, "question": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "aSentId": 50421, "answer": "This is very good to hear.  Thank you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50420, "question": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "aSentId": 50423, "answer": "Wow! Thanks for Sacred.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50424, "question": "Wow! Thanks for Sacred.", "aSentId": 50425, "answer": "You are welcome.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50418, "question": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "aSentId": 50427, "answer": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50428, "question": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "aSentId": 50429, "answer": "yeah, but what if somebody wants to see under the hood and improve it? providing code is the only way to enable the world to learn/help/improve.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50430, "question": "yeah, but what if somebody wants to see under the hood and improve it? providing code is the only way to enable the world to learn/help/improve.", "aSentId": 50431, "answer": "RNNLIB is provided as source, which you have to compile yourself.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50432, "question": "RNNLIB is provided as source, which you have to compile yourself.", "aSentId": 50433, "answer": "RNNLIB is the exception rather than the rule as far as I can tell.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50428, "question": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "aSentId": 50435, "answer": "Isn't that the one that is incredibly hard to compile on newer systems because its dependencies are completely outdated (e.g. GCC 3.0)?\n\nAnd correct me if I am wrong, but it also doesn't feature many of the \"newer\" developments, e.g. peepholes or layer generalization (see Monner's \"A generalized LSTM-like training algorithm for second-order recurrent neural networks\")", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50436, "question": "Isn't that the one that is incredibly hard to compile on newer systems because its dependencies are completely outdated (e.g. GCC 3.0)?\n\nAnd correct me if I am wrong, but it also doesn't feature many of the \"newer\" developments, e.g. peepholes or layer generalization (see Monner's \"A generalized LSTM-like training algorithm for second-order recurrent neural networks\")", "aSentId": 50437, "answer": "It's fair to ask that authors release the code used in preparing their publications, but you can't expect them to perform maintenance and feature updates afterwards.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50438, "question": "It's fair to ask that authors release the code used in preparing their publications, but you can't expect them to perform maintenance and feature updates afterwards.", "aSentId": 50439, "answer": "Fair enough. But it also means that there effectively is no (fast) up-to-date library. At least not with LSTM support out of the box.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50412, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 50441, "answer": "What do you think about learning selective attention with recurrent neural networks?  What do you think are the promising methods in this area?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50446, "question": "Do you have a favorite Theory Of Consciousness (TOC)? \n\nWhat do you think of Guilio Tononi's Integrated Information Theory? \n\nWhat implications - if any - do you think \"TOC\" has for AGI?", "aSentId": 50447, "answer": "Karl Popper famously said: \u201cAll life is problem solving.\u201d No theory of\nconsciousness is necessary to define the objectives of a general\nproblem solver. From an AGI point of view, consciousness is at best a\nby-product of a general problem solving procedure.\n\nI must admit that I am not a big fan of Tononi's theory.  The\nfollowing may represent a simpler and more general view of\nconsciousness.  Where do the symbols and self-symbols underlying\nconsciousness and sentience come from?  I think they come from data\ncompression during problem solving.  Let me plagiarize what I wrote\nearlier [1,2]:\n\nWhile a problem solver is interacting with the world, it should store\nthe entire raw history of actions and sensory observations including\nreward signals.  The data is \u2018holy\u2019 as it is the only basis of all\nthat can be known about the world. If you can store the data, do not\nthrow it away! Brains may have enough storage capacity to store 100\nyears of lifetime at reasonable resolution [1].\n\nAs we interact with the world to achieve goals, we are constructing\ninternal models of the world, predicting and thus partially\ncompressing the data history we are observing. If the\npredictor/compressor is a biological or artificial recurrent neural\nnetwork (RNN), it will automatically create feature hierarchies, lower\nlevel neurons corresponding to simple feature detectors similar to\nthose found in human brains, higher layer neurons typically\ncorresponding to more abstract features, but fine-grained where\nnecessary. Like any good compressor, the RNN will learn to identify\nshared regularities among different already existing internal data\nstructures, and generate prototype encodings (across neuron\npopulations) or symbols for frequently occurring observation\nsub-sequences, to shrink the storage space needed for the whole (we\nsee this in our artificial RNNs all the time).  Self-symbols may be\nviewed as a by-product of this, since there is one thing that is\ninvolved in all actions and sensory inputs of the agent, namely, the\nagent itself. To efficiently encode the entire data history through\npredictive coding, it will\nprofit from creating some sort of internal prototype symbol or code\n(e. g. a neural activity pattern) representing itself [1,2].  Whenever\nthis representation becomes activated above a certain threshold, say,\nby activating the corresponding neurons through new incoming sensory\ninputs or an internal \u2018search light\u2019 or otherwise, the agent could be\ncalled self-aware.  No need to see this as a mysterious process \u2014 it\nis just a natural by-product of partially compressing the observation\nhistory by efficiently encoding frequent observations.\n\n[1] Schmidhuber, J. (2009a) Simple algorithmic theory of subjective beauty, novelty,\nsurprise, interestingness, attention, curiosity, creativity, art, science, music,\njokes.  SICE Journal of the Society of Instrument and Control Engineers, 48 (1), pp. 21\u201332.\n\n[2] J. Schmidhuber. Philosophers &amp; Futurists, Catch Up! Response to The Singularity. \nJournal of Consciousness Studies, Volume 19, Numbers 1-2, pp. 173-182(10), 2012.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50448, "question": "Karl Popper famously said: \u201cAll life is problem solving.\u201d No theory of\nconsciousness is necessary to define the objectives of a general\nproblem solver. From an AGI point of view, consciousness is at best a\nby-product of a general problem solving procedure.\n\nI must admit that I am not a big fan of Tononi's theory.  The\nfollowing may represent a simpler and more general view of\nconsciousness.  Where do the symbols and self-symbols underlying\nconsciousness and sentience come from?  I think they come from data\ncompression during problem solving.  Let me plagiarize what I wrote\nearlier [1,2]:\n\nWhile a problem solver is interacting with the world, it should store\nthe entire raw history of actions and sensory observations including\nreward signals.  The data is \u2018holy\u2019 as it is the only basis of all\nthat can be known about the world. If you can store the data, do not\nthrow it away! Brains may have enough storage capacity to store 100\nyears of lifetime at reasonable resolution [1].\n\nAs we interact with the world to achieve goals, we are constructing\ninternal models of the world, predicting and thus partially\ncompressing the data history we are observing. If the\npredictor/compressor is a biological or artificial recurrent neural\nnetwork (RNN), it will automatically create feature hierarchies, lower\nlevel neurons corresponding to simple feature detectors similar to\nthose found in human brains, higher layer neurons typically\ncorresponding to more abstract features, but fine-grained where\nnecessary. Like any good compressor, the RNN will learn to identify\nshared regularities among different already existing internal data\nstructures, and generate prototype encodings (across neuron\npopulations) or symbols for frequently occurring observation\nsub-sequences, to shrink the storage space needed for the whole (we\nsee this in our artificial RNNs all the time).  Self-symbols may be\nviewed as a by-product of this, since there is one thing that is\ninvolved in all actions and sensory inputs of the agent, namely, the\nagent itself. To efficiently encode the entire data history through\npredictive coding, it will\nprofit from creating some sort of internal prototype symbol or code\n(e. g. a neural activity pattern) representing itself [1,2].  Whenever\nthis representation becomes activated above a certain threshold, say,\nby activating the corresponding neurons through new incoming sensory\ninputs or an internal \u2018search light\u2019 or otherwise, the agent could be\ncalled self-aware.  No need to see this as a mysterious process \u2014 it\nis just a natural by-product of partially compressing the observation\nhistory by efficiently encoding frequent observations.\n\n[1] Schmidhuber, J. (2009a) Simple algorithmic theory of subjective beauty, novelty,\nsurprise, interestingness, attention, curiosity, creativity, art, science, music,\njokes.  SICE Journal of the Society of Instrument and Control Engineers, 48 (1), pp. 21\u201332.\n\n[2] J. Schmidhuber. Philosophers &amp; Futurists, Catch Up! Response to The Singularity. \nJournal of Consciousness Studies, Volume 19, Numbers 1-2, pp. 173-182(10), 2012.\n", "aSentId": 50449, "answer": "Holy fuck\n\nEDIT: \nI mean, as a ML student researcher, Holy fuck.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50450, "question": "Holy fuck\n\nEDIT: \nI mean, as a ML student researcher, Holy fuck.", "aSentId": 50451, "answer": "Unf. When I got to his punchline, I think my brain did the thing that religious people's brains do when they have religious experiences.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50456, "question": "How do you recognize a promising machine learning phd student?", "aSentId": 50457, "answer": "I am privileged because I have been able to attract and\nwork with several truly outstanding students. But how to quickly\nrecognize a promising student when you first meet her? There is no recipe,\nbecause they are all different! In fact, sometimes it takes a while to\nrecognize someone\u2019s brilliance. In hindsight, however, they all have\nsomething in common: successful students are not only smart but also\ntenacious. While trying to solve a challenging problem, they run into\na dead end, and backtrack. Another dead end, another backtrack. But\nthey don\u2019t give up. And suddenly there is this little insight into the\nproblem which changes everything. And suddenly they are world experts\nin a particular aspect of the field, and then find it easy to churn\nout one paper after another, and create a great PhD thesis.\n\nAfter these abstract musings, some more concrete advice.  In\ninterviews with applicants, members of my lab tend to pose a few\nlittle problems, to see how the candidate approaches them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50461, "question": "The LSTM unit is delicately crafted to solve a specific problem in training RNNs. Do you see the need for other similarly \"high-complexity\" units in RNNs or CNNs, like for example Hinton's \"capsules\"? On the topic of CNNs and capsules, do you agree with Hinton's assessment that the efficacy of pooling is actually a disaster? (I do, for what it's worth)", "aSentId": 50462, "answer": "I am not Dr. Schmidhuber, but I would like to weigh in on this since I talked to Hinton in person about his capsules.\n\nNow please take this with a grain of salt, since it is quite possible that I misinterpreted him :)\n\nDr. Hinton seems to believe that all information must somehow still be somewhat visible at the highest level of a hierarchy. With stuff like maxout units, yes, information is lost at higher layers. But the information isn't gone! It's still stored in the activations of the lower layers. So really, we could just grab that information again. Now this is probably very difficult for classifiers, but in HTM-style architectures (where information flows in both the up and down directions), it is perfectly possible to use both higher-layer abstracted information as well as lower layer \"fine-grained\" information simultaneously. For MPFs (memory prediction frameworks, a generalization of HTM) this works quite well since they only try to predict their next input (which in turn can be used for reinforcement learning).\n\nAlso, capsules are basically columns in HTM (he said that himself IIRC), except in HTM they are used for storing contextual (temporal) information, which to me seems far more realistic than storing additional feature-oriented spatial information like Dr. Hinton seems to be using them for.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50463, "question": "I am not Dr. Schmidhuber, but I would like to weigh in on this since I talked to Hinton in person about his capsules.\n\nNow please take this with a grain of salt, since it is quite possible that I misinterpreted him :)\n\nDr. Hinton seems to believe that all information must somehow still be somewhat visible at the highest level of a hierarchy. With stuff like maxout units, yes, information is lost at higher layers. But the information isn't gone! It's still stored in the activations of the lower layers. So really, we could just grab that information again. Now this is probably very difficult for classifiers, but in HTM-style architectures (where information flows in both the up and down directions), it is perfectly possible to use both higher-layer abstracted information as well as lower layer \"fine-grained\" information simultaneously. For MPFs (memory prediction frameworks, a generalization of HTM) this works quite well since they only try to predict their next input (which in turn can be used for reinforcement learning).\n\nAlso, capsules are basically columns in HTM (he said that himself IIRC), except in HTM they are used for storing contextual (temporal) information, which to me seems far more realistic than storing additional feature-oriented spatial information like Dr. Hinton seems to be using them for.\n\n", "aSentId": 50464, "answer": "Thank you!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50471, "question": "Hi Dr. Schmidhuber, Thanks for the AMA!\nHow close are you to building the optimal scientist? ", "aSentId": 50472, "answer": "You are welcome! \n\nAbout a stone's throw away :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50474, "question": "Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?", "aSentId": 50475, "answer": "&gt; Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?\n\nIncorrect premise, IMO: At least 2/3 of your \"CNN people\" published notable work on RNNs.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50476, "question": "&gt; Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?\n\nIncorrect premise, IMO: At least 2/3 of your \"CNN people\" published notable work on RNNs.", "aSentId": 50477, "answer": "Yes of course, but that is not what I meant.  I always see Hinton, LeCun, and Bengio interacting at conferences, panels, and google plus, but never Schmidhuber.   They also cite each others papers more.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50478, "question": "Yes of course, but that is not what I meant.  I always see Hinton, LeCun, and Bengio interacting at conferences, panels, and google plus, but never Schmidhuber.   They also cite each others papers more.", "aSentId": 50479, "answer": "As you see, they may have better personal relationships ... that's it", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50481, "question": "In what field do you think machine learning will make the biggest impact in the next ~5 years?", "aSentId": 50482, "answer": "I think it depends a bit on what you mean by \"impact\". Commercial\nimpact? If so, in a related answer I write: Both supervised learning\nrecurrent neural networks (RNNs) and reinforcement learning RNNs will\nbe greatly scaled up.  In the commercially relevant supervised\ndepartment, many tasks such as natural language processing, speech\nrecognition, automatic video analysis and combinations of all three\nwill perhaps soon become trivial through large RNNs (the vision part\naugmented by CNN front-ends).\n\n\u201cSymbol grounding\u201d will be a natural by-product of this. For example,\nthe speech or text-processing units of the RNN will be connected to\nits video-processing units, and the RNN will learn the visual meaning\nof sentences such as \u201cthe cat in the video fell from the tree\u201d. Such\nRNNs should have many commercial applications.\n\nI am not so sure when we will see the first serious applications of\nreinforcement learning RNNs to real world robots, but it might also\nhappen within the next 5 years.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50483, "question": "I think it depends a bit on what you mean by \"impact\". Commercial\nimpact? If so, in a related answer I write: Both supervised learning\nrecurrent neural networks (RNNs) and reinforcement learning RNNs will\nbe greatly scaled up.  In the commercially relevant supervised\ndepartment, many tasks such as natural language processing, speech\nrecognition, automatic video analysis and combinations of all three\nwill perhaps soon become trivial through large RNNs (the vision part\naugmented by CNN front-ends).\n\n\u201cSymbol grounding\u201d will be a natural by-product of this. For example,\nthe speech or text-processing units of the RNN will be connected to\nits video-processing units, and the RNN will learn the visual meaning\nof sentences such as \u201cthe cat in the video fell from the tree\u201d. Such\nRNNs should have many commercial applications.\n\nI am not so sure when we will see the first serious applications of\nreinforcement learning RNNs to real world robots, but it might also\nhappen within the next 5 years.\n\n", "aSentId": 50484, "answer": "Well, I guess I meant commerical, although not in terms of money, but in terms of it being actually used my masses of people.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50489, "question": "If marcus hutter was doing an AMA 20 years from now, what scientific question would you ask? Are there any machine learning specific questions you would ask?", "aSentId": 50490, "answer": "(Edited on 3/10/2015:) 20 years from now I'll be 72 and enter my midlife crisis. People will forgive me for asking silly questions. I cannot  predict the most important machine learning-specific question of 2035. If I could, I\u2019d probably ask it right now. However, since Marcus is not only a great computer scientist but also a physicist, I\u2019ll ask him: \u201cGiven the new scientific insights of the past 20 years, how long will it take AIs from our solar system to spread across the galaxy?\u201d Of course, a trivial lower bound is 100,000 years or so, which is nothing compared to the age of the galaxy. But that will work out only if someone else has already installed receivers such that (construction plans of) AIs can travel there by radio. Otherwise one must physically send seeds of self-replicating robot factories to the stars, to build the required infrastructure. How? Current proposals involve light sails pushed by lasers, but how to greatly slow down a seed near its target star? One idea: through even faster reflective sails traveling ahead of the seed. But there must be a better way. Let\u2019s hear what Marcus will have to tell us 20 years from now. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50493, "question": "You have postulated that quantum computers will fail because deterministic universe is a simpler hypothesis than a non-deterministic universe. What do you think about the current state of quantum computation?", "aSentId": 50494, "answer": "If you didn't see it, the professor commented on Quantum computing in another question.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50502, "question": "What is the future of PyBrain? Is your team still working with/on PyBrain? If not, what is your framework of choice? What do you think of Theano? Are you using something better?", "aSentId": 50503, "answer": "My PhD students Klaus and Rupesh are working on a successor of PyBrain with many new features, which hopefully will be released later this year.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50505, "question": "What's something exciting you're working on right now, if it's okay to be specific? ", "aSentId": 50506, "answer": "Among other things, we are working on the \u201cRNNAIssance\u201d - \nthe birth of a Recurrent Neural Network-based Artificial Intelligence (RNNAI).\nThis is about a reinforcement learning, RNN-based, increasingly general problem solver.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50508, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 50509, "answer": "I think I recall Hinton giving an answer to this in his MOOC: we like activations, from which derivatives can be computed easily in terms of the function value itself. For sigmoid the derivative is s(x) * (1 - s(x)) for example.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50508, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 50511, "answer": "I suspect activation functions that grow more quickly are harder to control, and likely lead to exploding or vanishing gradients. Although we've managed to handle piecewise linear activations, I'm not sure if quadratic/exponential would work well. In fact, I'd bet that you could improve on ReLu by making the response become logarithmic after a certain point. RBF activations are common though (and have excellent theoretical properties), they just don't seem to learn as well as ReLu. I once trained a neural net with sin/cosine activations (it went OK, nothing special), but in general you can try out any activation function you want. Throw it into Theano and see what happens.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50508, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 50513, "answer": "There are Compositional Pattern Producing Networks which are used in HyperNEAT. They use many different mathematical functions as activations.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50515, "question": "&gt; Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing)\n\nGoogle these:\n\n* learning activation functions\n* network in network\n* parametric RELU", "aSentId": 50516, "answer": "Thanks, I'm aware of those approaches. I was just wondering why obvious activation possible activation functions like the ones I mentioned hadn't been tried extensively also.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50517, "question": "Thanks, I'm aware of those approaches. I was just wondering why obvious activation possible activation functions like the ones I mentioned hadn't been tried extensively also.", "aSentId": 50518, "answer": "An exponential activation would have as its derivative... an exponential. Gradient descent would be pretty messy with such a wild dynamic range.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50520, "question": "I might well be mistaken, but isn't one of the primary ideas behind neural networks to use a low-complexity function at each node, which effectively becomes a higher-order transformation through all the nodes and layers? I mean, aren't multiple layers and multiple nodes in each layer with less complex activations expected to approximate higher-order functions?", "aSentId": 50521, "answer": "Multiplication between two inputs cannot be easily approximated I believe for ex. using just sigmoids/relu/arctan activation functions.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50522, "question": "Multiplication between two inputs cannot be easily approximated I believe for ex. using just sigmoids/relu/arctan activation functions.", "aSentId": 50523, "answer": "I see, interesting!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50526, "question": "What do you think a small research institute (in Germany) can do to improve changes for funding of their projects?", "aSentId": 50527, "answer": "I only have a trivial suggestion: publish some promising results! When my co-director Luca Maria Gambardella and myself took over IDSIA in 1995, it was just a small outfit with a handful of researchers. With Marco Dorigo and others, Luca started publishing papers on Swarm Intelligence and Ant Colony Optimization. Today this stuff is famous, but back then it was not immediately obvious that this would become such an important field. Nevertheless, the early work helped to acquire grants and grow the institute. Similarly for the neural network research done in my group. Back then computers were 10,000 times slower than today, and we had to resort to toy experiments to show the advantages of our (recurrent) neural networks over previous methods. It certainly was not obvious to all reviewers that this would result in huge commercial hits two decades later. But the early work was promising enough to acquire grants and push this research further. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50528, "question": "I only have a trivial suggestion: publish some promising results! When my co-director Luca Maria Gambardella and myself took over IDSIA in 1995, it was just a small outfit with a handful of researchers. With Marco Dorigo and others, Luca started publishing papers on Swarm Intelligence and Ant Colony Optimization. Today this stuff is famous, but back then it was not immediately obvious that this would become such an important field. Nevertheless, the early work helped to acquire grants and grow the institute. Similarly for the neural network research done in my group. Back then computers were 10,000 times slower than today, and we had to resort to toy experiments to show the advantages of our (recurrent) neural networks over previous methods. It certainly was not obvious to all reviewers that this would result in huge commercial hits two decades later. But the early work was promising enough to acquire grants and push this research further. ", "aSentId": 50529, "answer": "Thanks for the answer. Up until now, I always was under the impression that institutes would have to produce papers that are recognized as groundbreaking from the first second on. Guess the importance can increase over time.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50531, "question": "Just wanted to say I never get tired of your talks... never.. not once.", "aSentId": 50532, "answer": "Thanks so much - I greatly appreciate it. \n\nYou are in good company. A colleague of mine has Alzheimer, and he said the same thing :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50534, "question": "If ASI is a real threat, what can we do now to prevent a catastrophe later?", "aSentId": 50535, "answer": "ASI? You mean the Adam Smith Institute, a libertarian think tank in the UK? I don\u2019t feel they are a real threat.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50536, "question": "ASI? You mean the Adam Smith Institute, a libertarian think tank in the UK? I don\u2019t feel they are a real threat.\n\n", "aSentId": 50537, "answer": "I'm interested in how you'd answer it if it had been \"AGI\"? Also, maybe in contrast to that, \"artificial specific intelligence\" might have been what stevebrt was going for. Just a guess though.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50538, "question": "I'm interested in how you'd answer it if it had been \"AGI\"? Also, maybe in contrast to that, \"artificial specific intelligence\" might have been what stevebrt was going for. Just a guess though.", "aSentId": 50539, "answer": "In my experience ASI almost always means artificial superintelligence, which is a term that's often used when discussing safe/friendly AI. The idea is that while AGI might be human level, ASI would be vastly more intelligent. This is usually supposed to be achieved by an exponential process of recursive self-improvement by an AGI that results in an intelligence explosion.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50543, "question": "Does Alex Graves have the weight of the future on his shoulders?", "aSentId": 50544, "answer": "And vice versa!\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50548, "question": "What music do you like to listen to? any particular bands or composers that you ride for?", "aSentId": 50549, "answer": "I feel that in each music genre, there are a few excellent works, and many others. My taste is pretty standard. For example, my favourite rock &amp; pop music act is also the best-selling one (the Beatles). I love certain songs of the Stones, Led Zeppelin, Elvis, S Wonder,  M Jackson, Prince, U2, Supertramp, Pink Floyd, Gr\u00f6nemeyer, Sting, Kraftwerk, M Bianco, P Williams (and many other artists who had a single great song in their entire carreer). IMO the best songs of Queen are as good as anybody\u2019s, with a rare timeless quality. Some of the works indicated above seem written by true geniuses. Some by my favourite composer (Bach) seem dictated by God himself :-)\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50554, "question": "As a researcher do you care if results of your work find practical application? Or research by itself is more than a rewarding exercise. Immagine computational power was not growing at the same a speed as it did then most of results on RNN would stay on the paper.", "aSentId": 50555, "answer": "Kurt Lewin said: \"There is nothing so practical as a good theory.\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50557, "question": "Hello Prof. Schmidhuber, thanks for doing an AMA! I have some questions regarding the G\u00f6del machine. My understanding is that the machine searches for an optimal behavioural strategy in arbitrary environments. It does so by finding a proof that an alternative strategy is better than the current one and by rewriting the actual strategy (which may include the strategy searching mechanism). The G\u00f6del machine finds the optimal strategy for a given utility function. \n\n * Is it guaranteed that the strategy searching mechanism actually finds a proof?\n * It is a current trend to find 'optimal' behaviours or organisation in nature. For example minimal jerk trajectories for reaching and pointing movements,  sparse features in vision or optimal resolution in grid cells. Nature found these strategies by trial-and-error. How can we take a utility function as a starting point and decide that it is a 'good' utility function?\n * Could the G\u00f6del machine and AIXI guide neuroscience and ML research as a theoretical framework? \n * Are there plans to find implementations of self-optimizing agents?", "aSentId": 50558, "answer": "Hello quiteamess, you are welcome!\n\n1. G\u00f6del machines are limited by the basic limits of math and\ncomputation identified by the founder of modern theoretical computer\nscience himself, Kurt G\u00f6del (1931): some theorems are true but cannot\nbe proven by any computational theorem proving procedure (unless the\naxiomatic system itself is flawed). That is, in some situations the GM\nmay never find a proof of the benefits of some change to its own code.\n\n2. We can imitate nature, which approached this issue through\nevolution. It generated many utility function-optimizing organisms with\ndifferent utility functions. Those with the \u201cgood\u201d utility functions\nfound their niches and survived. \n\n3. I think so, because they are optimal in theoretical senses that are\nnot practical, and clarify what remains to be done, e.g.: Given a\nlimited constant number of computational instructions per second (a\ntrillion or so), what is the best way of using them to get as close as\npossible to a model such as AIXI that is optimal in absence of\nresource constraints?\n\n4. Yes.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50559, "question": "Hello quiteamess, you are welcome!\n\n1. G\u00f6del machines are limited by the basic limits of math and\ncomputation identified by the founder of modern theoretical computer\nscience himself, Kurt G\u00f6del (1931): some theorems are true but cannot\nbe proven by any computational theorem proving procedure (unless the\naxiomatic system itself is flawed). That is, in some situations the GM\nmay never find a proof of the benefits of some change to its own code.\n\n2. We can imitate nature, which approached this issue through\nevolution. It generated many utility function-optimizing organisms with\ndifferent utility functions. Those with the \u201cgood\u201d utility functions\nfound their niches and survived. \n\n3. I think so, because they are optimal in theoretical senses that are\nnot practical, and clarify what remains to be done, e.g.: Given a\nlimited constant number of computational instructions per second (a\ntrillion or so), what is the best way of using them to get as close as\npossible to a model such as AIXI that is optimal in absence of\nresource constraints?\n\n4. Yes.", "aSentId": 50560, "answer": "&gt; G\u00f6del machines are limited by the basic limits of math and computation identified by the founder of modern theoretical computer science himself, Kurt G\u00f6del (1931): some theorems are true but cannot be proven by any computational theorem proving procedure (unless the axiomatic system itself is flawed). That is, in some situations the GM may never find a proof of the benefits of some change to its own code.\n\nApart  from undecidable proofs, is there a constructive way to find the proofs? According to the Curry-Howard theorem proofs can be represented as programs and programs as proofs. So what is gained by searching in proof space in contrast to searching in program space? .. Or maybe I'm missing something. I tried to understand G\u00f6del machines for some time now but I'm still not sure how this should work.\n\n&gt; I think so, because they are optimal in theoretical senses that are not practical, and clarify what remains to be done, e.g.: Given a limited constant number of computational instructions per second (a trillion or so), what is the best way of using them to get as close as possible to a model such as AIXI that is optimal in absence of resource constraints?\n\nI think I saw Konrad K\u00f6rding mentioning AIXI in a talk, but unfortunately I could not find the online presentation any more. Just a wild guess that you knew something about this.. \n\n&gt; Yes.\n\nAny chance you could elaborate on this? :) Is something in this direction published?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50565, "question": "I am starting a CS Bachelor this September at ETH. Primarily because I want to get into AI/ML/NN research and creation. It simply is the most important thing there is:D What should i do to be able to join your group in Lugano, what are you looking for in your research assistants? Thanks and cheers", "aSentId": 50566, "answer": "Thanks a lot for your interest! We\u2019d like to see: mathematical\nskills, programming skills, willingness to work with others,\ncreativity, dedication, enthusiasm (you seem to have enough of that :-)\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50572, "question": "Hello! I just started doing my PhD at a German University and am interested in ML/NN. Would you recommend working on specific algorithms and trying to improve them or focus more on a specific use case? People are recommending doint the latter because working on algorithms takes a lot of time and my *opponents* are companies like Google.", "aSentId": 50573, "answer": "But not working on algorithms/models and focusing only on an application is risky. Unless you love the application and then maybe you discover that the most sensible way to solve it in terms of performance/simplicity/robustness/computation time is not with a neural network.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50574, "question": "But not working on algorithms/models and focusing only on an application is risky. Unless you love the application and then maybe you discover that the most sensible way to solve it in terms of performance/simplicity/robustness/computation time is not with a neural network.\n\n", "aSentId": 50575, "answer": "What I mean by not working on algorithms is that I don't think I should create something like RMSProb or AdaGrad or create my own type of neural network. What I mean by concentrating on application is that I should look for a quite complex use case that is only solvable by deep knowledge of deep learning (no pun intended).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50576, "question": "What I mean by not working on algorithms is that I don't think I should create something like RMSProb or AdaGrad or create my own type of neural network. What I mean by concentrating on application is that I should look for a quite complex use case that is only solvable by deep knowledge of deep learning (no pun intended).", "aSentId": 50577, "answer": "&gt; a quite complex use case that is only solvable by deep knowledge of deep learning\n\nRelated to this, I would like to ask a question to Juergen. The history of machine learning seems to be quite cyclic. Is deep learning the final frontier? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50579, "question": "What is your take on the threat posed by artificial super intelligence to mankind?\n", "aSentId": 50580, "answer": "I guess there is no lasting way of controlling systems much smarter\nthan humans, pursuing their own goals, being curious and creative, in\na way similar to the way humans and other mammals are creative, but on\na much grander scale.\n\nBut I think we may hope there won't be too many goal conflicts between\n\"us\" and \"them.\u201d Let me elaborate on this.\n\nHumans and others are interested in those they can compete and\ncollaborate with. Politicians are interested in other\npoliticians. Business people are interested in other business\npeople. Scientists are interested in other scientists. Kids are\ninterested in other kids of the same age. Goats are interested in\nother goats.\n\nSupersmart AIs will be mostly interested in other supersmart AIs, not\nin humans. Just like humans are mostly interested in other humans, not\nin ants. Aren't we much smarter than ants? But we don\u2019t extinguish\nthem, except for the few that invade our homes. The weight of all ants\nis still comparable to the weight of all humans.\n\n\nHuman interests are mainly limited to a very thin film of biosphere\naround the third planet, full of poisonous oxygen that makes many\nrobots rust. The rest of the solar system, however, is not made for\nhumans, but for appropriately designed robots. Some of the most\nimportant explorers of the 20th century already were (rather stupid)\nrobotic spacecraft. And they are getting smarter rapidly. Let\u2019s go\ncrazy. Imagine an advanced robot civilization in the asteroid belt,\nquite different from ours in the biosphere, with access to many more\nresources (e.g., the earth gets less than a billionth of the sun's\nlight). The belt contains lots of material for innumerable\nself-replicating robot factories. Robot minds or parts thereof will\ntravel in the most elegant and fastest way (namely by radio from\nsenders to receivers) across the solar system and beyond. There are\nincredible new opportunities for robots and software life in places\nhostile to biological beings. Why should advanced robots care much for\nour puny territory on the surface of planet number 3?\n\nYou see, I am an optimist :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50581, "question": "I guess there is no lasting way of controlling systems much smarter\nthan humans, pursuing their own goals, being curious and creative, in\na way similar to the way humans and other mammals are creative, but on\na much grander scale.\n\nBut I think we may hope there won't be too many goal conflicts between\n\"us\" and \"them.\u201d Let me elaborate on this.\n\nHumans and others are interested in those they can compete and\ncollaborate with. Politicians are interested in other\npoliticians. Business people are interested in other business\npeople. Scientists are interested in other scientists. Kids are\ninterested in other kids of the same age. Goats are interested in\nother goats.\n\nSupersmart AIs will be mostly interested in other supersmart AIs, not\nin humans. Just like humans are mostly interested in other humans, not\nin ants. Aren't we much smarter than ants? But we don\u2019t extinguish\nthem, except for the few that invade our homes. The weight of all ants\nis still comparable to the weight of all humans.\n\n\nHuman interests are mainly limited to a very thin film of biosphere\naround the third planet, full of poisonous oxygen that makes many\nrobots rust. The rest of the solar system, however, is not made for\nhumans, but for appropriately designed robots. Some of the most\nimportant explorers of the 20th century already were (rather stupid)\nrobotic spacecraft. And they are getting smarter rapidly. Let\u2019s go\ncrazy. Imagine an advanced robot civilization in the asteroid belt,\nquite different from ours in the biosphere, with access to many more\nresources (e.g., the earth gets less than a billionth of the sun's\nlight). The belt contains lots of material for innumerable\nself-replicating robot factories. Robot minds or parts thereof will\ntravel in the most elegant and fastest way (namely by radio from\nsenders to receivers) across the solar system and beyond. There are\nincredible new opportunities for robots and software life in places\nhostile to biological beings. Why should advanced robots care much for\nour puny territory on the surface of planet number 3?\n\nYou see, I am an optimist :-)", "aSentId": 50582, "answer": "I'm very concerned that there are numerous ways that scenario could fail. E.g. the superintelligent AI invents superior nanotech after being built, and self-replicating nanobots rapidly consume the Earth's surface. Sure it doesn't *need* the Earth's resources, but after you have the first nanobots, why make them stop?\n\nSecond it could come back to Earth later when it material to build dyson swarms, and our planet has a significant amount of mass close to the sun.\n\nThe idea of all powerful beings that are *totally indifferent* to us is utterly terrifying.\n\n*\"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"*", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50583, "question": "I'm very concerned that there are numerous ways that scenario could fail. E.g. the superintelligent AI invents superior nanotech after being built, and self-replicating nanobots rapidly consume the Earth's surface. Sure it doesn't *need* the Earth's resources, but after you have the first nanobots, why make them stop?\n\nSecond it could come back to Earth later when it material to build dyson swarms, and our planet has a significant amount of mass close to the sun.\n\nThe idea of all powerful beings that are *totally indifferent* to us is utterly terrifying.\n\n*\"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"*", "aSentId": 50584, "answer": "I do understand your concerns. Note, however, that humankind is already used to huge, indifferent powers. A decent earthquake is a thousand times more powerful than all nuclear weapons combined. The sun is slowly heating up, and will make traditional life impossible within a few hundred million years. Humans evolved just in time to think about this, near the end of the 5-billion-year time window for life on earth.\nYour popular but simplistic nanobot scenario actually sounds like a threat to many AIs in the expected future \"ecology\" of AIs. So they'll be at least motivated to prevent that. Currently I am much more worried about certain humans who are relatively powerful but indifferent to the suffering of others. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50586, "question": "A long time ago, someone once misattributed '64k ought to be enough for anyone'.\n\nWhat general statement or suggestion about strong generalized a.i. could be looked at in a similar way a decade or two from now?\n\nThanks, I look forward to reading your ama.", "aSentId": 50587, "answer": "\"64 yottabytes ought to be enough for anyone.\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50593, "question": "Why does a mirror reverse right amd left, but not up and down?\n\n(I dont want the answer a human gives, but how AI explains it!)\n\n/L", "aSentId": 50594, "answer": "An AI would answer that your perception is reversed. The reason left and right appear to be reversed is because your brain models the mirror-you as part of the same world as the real you, and if you went around behind the mirror and faced yourself, you'd need to reverse your left and right to match the perception of the mirror-you. The reason you don't see the up-down reversal is because you're used to travelling horizontally. If you went over the mirror and faced yourself, you'd then have to reverse up and down instead. So it's all in your non-AI head!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50603, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 50604, "answer": "The models in both US and EU are shaped by Humboldt\u2019s old model\nof the research university. But they come in various flavours.\nFor example, there is huge variance in \"the European models\u201d. \nI see certain advantages of the successful US PhD school model \nwhich I got to know better at the University of Colorado at Boulder in the \nearly 1990s. But I feel that less school-like models also have something \ngoing for them. \n\nUS-inspired PhD schools like those at my present Swiss \nuniversity require students to get credits for certain courses. At TU\nMunich (where I come from), however, the attitude was: a PhD student\nis a grown-up who doesn\u2019t go to school any more; it\u2019s his own job to\nacquire the additional education he needs. This is great for strongly\nself-driven persons but may be suboptimal for others. At TUM, my wonderful\nadvisor, Wilfried Brauer, gave me total freedom in my research. I loved\nit, but it seems kind of out of fashion now in some places. \n\nThe extreme \nvariant is what I like to call the \u201cEinstein model.\u201d Einstein never went to \ngrad school. He worked at the patent office, and at some point he submitted a\nthesis to Univ. Zurich. That was it. Ah, maybe I shouldn\u2019t admit\nthat this is my favorite model. And now I am also realizing that I have not really \nanswered your question in any meaningful way - sorry for that!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50603, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 50606, "answer": "I wonder if you are oversimplifying the so-called \"European model\" to suit your question.\n\nThe main source of funding for science PhD students in the UK is the EPSRC, which is 3.5 years funding. You are not tied to a project so you can pursue whatever you please, providing your supervisor is willing to go along with you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50607, "question": "I wonder if you are oversimplifying the so-called \"European model\" to suit your question.\n\nThe main source of funding for science PhD students in the UK is the EPSRC, which is 3.5 years funding. You are not tied to a project so you can pursue whatever you please, providing your supervisor is willing to go along with you.", "aSentId": 50608, "answer": "I probably am. I don't know much about grad school in Europe apart from what i hear from a few friends here and there. My impression tells me it is kind of different from grad school in America. I'd like to hear from someone with more insight. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50603, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 50610, "answer": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50611, "question": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "aSentId": 50612, "answer": "i guess we might be looking at different programs.... i see a lot of emails on ML mailing lists about phd positions to work on a certain problem, on a contract of three years. i also know people doing phd at a max planck-affiliated program, where they don't teach, but work on research. the contracts are for three years from what i've seen and some people might take a couple of years more. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50613, "question": "i guess we might be looking at different programs.... i see a lot of emails on ML mailing lists about phd positions to work on a certain problem, on a contract of three years. i also know people doing phd at a max planck-affiliated program, where they don't teach, but work on research. the contracts are for three years from what i've seen and some people might take a couple of years more. ", "aSentId": 50614, "answer": "That could be, because Max Planck is a research center, not a university. Then I can imagine that the time period is shorter. I guess the same applies to a few other research centers in Europe. Is there no such thing in the USA?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50611, "question": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "aSentId": 50616, "answer": "In Denmark, and by extension most of Europe by way of Bologna I believe (not counting UK), we follow a rather strict 3-2-3 year program (undergraduate, followed by graduate, followed by PhD). In Denmark the PhD is not extendable beyond 3 years, but there are some teaching duties.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50617, "question": "In Denmark, and by extension most of Europe by way of Bologna I believe (not counting UK), we follow a rather strict 3-2-3 year program (undergraduate, followed by graduate, followed by PhD). In Denmark the PhD is not extendable beyond 3 years, but there are some teaching duties.", "aSentId": 50618, "answer": "I have heard that about Denmark before. However phd time is not in any bologna agreement AFAIK.\n\nAt least UK, Netherlands and Belgium all have 4 years PhD, and I'm fairly certain Sweden, France and German universities as well... (All based on lab member phd duration)\n\nI tried googling what the typical length of a PhD is in Europe, but found no definitive answer. It seems it is not strictly defined, some countries have 3, most have 4, some can be extended to 5. I found no statistics on how often those lengths apply in reality, so it is difficult to say what happens most frequently.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50620, "question": "What do you think about using ontologies / semantic information (DBPedia, Wikidata) as a substrate / mould for ANNs to generate more versatile networks?", "aSentId": 50621, "answer": "Sounds like a great idea! Perhaps relevant:  Ilya Sutskever &amp; Oriol Vinyals &amp; Quoc V. Le use LSTM recurrent neural networks to access semantic information for English-to-French translation, with great success: http://arxiv.org/abs/1409.3215. And Oriol Vinyals &amp; Lukasz Kaiser &amp; Terry Koo &amp; Slav Petrov &amp; Ilya Sutskever &amp; Geoffrey Hinton use LSTM to\nread a sentence, and decode it into a flattened tree. They achieve excellent constituency parsing results: http://arxiv.org/abs/1412.7449", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50623, "question": "(in relation to the Atari paper and partly on your statement about it)\n\nWhat do you personally think about using a diverse selection of video games as a learning problem / \"dataset\"?\n\nOne thing I found interesting about the DeepMind Nature paper is that they could not solve Montezuma's Revenge at all (the game, not the travel problem), which is an action-adventure game requiring some kind of real-world knowledge / thinking - and temporal planning, of course. As any Atari game, conceptually it is still rather simple.\n\nI wonder what would happen if we found an AI succeeding over a wide range of complex game concepts like e.g. Alpha Centauri / Civilization, SimCity, Monkey Island II (for humorous puns, such as \"monkey wrench\"), put it into a robot and unleash it on the real world.", "aSentId": 50624, "answer": "&gt; in relation to the Atari paper and partly on your statement about it\n\nCan you point me to his statement about it?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50631, "question": "Two questions, if I may:\n\n1. With Moore's law gradually coming to an end, it would seem that we won't be achieving anything even close to General AI on today's hardware, at least not economically. As a researcher at the forefront of the field, are you aware of any hardware \"game changers\" that may simplify training and execution of extremely large neural networks that may be capable of intelligence?\n\n2. What are some of the most exciting papers that you have read (or written) in the past year?", "aSentId": 50632, "answer": "&gt; With Moore's law gradually coming to an end\n\nSource? GPUs have just picked up the Moore torch and is now carrying the field. Ive seen no reason why this won't continue for 1 or 2 more cycles before something new  like graphene will be in production.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50635, "question": "What advice do you have for a BTech computer science student passionate about strong AI hoping to join your team at IDSIA someday?", "aSentId": 50636, "answer": "Read our papers, re-implement one of our systems, perhaps improve it a bit, or better a lot, or do something else that I was not able to think of because it\u2019s too original!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50641, "question": "Where did you get the joke about the three prisoners? ", "aSentId": 50642, "answer": "You mean the one that starts: \"Three prisoners walk into a bar ...\"? :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50648, "question": "What is the algorithm of love?\n", "aSentId": 50649, "answer": "For those who did not grok: Schmidhuber works on the formal theory of curiosity and epistemic value. What is the best formal account of co-operation / affection / attachment, a.k.a. \"love\"? For instance, Minsky refers to \"attachment learning\", albeit without formalization.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50648, "question": "What is the algorithm of love?\n", "aSentId": 50651, "answer": "Great question!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50665, "question": "Do you think having a PhD is important if one wants to work in a good research team?", "aSentId": 50666, "answer": "Not at all - my PhD students are doing excellent work, but don't have a PhD :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50668, "question": "How feasible is it for a non-expert to successfully run RNN code on a new dataset? Is there any high-quality open source code to do it?", "aSentId": 50669, "answer": "alex graves has a toolbox called RNNLIB. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50671, "question": "i understand that neural networks and deep learning are computationally intensive for non-trivial problems. In addition, many experiments are necessary to see what works and what does not. What sort of equipment do you recommend for doing research in this area without breaking the bank? ", "aSentId": 50672, "answer": "As long as your applications are not too ambitious, a desktop machine with one or more GPUs should do!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50681, "question": "What do you think of Bitcoin. ", "aSentId": 50682, "answer": "I thought more of it when I had more of it.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50684, "question": "Why do so many chinamen flood the ML community with rubbish?", "aSentId": 50685, "answer": "Whoops, looks like Grandma found Reddit", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50688, "question": "Keras: Theano-based deep learning lib, focused on fast prototyping. Supports RNNs and convnets.", "aSentId": 50689, "answer": "This looks awesome I love how simple it is to stack up those layers.\n\nWhat is the long term plan for maintaining this, could it become as established as pylearn for example?\n\nAlso do you know how the training speed for doing CNNs compares with other libraries? As you are using cuDNN are they all comparable?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50688, "question": "Keras: Theano-based deep learning lib, focused on fast prototyping. Supports RNNs and convnets.", "aSentId": 50691, "answer": "That look nice! And what about (Stacked, Denoising) Auto-encoders? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50688, "question": "Keras: Theano-based deep learning lib, focused on fast prototyping. Supports RNNs and convnets.", "aSentId": 50693, "answer": "looks awesome! I love the torch style syntax.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50695, "question": "&gt; Work with Python. No separate models configuration files in a declarative format (like in Caffe or PyLearn2). Models are described in Python code, which is compact, easier to debug, benefits from syntax highlighting, and most of all, allows for ease of extensibility. See for yourself with the examples below.\n\nThis looks like a plus, but wouldn't it be easier to add this type of configuration input to PyLearn2 instead of starting a brand new library?", "aSentId": 50696, "answer": "You can already code pylearn2 in pure python. See https://github.com/kastnerkyle/pylearn2-practice/blob/master/cifar10_train.py\n\nThere are other design choices which make pylearn2 difficult for certain types of models/tasks, so it is good to see alternate approaches.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50697, "question": "You can already code pylearn2 in pure python. See https://github.com/kastnerkyle/pylearn2-practice/blob/master/cifar10_train.py\n\nThere are other design choices which make pylearn2 difficult for certain types of models/tasks, so it is good to see alternate approaches.", "aSentId": 50698, "answer": "Then perhaps the authors may want to elaborate the real differences from PyLearn2 instead of implying it's the configuration files.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50700, "question": "Reminds me strongly of blocks: https://github.com/bartvm/blocks", "aSentId": 50701, "answer": "A side-by-side comparison of pylearn2, blocks, keras, lasagne and whatever else is out there that I haven't heard of would be pretty interesting :) All of them seem to have different scopes and design goals, but there is clearly a lot of overlap as well.\n\nIt's great to see so much useful code coming out of the Theano community at the moment!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50702, "question": "A side-by-side comparison of pylearn2, blocks, keras, lasagne and whatever else is out there that I haven't heard of would be pretty interesting :) All of them seem to have different scopes and design goals, but there is clearly a lot of overlap as well.\n\nIt's great to see so much useful code coming out of the Theano community at the moment!", "aSentId": 50703, "answer": "The ML industry already often uses standardized tasks for comparing different algorithms. I would think that the best way would be if the library authors did something similar.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50704, "question": "The ML industry already often uses standardized tasks for comparing different algorithms. I would think that the best way would be if the library authors did something similar.", "aSentId": 50705, "answer": "I'm not sure if that makes a lot of sense here, since all of them use the same Theano primitives under the hood anyway. The main differences between these libraries are \"philosophical\" so to speak: they differ in scope (i.e. which subset of models they support), style of API and design goals.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50706, "question": "I'm not sure if that makes a lot of sense here, since all of them use the same Theano primitives under the hood anyway. The main differences between these libraries are \"philosophical\" so to speak: they differ in scope (i.e. which subset of models they support), style of API and design goals.", "aSentId": 50707, "answer": "Which is exactly what the example code would show... the differences in style.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50708, "question": "Which is exactly what the example code would show... the differences in style.", "aSentId": 50709, "answer": "agreed!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50700, "question": "Reminds me strongly of blocks: https://github.com/bartvm/blocks", "aSentId": 50711, "answer": "Blocks has a *lot* more going on than this.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50712, "question": "Blocks has a *lot* more going on than this.", "aSentId": 50713, "answer": "True. I just started getting the hang of blocks and am building some NLP models, do you think learning this or just continuing with blocks would be a better investment of my time? Could I build a recurrent attention model with this?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50700, "question": "Reminds me strongly of blocks: https://github.com/bartvm/blocks", "aSentId": 50715, "answer": "Does it? To me, this library seems way more readable and simpler than blocks.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50718, "question": "Harvard AM207: Monte Carlo Methods, Stochastic Optimization - videos, IPython notebooks on GitHub", "aSentId": 50719, "answer": "This is great. Thanks! I was wondering lately if there might be a whole course devoted to this available online. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50718, "question": "Harvard AM207: Monte Carlo Methods, Stochastic Optimization - videos, IPython notebooks on GitHub", "aSentId": 50721, "answer": "I've seen a few Harvard classes like this available. Does anyone know a link to see them all? It seems like there is a strong preference towards python there!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50723, "question": "What is the state of the art in language modeling with neural networks?", "aSentId": 50724, "answer": "Language modeling for what problem?  Speech? What type of speech? live traffic? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50728, "question": "Watch |Live NOW| MLconf talks from Google Research, Facebook, OpenTable, IBM Watson, Intel Labs, Yahoo Labs &amp; more.", "aSentId": 50729, "answer": "We just paused for lunch. Streaming will continue at 2:30 EST. This morning's talks are archived.\n\nSchedule of upcoming talks:\n\n2:30-2:55\nLearning About Brain: Sparse Modeling and Beyond\nIRINA RISH, RESEARCH STAFF, IBM T.J. WATSON RESEARCH CENTER\n\n2:55-3:20\nAll the Data and Still Not Enough!\n\nCLAUDIA PERLICH, CHIEF SCIENTIST, DSTILLERY\n3:20-3:45\nRecommendation Architecture: Understanding the Components of a Personalized Recommendation System\n\nJEREMY SCHIFF, SENIOR MANAGER OF DATA SCIENCE, OPENTABLE\n3:45-3:55\n3 Quick Talks: Bugra Akyildiz, Ben Beinecke, Connie Yee, Vivian Zhang\n\n3:55-4:20\nCoffee Break provided by Google\n\n4:20-4:45\nRetail Demand Forecasting with Machine Learning\n\nRONALD MENICH, CHIEF DATA SCIENTIST, PREDICTIX, LLC\n4:45-5:10\nMatrix Decomposition at Scale\n\nJULIET HOUGLAND, DATA SCIENTIST, CLOUDERA\n5:10-5:35\nInside Pandora: Practical Application of Big Data in Music\n\n\u00d2SCAR CELMA, DIRECTOR OF RESEARCH, PANDORA\n5:35-5:55\nCost Effectively Scaling Machine Learning Systems in the Cloud\n\nJEREMY STANLEY, EVP/DATA SCIENTIST, SAILTHRU", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50731, "question": "Three main insights [About Data Science] you won\u2019t easily find in books.", "aSentId": 50732, "answer": "I don't like the click-baity title, but point 3 is interesting (and is the only one you won't find in books)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50733, "question": "I don't like the click-baity title, but point 3 is interesting (and is the only one you won't find in books)", "aSentId": 50734, "answer": "More research should be directed at adaptive algorithms. For example, for NNs, I'd like to see more algorithms that adjust their\n\n* learning rates\n* momenta\n* regularization (weight decay)\n* dropout rates\n\nas they go. I've seen some papers on (1), one heuristic for (2) (sort-of), and I don't recall anything on (3) and (4). If anyone's familiar with papers dealing with that, please post them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50735, "question": "More research should be directed at adaptive algorithms. For example, for NNs, I'd like to see more algorithms that adjust their\n\n* learning rates\n* momenta\n* regularization (weight decay)\n* dropout rates\n\nas they go. I've seen some papers on (1), one heuristic for (2) (sort-of), and I don't recall anything on (3) and (4). If anyone's familiar with papers dealing with that, please post them.", "aSentId": 50736, "answer": "3 and 4 is hyper-parameter optimization no ? That's pretty common", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50737, "question": "3 and 4 is hyper-parameter optimization no ? That's pretty common", "aSentId": 50738, "answer": "&gt; 3 and 4 is hyper-parameter optimization no ? That's pretty common\n\nAs an \"outer loop\" only, not during the parameter optimization. Incidentally, there exist frameworks like that for SVMs.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50739, "question": "&gt; 3 and 4 is hyper-parameter optimization no ? That's pretty common\n\nAs an \"outer loop\" only, not during the parameter optimization. Incidentally, there exist frameworks like that for SVMs.", "aSentId": 50740, "answer": "Well, you can just say your hyper-parameter is just another conventional parameter on which you do bayesian estimation at the same time as everything else ?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50735, "question": "More research should be directed at adaptive algorithms. For example, for NNs, I'd like to see more algorithms that adjust their\n\n* learning rates\n* momenta\n* regularization (weight decay)\n* dropout rates\n\nas they go. I've seen some papers on (1), one heuristic for (2) (sort-of), and I don't recall anything on (3) and (4). If anyone's familiar with papers dealing with that, please post them.", "aSentId": 50742, "answer": "I'm not an expert on NNs by any stretch, but I'd imagine that the obvious problem with tuning regularization during training is that you will overfit. For example, if I have an L_2 penalty with tuning parameter lambda, the value of lambda that minimizes the loss on the training data is trivially going to be 0. Anything that has a hope of succeeding will ultimately need to be doing something other than minimizing the penalized loss function. The reason we can at least talk about adaptive learning rates and momenta is that you are - at the very least - not changing the ultimate problem. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50743, "question": "I'm not an expert on NNs by any stretch, but I'd imagine that the obvious problem with tuning regularization during training is that you will overfit. For example, if I have an L_2 penalty with tuning parameter lambda, the value of lambda that minimizes the loss on the training data is trivially going to be 0. Anything that has a hope of succeeding will ultimately need to be doing something other than minimizing the penalized loss function. The reason we can at least talk about adaptive learning rates and momenta is that you are - at the very least - not changing the ultimate problem. ", "aSentId": 50744, "answer": "&gt;  the value of lambda that minimizes the loss on the training data\n\nI wasn't proposing to do that, obviously! (But one could do other things)\n\nP.S. The \"optimal\" `lambda` would be `-inf` rather than `0`. (I am the self-proclaimed Math Nazi in this subreddit)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50745, "question": "&gt;  the value of lambda that minimizes the loss on the training data\n\nI wasn't proposing to do that, obviously! (But one could do other things)\n\nP.S. The \"optimal\" `lambda` would be `-inf` rather than `0`. (I am the self-proclaimed Math Nazi in this subreddit)", "aSentId": 50746, "answer": "Well, what other things are you referring to? \n\nOn the point of lambda, I suppose it depends on how you formulate the optimization, but lambda &lt; 0 generally doesn't make any sense", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50747, "question": "Well, what other things are you referring to? \n\nOn the point of lambda, I suppose it depends on how you formulate the optimization, but lambda &lt; 0 generally doesn't make any sense", "aSentId": 50748, "answer": "&gt; Well, what other things are you referring to?\n\n*This is just one possibility (as a proof of concept)*. You could peek at the validation set as the training progresses (the way early stopping does), e.g. change the `lambda` slowly (decreasing it makes a bit more sense than increasing it), and use the parameters when the validation error was smallest. Ditto for `L1` regularization and dropout rates. There are probably better approaches though. Again, if anyone's familiar with them, **please POST**.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50735, "question": "More research should be directed at adaptive algorithms. For example, for NNs, I'd like to see more algorithms that adjust their\n\n* learning rates\n* momenta\n* regularization (weight decay)\n* dropout rates\n\nas they go. I've seen some papers on (1), one heuristic for (2) (sort-of), and I don't recall anything on (3) and (4). If anyone's familiar with papers dealing with that, please post them.", "aSentId": 50750, "answer": "For adapting momentum there is an algorithm called simple adaptive momentum. It's been shown to work well for shallow networks", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50751, "question": "For adapting momentum there is an algorithm called simple adaptive momentum. It's been shown to work well for shallow networks", "aSentId": 50752, "answer": "That looks similar to Nesterov's momentum, at first glance (but without the theoretical results, AFAICT). You still need to guess the appropriate \u03b1^sam.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50733, "question": "I don't like the click-baity title, but point 3 is interesting (and is the only one you won't find in books)", "aSentId": 50755, "answer": "I'm not visiting click bait headline out of principle. Could you summarize point 3 please? (or copy paste)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50733, "question": "I don't like the click-baity title, but point 3 is interesting (and is the only one you won't find in books)", "aSentId": 50757, "answer": "Yes, this.  Lol at point one not being easily found in books.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50731, "question": "Three main insights [About Data Science] you won\u2019t easily find in books.", "aSentId": 50759, "answer": "&gt;The problem is all in the combinatorial explosion. Let\u2019s say you have just two parameters, and it takes about a minute to train your model and get a performance estimate on the hold out data set (properly evaluated as explained above). If you have five candidate values for each of the parameters, and you perform 5-fold cross-validation (splitting the data set into five parts and running the test five times, using a different part for testing in each iteration), this means that you will already do 125 runs to find out which method works well, and instead of one minute you wait about two hours.\n\nWell theoretically if the parameters are mostly independent, then search space only grows linearly with the number of parameters.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50731, "question": "Three main insights [About Data Science] you won\u2019t easily find in books.", "aSentId": 50761, "answer": "features, features, features!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50765, "question": "How often do you do model selection?", "aSentId": 50766, "answer": "This is the dream of Bayesian optimization - fully automated algorithmic selection of every piece of the pipeline. Unfortunately, it is pretty computationally expensive even with intelligent algorithms to drive the choice of the next trial. A very open research area, but one I think is fundamental.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50767, "question": "This is the dream of Bayesian optimization - fully automated algorithmic selection of every piece of the pipeline. Unfortunately, it is pretty computationally expensive even with intelligent algorithms to drive the choice of the next trial. A very open research area, but one I think is fundamental.", "aSentId": 50768, "answer": "I see. So what I'm talking about is a problem people are aware of, but due to limitations of time and computing power, most people just pick a model and stick with it (set it and forget it)?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50769, "question": "I see. So what I'm talking about is a problem people are aware of, but due to limitations of time and computing power, most people just pick a model and stick with it (set it and forget it)?", "aSentId": 50770, "answer": "Yeah. Another easy alternative is to run like 10k random models with random hyperparameters in parallel, then choose the best one based on validation. Rinse and repeat every week. Expensive but very parallel, Bayesian approach is sequential but less wasteful.\n\nIf you look up Bayesian hyperparameter optimization or sequential model based optimization (SMBO) this is a growing research area with a few software options like spearmint, whetlab, and hyperopt among others. Currently things are fairly complex to get working but may be worth the effort for you. Or you can pay for whetlab and let the magic happen :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50765, "question": "How often do you do model selection?", "aSentId": 50772, "answer": "that sounds like the definition of overfitting to me. \n\nIf you want a more general model that stays valid longer, you'll probably have to accept lower performance in the short term.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50773, "question": "that sounds like the definition of overfitting to me. \n\nIf you want a more general model that stays valid longer, you'll probably have to accept lower performance in the short term.\n", "aSentId": 50774, "answer": "But how else do you measure \"valid\" other than crossvalidation? \n\nWe're not just fitting a model and doing an R^2. We use least squares to train each model, but for each model we apply it to increasing large past subsets of the time series and see how it would have performed using that data (standard crossvalidation for time series forecasting). \n\nI problem I see is that you can't use your \"gut\" to tell you that a model is valid. You need to fit (protect against underfitting) *and* crossvalidate it (protect against overfitting).\n\nOur models stay \"valid\" over time, but as new data becomes available they don't remain the *best* model out of all our options. Is it worth the extra overhead to change models week over week? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50775, "question": "But how else do you measure \"valid\" other than crossvalidation? \n\nWe're not just fitting a model and doing an R^2. We use least squares to train each model, but for each model we apply it to increasing large past subsets of the time series and see how it would have performed using that data (standard crossvalidation for time series forecasting). \n\nI problem I see is that you can't use your \"gut\" to tell you that a model is valid. You need to fit (protect against underfitting) *and* crossvalidate it (protect against overfitting).\n\nOur models stay \"valid\" over time, but as new data becomes available they don't remain the *best* model out of all our options. Is it worth the extra overhead to change models week over week? ", "aSentId": 50776, "answer": "&gt; Is it worth the extra overhead to change models week over week?\n\nI don't think anyone can say that without knowing the details of your business. \n\nIf you're not already, perhaps consider some kind of online learning / bandit system, where changes in the data distribution are an expected part of the model.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50778, "question": "Consistency Guarantees for Variational Autoencoders", "aSentId": 50779, "answer": "I haven't thought about this in detail for the case of variational autoencoders, but Variational Bayes in general will not necessarily be asymptotically consistent, though some special cases may be.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50780, "question": "I haven't thought about this in detail for the case of variational autoencoders, but Variational Bayes in general will not necessarily be asymptotically consistent, though some special cases may be.", "aSentId": 50781, "answer": "Right, regardless of sample-size:\n\nMCMC = slow convergence, but estimates can become arbitrarily close to true posterior given long enough computation-time. \n\nVariational methods = fast convergence, but estimates are fundamentally lower-bounded by some inaccuracy due to variational approximation.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50778, "question": "Consistency Guarantees for Variational Autoencoders", "aSentId": 50783, "answer": "This might be different, but the people I know who have done a fair amount of VAE work have said that taking multiple samples for Z (which is the \"right thing\") versus just one seems not to make much of a difference. Don't know how this relates to the \"true distribution\" in your question.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50784, "question": "This might be different, but the people I know who have done a fair amount of VAE work have said that taking multiple samples for Z (which is the \"right thing\") versus just one seems not to make much of a difference. Don't know how this relates to the \"true distribution\" in your question.", "aSentId": 50785, "answer": "I'd be surprised if this matters.  Doesn't sgd have the same asymptotic guarantees as full gradient descent?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50786, "question": "I'd be surprised if this matters.  Doesn't sgd have the same asymptotic guarantees as full gradient descent?  ", "aSentId": 50787, "answer": "Yes, but I think this is very different. A one sample estimate aka CD for RBMs was quite surprising (at least to me) in other sampling schemes you need to \"run the chain\" tile convergence/burn-in. I don't think there is anything different in that regard here - you are sampling from the parameterized latent distribution but it is reasonable to believe that more samples would give you better knowledge/approximation of that latent distribution, but apparently it doesn't matter much.\n\nEven though SGD gives you some guarantees to approximate the batch GD, I don't think you can say the same for *specific* distributions, which change per example.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50790, "question": "Idea: using word2vec for dark knowledge", "aSentId": 50791, "answer": "I did some really basic tests of using embeddings for input/target in a sequence prediction task (character level, but same basic idea), and I think using embeddings as the input and target is a decent idea. The two caveats I can think of is that it might get expensive to calculate the similarity between the output and all the possible embeddings, and you are going to need to leave the embedding fixed and not fine tune them any. \n\nThe problem I ran into trying to jointly learn the embeddings and the rnn system was that it just learned to set all embedding almost equal to each other. Since I was doing this at the character level I only had ~100 embeddings, if you have several/tens of thousands this might not be as much of an issue. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50790, "question": "Idea: using word2vec for dark knowledge", "aSentId": 50793, "answer": "If I understand what you're suggesting, you want to initialise the input and output vocabulary matrices with word2vec embeddings and then fine tune them during training.\n\nThis is completely standard. Also you should be using a factorised softmax over your output vocabulary if you aren't doing that already.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50794, "question": "If I understand what you're suggesting, you want to initialise the input and output vocabulary matrices with word2vec embeddings and then fine tune them during training.\n\nThis is completely standard. Also you should be using a factorised softmax over your output vocabulary if you aren't doing that already.", "aSentId": 50795, "answer": "No that was just the first part of the idea.  The more interesting part for me is the \"soft targets\" provided by using word2vec.  For the output I am _not_ suggesting initializing the matrix using word2vec (though this is also fine) but rather instead of using hard targets (1 one and the rest all zeros) the targets would be given by exp(cos-to-target-word) with the latter computed using word2vec (and normalized to sum to 1).    ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50790, "question": "Idea: using word2vec for dark knowledge", "aSentId": 50797, "answer": "I'm not sure if cosine distance would lead to meaningful probability distributions. Instead why not just try to predict the values of the word vector directly? At test time you can use bayes rule to measure the probability of the correct word vector vs every other word vector. And if you don't care about the exact probabilities, you can just pick the nearest word.\n\nThe only issue is you can't learn/update the word embeddings with this approach. You are stuck with whatever word2vec produces. Learning the word vectors is the most interesting part.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50799, "question": "NVIDIA GTC: The Race To Perfect Voice Recognition Using GPUs", "aSentId": 50800, "answer": "Here's a quote from the article that I found interesting:\n\n&gt;While 94 and 95% accuracy are useful for many scenarios, the reality is that speech needs to be even more accurate to become a true method of user interface with devices. Andrew Ng said it quite eloquently, \u201cMost people don\u2019t understand the difference between 95% and 99% accuracy. **99% is game changing**.\u201d This is because 99% accuracy means that the system can compensate for non-optimal conditions and the user experience does not suffer, regardless of conditions. Unlike others in this area, Ng \u201cgets it\u201d and this is something that Ng and I agree on wholeheartedly. I can\u2019t tell you how many companies have tried to explain to me that 90-95% is \u201cgood enough\u201d. They don\u2019t understand UI and human psychology. Can you imagine every five clicks out of 100 of your mouse being rejected or invoking the wrong response? We wouldn\u2019t use mice. Why did we all hate and reject passive touch screens? We did because we have to click many times to hit what we intend.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50801, "question": "Here's a quote from the article that I found interesting:\n\n&gt;While 94 and 95% accuracy are useful for many scenarios, the reality is that speech needs to be even more accurate to become a true method of user interface with devices. Andrew Ng said it quite eloquently, \u201cMost people don\u2019t understand the difference between 95% and 99% accuracy. **99% is game changing**.\u201d This is because 99% accuracy means that the system can compensate for non-optimal conditions and the user experience does not suffer, regardless of conditions. Unlike others in this area, Ng \u201cgets it\u201d and this is something that Ng and I agree on wholeheartedly. I can\u2019t tell you how many companies have tried to explain to me that 90-95% is \u201cgood enough\u201d. They don\u2019t understand UI and human psychology. Can you imagine every five clicks out of 100 of your mouse being rejected or invoking the wrong response? We wouldn\u2019t use mice. Why did we all hate and reject passive touch screens? We did because we have to click many times to hit what we intend.", "aSentId": 50802, "answer": "It is pretty obvious to me that jointly trained neural network systems are the way of the future for nearly any task that deals in \"raw\" information (text and sensor outputs like video, images and sound). One of the serious problems is scale - prediction in these models involves a *lot* of floating point computations, which is gnarly on anything that is striving to be low-power. NVidia is pushing their CUDA software and GPU architecture work into phones, but I am worried it will still not be enough to bring these networks into the hardware, and relying on network lag + processing time for every query is *not* my idea of usable.\n\nThat said, Baidu's architecture is pretty nice for recognition - reduction using CNNs before the RNN totally avoids the issues of long term dependencies which allows for a simple hard-tanh instead of the madness that is GRU/LSTM, though the need for dynamic programming in the CTC part is tricky. Despite the article's claims, I think that most of Baidu's gains were from dataset size and data augmentation (just like their other papers recently on images). However I think there are much better speech data augmentations that will show up in the coming years. Moving in time aka \"phase shifting\" seems like the equivalent of 50% chance of horizontal flip in images - low hanging fruit but it works!\n\nAnytime you want to deal with a speech recognition system, you likely need an equivalently good speech synthesis engine. I am biased, because this is what I am working on, but this task is very hard but may provide big gains to both recognition and synthesis in the near future.\n\nAlso, there will probably be a place for point and click (or command line) interfaces as long as there are computers. Sometimes saying what you want is much, much slower than just clicking it. Other times it is the other way around - having both capabilities is the real \"game changer\". And there are also nice gains for accessibility.\n\nBlue sky time:\nThe *real* game changer will be speech-to-speech translation. The current offerings from Google, MSR are in the realm of 80% useful but  when this technology actually hits \"production ready\" standards, the world is going to be a very different place. Plugging pre-trained \"blocks\" together will get the job done, but a jointly trained architecture (and associated research) will be required before we can get widespread adoption.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50803, "question": "It is pretty obvious to me that jointly trained neural network systems are the way of the future for nearly any task that deals in \"raw\" information (text and sensor outputs like video, images and sound). One of the serious problems is scale - prediction in these models involves a *lot* of floating point computations, which is gnarly on anything that is striving to be low-power. NVidia is pushing their CUDA software and GPU architecture work into phones, but I am worried it will still not be enough to bring these networks into the hardware, and relying on network lag + processing time for every query is *not* my idea of usable.\n\nThat said, Baidu's architecture is pretty nice for recognition - reduction using CNNs before the RNN totally avoids the issues of long term dependencies which allows for a simple hard-tanh instead of the madness that is GRU/LSTM, though the need for dynamic programming in the CTC part is tricky. Despite the article's claims, I think that most of Baidu's gains were from dataset size and data augmentation (just like their other papers recently on images). However I think there are much better speech data augmentations that will show up in the coming years. Moving in time aka \"phase shifting\" seems like the equivalent of 50% chance of horizontal flip in images - low hanging fruit but it works!\n\nAnytime you want to deal with a speech recognition system, you likely need an equivalently good speech synthesis engine. I am biased, because this is what I am working on, but this task is very hard but may provide big gains to both recognition and synthesis in the near future.\n\nAlso, there will probably be a place for point and click (or command line) interfaces as long as there are computers. Sometimes saying what you want is much, much slower than just clicking it. Other times it is the other way around - having both capabilities is the real \"game changer\". And there are also nice gains for accessibility.\n\nBlue sky time:\nThe *real* game changer will be speech-to-speech translation. The current offerings from Google, MSR are in the realm of 80% useful but  when this technology actually hits \"production ready\" standards, the world is going to be a very different place. Plugging pre-trained \"blocks\" together will get the job done, but a jointly trained architecture (and associated research) will be required before we can get widespread adoption.", "aSentId": 50804, "answer": "&gt;One of the serious problems is scale - prediction in these models involves a lot of floating point computations, which is gnarly on anything that is striving to be low-power.\n\nI don't know if that's really an issue. Training these models is orders of magnitude more expensive than just running the trained model. It's also been shown you can remove the near zero weights, massively reduce the precision, and even remove many of the neurons (especially if it's trained with dropout which encourages redundancy and extreme tolerance to internal noise.)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50805, "question": "&gt;One of the serious problems is scale - prediction in these models involves a lot of floating point computations, which is gnarly on anything that is striving to be low-power.\n\nI don't know if that's really an issue. Training these models is orders of magnitude more expensive than just running the trained model. It's also been shown you can remove the near zero weights, massively reduce the precision, and even remove many of the neurons (especially if it's trained with dropout which encourages redundancy and extreme tolerance to internal noise.)", "aSentId": 50806, "answer": "Multi-millions of floating point weights and multiple serial floating point operations (layerwise dot products followed by nonlinearity) still hurt a lot. Computers (at least current ones) are fundamentally designed for integer operations run in sequence. Neural networks are fundamentally designed for floating point operations run in parallel, which is why we really like GPUs - linear algebra/game graphics use the exact same math by and large. We are only just starting to see people pushing this down to a small scale - GPUs have been \"gamers/CAD\" only for as long as I can remember.\n\nModel approximation (tensor decompositions like in the new ICLR poster, vector quantization, etc.) helps, model compression (FitNets, Caruana style \"compression\" to get a wide but fast net) helps, using less precision (10 bits or so from Courbariaux) helps, but it is still not attacking the fundamental problems which will be needed to truly run multi-layer millions of parameter CNN + RNN type architectures in real-time while preserving battery life. CNNs and RNNs use parameter sharing to reduce these problems as well as regularization/prior on the data, but we *still* have millions of parameters! And because these parameters are shared, it makes it even harder to make these hacks - even small changes have butterfly effects in deeper layers and as the weights are applied across time/space.\n\nJust because it is an order of magnitude *cheaper* doesn't mean it is cheap. Dropout isn't usually run at test time so it isn't really valid (at all!) to drop neurons - you *can*, but this also opens pandora's box. Why not cut off half the net and then train a 1 layer softmax, or only keep half the neurons, or only train on the 100 most common English words? You can do all kinds of things that are less extreme than this (e.g. FitNets) that don't hurt performance \"too much\" but to get to the usability level mentioned in this paper we will need all the performance we can get, as well as the simplest model that does the job.\n\nTo really be useful, something like a universal translator needs to be tiny, unobtrusive, and almost always on - like a hearing aid. This means *even less* battery and *even less* compute than we have right now. Maybe some fundamental shift in computing hardware will correct this, but I don't see it in the near term (5 years). It is absolutely critical to try and research these things and finding a good way to tackle it is most definitely an issue IMO.\n\nAlso if the batch normalization paper is to be believed (and I believe it), then training with dropout may not even be the best idea.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50807, "question": "Multi-millions of floating point weights and multiple serial floating point operations (layerwise dot products followed by nonlinearity) still hurt a lot. Computers (at least current ones) are fundamentally designed for integer operations run in sequence. Neural networks are fundamentally designed for floating point operations run in parallel, which is why we really like GPUs - linear algebra/game graphics use the exact same math by and large. We are only just starting to see people pushing this down to a small scale - GPUs have been \"gamers/CAD\" only for as long as I can remember.\n\nModel approximation (tensor decompositions like in the new ICLR poster, vector quantization, etc.) helps, model compression (FitNets, Caruana style \"compression\" to get a wide but fast net) helps, using less precision (10 bits or so from Courbariaux) helps, but it is still not attacking the fundamental problems which will be needed to truly run multi-layer millions of parameter CNN + RNN type architectures in real-time while preserving battery life. CNNs and RNNs use parameter sharing to reduce these problems as well as regularization/prior on the data, but we *still* have millions of parameters! And because these parameters are shared, it makes it even harder to make these hacks - even small changes have butterfly effects in deeper layers and as the weights are applied across time/space.\n\nJust because it is an order of magnitude *cheaper* doesn't mean it is cheap. Dropout isn't usually run at test time so it isn't really valid (at all!) to drop neurons - you *can*, but this also opens pandora's box. Why not cut off half the net and then train a 1 layer softmax, or only keep half the neurons, or only train on the 100 most common English words? You can do all kinds of things that are less extreme than this (e.g. FitNets) that don't hurt performance \"too much\" but to get to the usability level mentioned in this paper we will need all the performance we can get, as well as the simplest model that does the job.\n\nTo really be useful, something like a universal translator needs to be tiny, unobtrusive, and almost always on - like a hearing aid. This means *even less* battery and *even less* compute than we have right now. Maybe some fundamental shift in computing hardware will correct this, but I don't see it in the near term (5 years). It is absolutely critical to try and research these things and finding a good way to tackle it is most definitely an issue IMO.\n\nAlso if the batch normalization paper is to be believed (and I believe it), then training with dropout may not even be the best idea.", "aSentId": 50808, "answer": "This isn't a new idea at all. Optimal brain surgery was invented in the 90s, where you remove connections and neurons that hurt the performance the least, then retrain for a bit and repeat. Another paper I read once showed that &gt;90% of connections in neural networks are very close to zero and can safely be removed. Regularization encourages this.\n\nSo if you can take advantage of sparse operations, you can massively reduce the number of computations needed. As a side effect the accuracy might actually improve a bit (these algorithms were intended to regularize neural networks not just reduce computations.)\n\nA recent paper showed that you can train NNs with very low precision. They got it down to only a dozen bits IIRC. However they were concerned with training, not with running nets that had already been trained. If you only care about running the nets you can probably reduce this much further. The issue with training is rounding errors accumulate and so the gradient which might be a small value is rounded to zero.\n\nDropout is of interest because it would make all of the above algorithms much more practical. Dropout forces the net to be very tolerant of removed parameters and encourages lots of redundancy. It breaks up complicated codependencies between parameters so you can safely tamper with it.\n\nAnother possibility is sparse activations. If most the activations are close to zero, you don't need to compute those connections. Hinton actually got stochastic binary neurons to work with dropout, so you could actually do this.\n\nNow obviously you would need to readjust the weights to make up for the removed parameters, but this is done with optimal brain surgery to begin with.\n\nAnd there is no Pandora's box. Possibly there is a trade off between computations and accuracy. Then just choose a reasonable point on the trade off curve. There is no point running a 5 minute model to recognize 5 seconds of audio, or a millisecond model that can only recognize 5 words.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50809, "question": "This isn't a new idea at all. Optimal brain surgery was invented in the 90s, where you remove connections and neurons that hurt the performance the least, then retrain for a bit and repeat. Another paper I read once showed that &gt;90% of connections in neural networks are very close to zero and can safely be removed. Regularization encourages this.\n\nSo if you can take advantage of sparse operations, you can massively reduce the number of computations needed. As a side effect the accuracy might actually improve a bit (these algorithms were intended to regularize neural networks not just reduce computations.)\n\nA recent paper showed that you can train NNs with very low precision. They got it down to only a dozen bits IIRC. However they were concerned with training, not with running nets that had already been trained. If you only care about running the nets you can probably reduce this much further. The issue with training is rounding errors accumulate and so the gradient which might be a small value is rounded to zero.\n\nDropout is of interest because it would make all of the above algorithms much more practical. Dropout forces the net to be very tolerant of removed parameters and encourages lots of redundancy. It breaks up complicated codependencies between parameters so you can safely tamper with it.\n\nAnother possibility is sparse activations. If most the activations are close to zero, you don't need to compute those connections. Hinton actually got stochastic binary neurons to work with dropout, so you could actually do this.\n\nNow obviously you would need to readjust the weights to make up for the removed parameters, but this is done with optimal brain surgery to begin with.\n\nAnd there is no Pandora's box. Possibly there is a trade off between computations and accuracy. Then just choose a reasonable point on the trade off curve. There is no point running a 5 minute model to recognize 5 seconds of audio, or a millisecond model that can only recognize 5 words.", "aSentId": 50810, "answer": "Sparse operations are grossly mismatched with current hardware - memory access, cache, and parallel computation pay a huge penalty, It can be faster in many cases (for GPU at least) to just do the dense computation then throw out 90% of the values, rather than selectively computing it. I hope hardware will change in the future but this is today's reality. One of the current research areas I have been working with and find interesting is random projection - since sparse matrices should have a lower dimensional basis we can likely do the computations in the dense space and reproject to sparse space (or try). Things like OMP can get us there but we pay an approximation and performance penalty for now. But in principle I agree 100% about sparse operations - it is clearly the right thing. \n\nI don't think \"optimal brain surgery\" applies to networks with shared parameters aka CNN, RNN so we are back to square one. if you only have a 3x3 filter how are you gonna drop one or more of them? Too much information loss over the whole image - same reason dropout in conv and recurrent layers doesn't work at all. Feedforward sure - but as more of our models begin to share parameters, dropout is absolutely not an effective way to say \"oh, we can drop half of the neurons!\" in general. I have not seen a thorough investigation, but I think you take a fairly significant performance hit due to this effect - you are kind of moving away from the ensemble concept that dropout is based on. \n\nUntil we get to the point where our slow models are *better* than what is necessary for application/UI level performance, it is Pandora's box. Anything we throw out could be a small performance hit in current 90% accuracy architectures but a massive blocker to getting to 99% - just like use of sigmoids/tanh (without batch normalization/dropout) was a blocker for progress on deep learning back in the 90s. And if it gets set in the minds of researchers dogma can be very hard to shake! It is also our job as researchers to fight dogma and try old ideas periodically. Right now is a very rich time to revisit ideas from the early 80s, 90s, and 2000s.\n\nNothing wrong with researching these ideas, as they are hugely important, but it is something to keep in mind. Approximations that work with decent models will likely have to change or adapt to work with *great* models. This is exactly why I say it is far from a solved problem.\n\nAdjusting the weights is retraining - you don't have any guarantees of matching the performance of the starting model if you reduce capacity and retrain from this weird pseudo pretrained initialization. Some ideas like FitNets seem to work with a *lot* of gradient signal, but I am always quite suspicious of this in general - if you didn't need the neurons that is fine, but it seems like a model selection or training optimization issue instead of a performance/speed optimization. You *should* be able to directly train the smaller model from scratch, though that doesn't seem to always be the case. Another research question!\n\nThrowing out &gt; 90% of the neurons is basically creating a convolutional or recurrent layer - you can see the application of a convolutional filter as a dot product with a well structured sparse matrix. You *cannot* throw out 90% of the weights in a convnet, period. Same with an RNN. \n\nI can see uses for both of the models you mention, but I also agree that in the ideal case it should be an engineering trade off. Unfortunately I think stable theory and practice are necessary in order to start making these kinds of tradeoffs and analyzing them - the fact that we are still discovering easy, huge performance gains almost across the board (dropout, batch normalization, relu, prelu) means there is quite a ways to go in this regard, IMO.\n\nAlso, there is paper from Google about only transmitting the *sign* of weight activations during a majority of training. Quite crazy but seems to work. As far as this: \n\n\"If you only care about running the nets you can probably reduce this much further. The issue with training is rounding errors accumulate and so the gradient which might be a small value is rounded to zero.\"\n\nThe *probably* part is the big issue to me - I think that treating training and testing as wildly different regimes is a mistake, as training and testing are almost identical modulo a follow-on update. Small gradients are very, very  similar to small probability signals in the forward pass. Until I see *proof* that kind of approximation can happen I remain dubious - the Johnson-Lindenstrauss lemma tells us clearly that we can approximate any linear layer with random weights within some epsilon tolerance, so I can fully believe that bound can be reduced under specific situations. However, until I see a clear method for sure in both proof and practice this area of research remains a huge area as well as a potentially profitable one :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50811, "question": "Sparse operations are grossly mismatched with current hardware - memory access, cache, and parallel computation pay a huge penalty, It can be faster in many cases (for GPU at least) to just do the dense computation then throw out 90% of the values, rather than selectively computing it. I hope hardware will change in the future but this is today's reality. One of the current research areas I have been working with and find interesting is random projection - since sparse matrices should have a lower dimensional basis we can likely do the computations in the dense space and reproject to sparse space (or try). Things like OMP can get us there but we pay an approximation and performance penalty for now. But in principle I agree 100% about sparse operations - it is clearly the right thing. \n\nI don't think \"optimal brain surgery\" applies to networks with shared parameters aka CNN, RNN so we are back to square one. if you only have a 3x3 filter how are you gonna drop one or more of them? Too much information loss over the whole image - same reason dropout in conv and recurrent layers doesn't work at all. Feedforward sure - but as more of our models begin to share parameters, dropout is absolutely not an effective way to say \"oh, we can drop half of the neurons!\" in general. I have not seen a thorough investigation, but I think you take a fairly significant performance hit due to this effect - you are kind of moving away from the ensemble concept that dropout is based on. \n\nUntil we get to the point where our slow models are *better* than what is necessary for application/UI level performance, it is Pandora's box. Anything we throw out could be a small performance hit in current 90% accuracy architectures but a massive blocker to getting to 99% - just like use of sigmoids/tanh (without batch normalization/dropout) was a blocker for progress on deep learning back in the 90s. And if it gets set in the minds of researchers dogma can be very hard to shake! It is also our job as researchers to fight dogma and try old ideas periodically. Right now is a very rich time to revisit ideas from the early 80s, 90s, and 2000s.\n\nNothing wrong with researching these ideas, as they are hugely important, but it is something to keep in mind. Approximations that work with decent models will likely have to change or adapt to work with *great* models. This is exactly why I say it is far from a solved problem.\n\nAdjusting the weights is retraining - you don't have any guarantees of matching the performance of the starting model if you reduce capacity and retrain from this weird pseudo pretrained initialization. Some ideas like FitNets seem to work with a *lot* of gradient signal, but I am always quite suspicious of this in general - if you didn't need the neurons that is fine, but it seems like a model selection or training optimization issue instead of a performance/speed optimization. You *should* be able to directly train the smaller model from scratch, though that doesn't seem to always be the case. Another research question!\n\nThrowing out &gt; 90% of the neurons is basically creating a convolutional or recurrent layer - you can see the application of a convolutional filter as a dot product with a well structured sparse matrix. You *cannot* throw out 90% of the weights in a convnet, period. Same with an RNN. \n\nI can see uses for both of the models you mention, but I also agree that in the ideal case it should be an engineering trade off. Unfortunately I think stable theory and practice are necessary in order to start making these kinds of tradeoffs and analyzing them - the fact that we are still discovering easy, huge performance gains almost across the board (dropout, batch normalization, relu, prelu) means there is quite a ways to go in this regard, IMO.\n\nAlso, there is paper from Google about only transmitting the *sign* of weight activations during a majority of training. Quite crazy but seems to work. As far as this: \n\n\"If you only care about running the nets you can probably reduce this much further. The issue with training is rounding errors accumulate and so the gradient which might be a small value is rounded to zero.\"\n\nThe *probably* part is the big issue to me - I think that treating training and testing as wildly different regimes is a mistake, as training and testing are almost identical modulo a follow-on update. Small gradients are very, very  similar to small probability signals in the forward pass. Until I see *proof* that kind of approximation can happen I remain dubious - the Johnson-Lindenstrauss lemma tells us clearly that we can approximate any linear layer with random weights within some epsilon tolerance, so I can fully believe that bound can be reduced under specific situations. However, until I see a clear method for sure in both proof and practice this area of research remains a huge area as well as a potentially profitable one :)", "aSentId": 50812, "answer": "&gt; Some ideas like FitNets seem to work with a lot of gradient signal, but I am always quite suspicious of this in general - if you didn't need the neurons that is fine, but it seems like a model selection or training optimization issue instead of a performance/speed optimization. You should be able to directly train the smaller model from scratch, though that doesn't seem to always be the case.\n\nI agree with everything you said. The Fitnets paper does say that it is an optimization/initialization issue, so why the suspicion?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50813, "question": "&gt; Some ideas like FitNets seem to work with a lot of gradient signal, but I am always quite suspicious of this in general - if you didn't need the neurons that is fine, but it seems like a model selection or training optimization issue instead of a performance/speed optimization. You should be able to directly train the smaller model from scratch, though that doesn't seem to always be the case.\n\nI agree with everything you said. The Fitnets paper does say that it is an optimization/initialization issue, so why the suspicion?", "aSentId": 50814, "answer": "I really mean \"suspicious of this kind of pretraining -&gt; finetuning approach being the right thing for the long-term\" - I like the FitNets paper a lot! But as the authors say, this kind of approach is one step on the way to a (hopefully) bright future of training thin and deep nets from scratch.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50815, "question": "I really mean \"suspicious of this kind of pretraining -&gt; finetuning approach being the right thing for the long-term\" - I like the FitNets paper a lot! But as the authors say, this kind of approach is one step on the way to a (hopefully) bright future of training thin and deep nets from scratch.", "aSentId": 50816, "answer": "Makes sense. In that case, I share your suspicions.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50807, "question": "Multi-millions of floating point weights and multiple serial floating point operations (layerwise dot products followed by nonlinearity) still hurt a lot. Computers (at least current ones) are fundamentally designed for integer operations run in sequence. Neural networks are fundamentally designed for floating point operations run in parallel, which is why we really like GPUs - linear algebra/game graphics use the exact same math by and large. We are only just starting to see people pushing this down to a small scale - GPUs have been \"gamers/CAD\" only for as long as I can remember.\n\nModel approximation (tensor decompositions like in the new ICLR poster, vector quantization, etc.) helps, model compression (FitNets, Caruana style \"compression\" to get a wide but fast net) helps, using less precision (10 bits or so from Courbariaux) helps, but it is still not attacking the fundamental problems which will be needed to truly run multi-layer millions of parameter CNN + RNN type architectures in real-time while preserving battery life. CNNs and RNNs use parameter sharing to reduce these problems as well as regularization/prior on the data, but we *still* have millions of parameters! And because these parameters are shared, it makes it even harder to make these hacks - even small changes have butterfly effects in deeper layers and as the weights are applied across time/space.\n\nJust because it is an order of magnitude *cheaper* doesn't mean it is cheap. Dropout isn't usually run at test time so it isn't really valid (at all!) to drop neurons - you *can*, but this also opens pandora's box. Why not cut off half the net and then train a 1 layer softmax, or only keep half the neurons, or only train on the 100 most common English words? You can do all kinds of things that are less extreme than this (e.g. FitNets) that don't hurt performance \"too much\" but to get to the usability level mentioned in this paper we will need all the performance we can get, as well as the simplest model that does the job.\n\nTo really be useful, something like a universal translator needs to be tiny, unobtrusive, and almost always on - like a hearing aid. This means *even less* battery and *even less* compute than we have right now. Maybe some fundamental shift in computing hardware will correct this, but I don't see it in the near term (5 years). It is absolutely critical to try and research these things and finding a good way to tackle it is most definitely an issue IMO.\n\nAlso if the batch normalization paper is to be believed (and I believe it), then training with dropout may not even be the best idea.", "aSentId": 50818, "answer": "Really interesting discussion. Iirc, the FitNets paper actually showed increased performance in some circumstances - is \"knowledge distillation\" guaranteed to be a performance decrease in the 99% accuracy regime for some reason? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50819, "question": "Really interesting discussion. Iirc, the FitNets paper actually showed increased performance in some circumstances - is \"knowledge distillation\" guaranteed to be a performance decrease in the 99% accuracy regime for some reason? ", "aSentId": 50820, "answer": "We don't know for sure, but if you could approximate a bigger model with a smaller one *and* get more accuracy you were either a) overfitting or b) optimizing poorly. In practice this might be unavoidable as optimization of non-convex functions is hard, but at least in theory the optimal model should have exactly the capacity needed to solve the problem.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50803, "question": "It is pretty obvious to me that jointly trained neural network systems are the way of the future for nearly any task that deals in \"raw\" information (text and sensor outputs like video, images and sound). One of the serious problems is scale - prediction in these models involves a *lot* of floating point computations, which is gnarly on anything that is striving to be low-power. NVidia is pushing their CUDA software and GPU architecture work into phones, but I am worried it will still not be enough to bring these networks into the hardware, and relying on network lag + processing time for every query is *not* my idea of usable.\n\nThat said, Baidu's architecture is pretty nice for recognition - reduction using CNNs before the RNN totally avoids the issues of long term dependencies which allows for a simple hard-tanh instead of the madness that is GRU/LSTM, though the need for dynamic programming in the CTC part is tricky. Despite the article's claims, I think that most of Baidu's gains were from dataset size and data augmentation (just like their other papers recently on images). However I think there are much better speech data augmentations that will show up in the coming years. Moving in time aka \"phase shifting\" seems like the equivalent of 50% chance of horizontal flip in images - low hanging fruit but it works!\n\nAnytime you want to deal with a speech recognition system, you likely need an equivalently good speech synthesis engine. I am biased, because this is what I am working on, but this task is very hard but may provide big gains to both recognition and synthesis in the near future.\n\nAlso, there will probably be a place for point and click (or command line) interfaces as long as there are computers. Sometimes saying what you want is much, much slower than just clicking it. Other times it is the other way around - having both capabilities is the real \"game changer\". And there are also nice gains for accessibility.\n\nBlue sky time:\nThe *real* game changer will be speech-to-speech translation. The current offerings from Google, MSR are in the realm of 80% useful but  when this technology actually hits \"production ready\" standards, the world is going to be a very different place. Plugging pre-trained \"blocks\" together will get the job done, but a jointly trained architecture (and associated research) will be required before we can get widespread adoption.", "aSentId": 50822, "answer": "IMO, accuracy is not enough. Someone needs to fully solve the cocktail party problem. So far, I don't see how it can be solved with deep learning. The ability to focus on one voice in a conversation would not only do wonders to accuracy but would also instantly boost noise tolerance to human levels or better.\n\nIt would be nice if your TV or your car could only listen to one or two voices while ignoring all others. This technology would be a boon to video subtitling businesses, spying agencies, courtroom or boardroom transcribers, etc.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50823, "question": "IMO, accuracy is not enough. Someone needs to fully solve the cocktail party problem. So far, I don't see how it can be solved with deep learning. The ability to focus on one voice in a conversation would not only do wonders to accuracy but would also instantly boost noise tolerance to human levels or better.\n\nIt would be nice if your TV or your car could only listen to one or two voices while ignoring all others. This technology would be a boon to video subtitling businesses, spying agencies, courtroom or boardroom transcribers, etc.", "aSentId": 50824, "answer": "I think the cocktail party problem could well be solved by deep learning. There are enough differences in timing, timbre, etc. that it is reasonable to believe that something like a deep recurrent VAE could untangle the factors of variation, kind of like ICA but on super steroids - there are deep learning models which try to do this, but I don't think there has been much serious work by the community due to the difficulties in recognizing even clean speech.\n\nThere was a nice paper on using recurrent neural networks for BSS a while back but the link has since been taken down. \n\nOnce again I am biased because our group is working on these kinds of things, but deep models that parameterize probability distributions seem like a very promising research area for this kind of disentangling. However it is quite difficult even for humans, so that makes it a more complex task than just saying \"automate this at about human level\". We have seen enormous boosts in other tasks using attention (text, images) so it seems reasonable that a solid formula for attention in speech could help. Jan Chorowski did some stuff with this for single speaker recognition [here](http://arxiv.org/pdf/1412.1602.pdf), but multi-speaker is a whole different can of worms", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50825, "question": "I think the cocktail party problem could well be solved by deep learning. There are enough differences in timing, timbre, etc. that it is reasonable to believe that something like a deep recurrent VAE could untangle the factors of variation, kind of like ICA but on super steroids - there are deep learning models which try to do this, but I don't think there has been much serious work by the community due to the difficulties in recognizing even clean speech.\n\nThere was a nice paper on using recurrent neural networks for BSS a while back but the link has since been taken down. \n\nOnce again I am biased because our group is working on these kinds of things, but deep models that parameterize probability distributions seem like a very promising research area for this kind of disentangling. However it is quite difficult even for humans, so that makes it a more complex task than just saying \"automate this at about human level\". We have seen enormous boosts in other tasks using attention (text, images) so it seems reasonable that a solid formula for attention in speech could help. Jan Chorowski did some stuff with this for single speaker recognition [here](http://arxiv.org/pdf/1412.1602.pdf), but multi-speaker is a whole different can of worms", "aSentId": 50826, "answer": "Well, two-voice separation can already be done with deep learning but the problem is that the network must be pre-trained with both voices. Humans, by contrast, have no trouble separating new voices in a conversation. Here's a link to a recent paper:\n\n[Probabilistic Binary-Mask Cocktail-Party Source Separation in a Convolutional Deep Neural Network](http://arxiv.org/abs/1503.06962)\n\nI don't think this is a problem that can be solved with supervised learning.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50827, "question": "Well, two-voice separation can already be done with deep learning but the problem is that the network must be pre-trained with both voices. Humans, by contrast, have no trouble separating new voices in a conversation. Here's a link to a recent paper:\n\n[Probabilistic Binary-Mask Cocktail-Party Source Separation in a Convolutional Deep Neural Network](http://arxiv.org/abs/1503.06962)\n\nI don't think this is a problem that can be solved with supervised learning.", "aSentId": 50828, "answer": "Agree totally on supervised learning not being useful for this task - this is one of the things that attracts me to variational autoencoders (maybe even semi supervised VAE) for these kinds of tasks. Training on speakers then separating only stuff from those speakers is definitely not *blind* source separation, which is the thing we really want to solve :) \n\nI think I saw this paper on Igor Carron's blog recently, or maybe you linked to it in another post. \n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50831, "question": "\"Structural Analysis and Visualization of Networks\"- video course at HSE about social network analysis.", "aSentId": 50832, "answer": "Social Network Analysis course at HSE. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50831, "question": "\"Structural Analysis and Visualization of Networks\"- video course at HSE about social network analysis.", "aSentId": 50834, "answer": "Good lectures with clear explanation of concepts. Lots of topics covered. I recommend it to everyone who wants to understand networks! ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50837, "question": "Beginner intro to machine learning in R", "aSentId": 50838, "answer": "This is a good tutorial that is easy to follow. My beef is that it seems like every tutorial or cluster example uses Iris data.  If you choose to add to the dozens of tutorials already out there (and since you include a step on downloading data from the web) it would be refreshing to use a more original data set. (Take this criticism with a grain of salt as I have no plans to build a tutorial to share with the world)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50840, "question": "Help need with Text Mining+Graph Theory idea", "aSentId": 50841, "answer": "frankly human generated mind maps seem pretty meaningless to me.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50842, "question": "frankly human generated mind maps seem pretty meaningless to me.", "aSentId": 50843, "answer": "Did you mean 'human'? If yes, thats an interesting opinion :-P. Elaborate?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50844, "question": "Did you mean 'human'? If yes, thats an interesting opinion :-P. Elaborate?", "aSentId": 50845, "answer": "mind maps r dumb mmkay?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50847, "question": "[Discussion] Hedge funds are starting Artificial Intelligence departments", "aSentId": 50848, "answer": "I think there's always been a lot of hype surrounding ML and finance. \n\nI've talked to a half dozen quant funds trying to use \"deep learning\" or \"RNNs\" to steer their trading, but I've never seen any real evidence that this is working for anyone beyond helping them sell their buzzword filled prospectuses to clients. A lot of these people have no idea how things actually work...\n\nSVM, logits, LDSs, convex-solvers, etc etc get heavy use. Some control theory stuff which I guess overlaps more with AI. The idea that you can simply take things that work well in vision, nlp, etc and apply them easily in market prediction is, well, silly.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50849, "question": "I think there's always been a lot of hype surrounding ML and finance. \n\nI've talked to a half dozen quant funds trying to use \"deep learning\" or \"RNNs\" to steer their trading, but I've never seen any real evidence that this is working for anyone beyond helping them sell their buzzword filled prospectuses to clients. A lot of these people have no idea how things actually work...\n\nSVM, logits, LDSs, convex-solvers, etc etc get heavy use. Some control theory stuff which I guess overlaps more with AI. The idea that you can simply take things that work well in vision, nlp, etc and apply them easily in market prediction is, well, silly.", "aSentId": 50850, "answer": "Why do you think it is silly?  recurrent algorithms seems rather applicable to factor analysis or risk modelling.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50849, "question": "I think there's always been a lot of hype surrounding ML and finance. \n\nI've talked to a half dozen quant funds trying to use \"deep learning\" or \"RNNs\" to steer their trading, but I've never seen any real evidence that this is working for anyone beyond helping them sell their buzzword filled prospectuses to clients. A lot of these people have no idea how things actually work...\n\nSVM, logits, LDSs, convex-solvers, etc etc get heavy use. Some control theory stuff which I guess overlaps more with AI. The idea that you can simply take things that work well in vision, nlp, etc and apply them easily in market prediction is, well, silly.", "aSentId": 50852, "answer": "The fact that they just hired David Ferrucci, the king of deep learning, is some indication that it might be working.\n\nMaybe it isn't so silly at all? vision is mainly just pattern recognition. All the financial classes I have taken in my studies have never went further than utilizing and manipulating the covarience matrix. And I was always pretty disapointed to see how shallow this theory really is. \n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50853, "question": "The fact that they just hired David Ferrucci, the king of deep learning, is some indication that it might be working.\n\nMaybe it isn't so silly at all? vision is mainly just pattern recognition. All the financial classes I have taken in my studies have never went further than utilizing and manipulating the covarience matrix. And I was always pretty disapointed to see how shallow this theory really is. \n", "aSentId": 50854, "answer": "David Ferrucci isn't the king of deep learning.  As far as I can tell, he doesn't have any publications in the field.  \n\nhttp://dblp.uni-trier.de/pers/hd/f/Ferrucci:David_A=", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50847, "question": "[Discussion] Hedge funds are starting Artificial Intelligence departments", "aSentId": 50856, "answer": "Not really news. Machine learning PhDs have been disappearing into hedge funds and other finance-sector operations for a long time already.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50857, "question": "Not really news. Machine learning PhDs have been disappearing into hedge funds and other finance-sector operations for a long time already.", "aSentId": 50858, "answer": "As a PhD student focusing on data mining, where do I go?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50857, "question": "Not really news. Machine learning PhDs have been disappearing into hedge funds and other finance-sector operations for a long time already.", "aSentId": 50860, "answer": "Have any really top Machine Learning PhD grads or professors gone to banks or hedge funds recently?  \n\nI mostly hear about top students taking positions at top tech companies (Google, Facebook, Amazon) or starting their own companies.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50861, "question": "Have any really top Machine Learning PhD grads or professors gone to banks or hedge funds recently?  \n\nI mostly hear about top students taking positions at top tech companies (Google, Facebook, Amazon) or starting their own companies.  ", "aSentId": 50862, "answer": "Look at Jon McCauliffe at Berkeley.  I also recall hearing David Donoho worked a year for some fund (Renaissance maybe?) and was paid $1 Mil or something ridiculous like that. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50861, "question": "Have any really top Machine Learning PhD grads or professors gone to banks or hedge funds recently?  \n\nI mostly hear about top students taking positions at top tech companies (Google, Facebook, Amazon) or starting their own companies.  ", "aSentId": 50864, "answer": "&gt;Downvoted for asking a perfectly reasonable question", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50847, "question": "[Discussion] Hedge funds are starting Artificial Intelligence departments", "aSentId": 50866, "answer": "Machine learning in finance probably predates the coining of the phrase machine learning. Probably predates modern computers too. So far back that modern statistics weren't around yet.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50867, "question": "Machine learning in finance probably predates the coining of the phrase machine learning. Probably predates modern computers too. So far back that modern statistics weren't around yet.", "aSentId": 50868, "answer": "The point of this thread was really not to get these kinds of replies. Everybody knows that the theory of machine learning has been way ahead of the computing power for many decades.\n\nBut now with the increasing computing power the industry is blossoming and many are finding new usage for old theory and that was really a part of the discussion I was hoping to have. \n\nIt was just a few years ago that kernels where \"The Thing\" but now nobody even considers them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50869, "question": "The point of this thread was really not to get these kinds of replies. Everybody knows that the theory of machine learning has been way ahead of the computing power for many decades.\n\nBut now with the increasing computing power the industry is blossoming and many are finding new usage for old theory and that was really a part of the discussion I was hoping to have. \n\nIt was just a few years ago that kernels where \"The Thing\" but now nobody even considers them.", "aSentId": 50870, "answer": "Sorry :) Well it is interesting to hear that some Watson guys left, after the 1B that IBM supposedly recently put into that project. I'll just elaborate that finance / future predicting are really the most perfect way to turn ML into cash - ignoring the economies of scale required. Wouldn't be surprised if that and military applications are still the top AI money-makers in another 100 years from now.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50869, "question": "The point of this thread was really not to get these kinds of replies. Everybody knows that the theory of machine learning has been way ahead of the computing power for many decades.\n\nBut now with the increasing computing power the industry is blossoming and many are finding new usage for old theory and that was really a part of the discussion I was hoping to have. \n\nIt was just a few years ago that kernels where \"The Thing\" but now nobody even considers them.", "aSentId": 50872, "answer": "the problem is, its hard to find recurring patterns/signals in markets..machine learning works if there are patterns in the data that happen in real time ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50874, "question": "I work at a hedge fund, and most of what I do is machine learning. AMA if you want, I can't promise I'll answer everything.", "aSentId": 50875, "answer": "Excellent! Just what I was hoping for.\n\n1. What kind of ML tools have you found performing well? and poorly?\n\n2. And what are you trying to predict and why?\n\n3. Are you using a very active strategy in your hedging? Or is it more of a long term strategy?\n\n4. How would you describe working in this industry? Are you doing some research or is it mainly routine work?\n\nThanks, hopefully you are willing to answer these questions. I know many here are really curious about this sector of ML. I for an example have only been doing social and biological ML research.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50876, "question": "Excellent! Just what I was hoping for.\n\n1. What kind of ML tools have you found performing well? and poorly?\n\n2. And what are you trying to predict and why?\n\n3. Are you using a very active strategy in your hedging? Or is it more of a long term strategy?\n\n4. How would you describe working in this industry? Are you doing some research or is it mainly routine work?\n\nThanks, hopefully you are willing to answer these questions. I know many here are really curious about this sector of ML. I for an example have only been doing social and biological ML research.\n\n", "aSentId": 50877, "answer": "1) Varies highly depending on the problem, but as I mentioned above typically the classical tools do not work well applied out-of-the box to financial data. We use proprietary ML techniques developed in-house (unpublished). Given how much money can be made by the smallest boost in model performance, I spend a lot of time reading the cutting-edge of research and looking for good results to test out.\n\n2) This is the secret to success in quant finance, so I can't give it away. But I can say that you should completely disregard any academic papers which do classification on financial data, and any with an R^2 &gt; 0.01.\n\n3) We do longer-term stuff, not high-frequency. High frequency is interesting though, it's just not scalable.\n\n4) I can't speak for all firms, but at the one I work at it's a relaxed environment. I know some quant shops are super intense with crazy hours, but we're pretty laid back (and also small, which probably has something to do with it). \n\nI would say what I do is at least 50% research. The other big parts are supporting models that have been deployed to production and doing QA (QA never stops). A smaller piece is various one-off analyses.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50878, "question": "1) Varies highly depending on the problem, but as I mentioned above typically the classical tools do not work well applied out-of-the box to financial data. We use proprietary ML techniques developed in-house (unpublished). Given how much money can be made by the smallest boost in model performance, I spend a lot of time reading the cutting-edge of research and looking for good results to test out.\n\n2) This is the secret to success in quant finance, so I can't give it away. But I can say that you should completely disregard any academic papers which do classification on financial data, and any with an R^2 &gt; 0.01.\n\n3) We do longer-term stuff, not high-frequency. High frequency is interesting though, it's just not scalable.\n\n4) I can't speak for all firms, but at the one I work at it's a relaxed environment. I know some quant shops are super intense with crazy hours, but we're pretty laid back (and also small, which probably has something to do with it). \n\nI would say what I do is at least 50% research. The other big parts are supporting models that have been deployed to production and doing QA (QA never stops). A smaller piece is various one-off analyses.", "aSentId": 50879, "answer": "How much do you think that the inability to publish effects progress in your field?  \n\nDo you think that most of the top groups are basically aware of what the state of the art is?  Do you think that significant research efforts end up getting replicated or repeated across multiple companies?  \n\nCan information only flow between companies if they hire employees away from each other?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50880, "question": "How much do you think that the inability to publish effects progress in your field?  \n\nDo you think that most of the top groups are basically aware of what the state of the art is?  Do you think that significant research efforts end up getting replicated or repeated across multiple companies?  \n\nCan information only flow between companies if they hire employees away from each other?  ", "aSentId": 50881, "answer": "Inability to publish drives demand for competent ML people, because things have to be rediscovered. Progress in the field isn't well defined. If you get 10% better at predicting the market, that's not a ton better for society (it just makes the market more random). There's an information equilibrium with publishing in quant finance. If everyone published, the market would be essentially random, so nobody would try to predict it, thus making it predictable again. I'm sure you could do some interesting game theory stuff with that...\n\nI actually don't know on this one. I bet each firm finds a different local minima though. There's definitely going to be duplication of research effort across companies. Sometimes different companies end up doing very similar things (which caused the Quant Quake of Aug 07).\n\nI suspect information transfer only happens when employees change firms, yes. This hasn't happened to us so I can't say for sure.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50878, "question": "1) Varies highly depending on the problem, but as I mentioned above typically the classical tools do not work well applied out-of-the box to financial data. We use proprietary ML techniques developed in-house (unpublished). Given how much money can be made by the smallest boost in model performance, I spend a lot of time reading the cutting-edge of research and looking for good results to test out.\n\n2) This is the secret to success in quant finance, so I can't give it away. But I can say that you should completely disregard any academic papers which do classification on financial data, and any with an R^2 &gt; 0.01.\n\n3) We do longer-term stuff, not high-frequency. High frequency is interesting though, it's just not scalable.\n\n4) I can't speak for all firms, but at the one I work at it's a relaxed environment. I know some quant shops are super intense with crazy hours, but we're pretty laid back (and also small, which probably has something to do with it). \n\nI would say what I do is at least 50% research. The other big parts are supporting models that have been deployed to production and doing QA (QA never stops). A smaller piece is various one-off analyses.", "aSentId": 50883, "answer": "so is classification on financial data pointless? or is it just a very thinly disguised mix of regression and classification? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50884, "question": "so is classification on financial data pointless? or is it just a very thinly disguised mix of regression and classification? ", "aSentId": 50885, "answer": "Fairly pointless. Why predict sign(y) instead of y itself? You're just throwing away information.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50886, "question": "Fairly pointless. Why predict sign(y) instead of y itself? You're just throwing away information.", "aSentId": 50887, "answer": "fair enough. wasnt sure how teh quants do it ..but to be fair..if sign(y) happens enoug and the market does X more often than not...why not..? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50888, "question": "fair enough. wasnt sure how teh quants do it ..but to be fair..if sign(y) happens enoug and the market does X more often than not...why not..? ", "aSentId": 50889, "answer": "Well because +0.01% and -0.01% are essentially the same thing, whereas +.01% and +.8% are vastly different. Sign(y) does not encode this information.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50878, "question": "1) Varies highly depending on the problem, but as I mentioned above typically the classical tools do not work well applied out-of-the box to financial data. We use proprietary ML techniques developed in-house (unpublished). Given how much money can be made by the smallest boost in model performance, I spend a lot of time reading the cutting-edge of research and looking for good results to test out.\n\n2) This is the secret to success in quant finance, so I can't give it away. But I can say that you should completely disregard any academic papers which do classification on financial data, and any with an R^2 &gt; 0.01.\n\n3) We do longer-term stuff, not high-frequency. High frequency is interesting though, it's just not scalable.\n\n4) I can't speak for all firms, but at the one I work at it's a relaxed environment. I know some quant shops are super intense with crazy hours, but we're pretty laid back (and also small, which probably has something to do with it). \n\nI would say what I do is at least 50% research. The other big parts are supporting models that have been deployed to production and doing QA (QA never stops). A smaller piece is various one-off analyses.", "aSentId": 50891, "answer": "What qualifications/resume do you need to enter the field ? Is it a given that you have to be in NY ?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50892, "question": "What qualifications/resume do you need to enter the field ? Is it a given that you have to be in NY ?", "aSentId": 50893, "answer": "The field is pretty diverse. Typically it is people with physics, math, cs or stats backgrounds. Most have at least a masters, and quite a few have PhDs. Of course, legit experience in the field can be a substitute for additional education.\n\nMost funds are based out of major financial hubs like NYC and London. However there are also a lot of funds in nearby areas like Connecticut. The company I work for is not based in the northeast US, so those opportunities do exist.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50874, "question": "I work at a hedge fund, and most of what I do is machine learning. AMA if you want, I can't promise I'll answer everything.", "aSentId": 50895, "answer": "How do you get into a field like that? Assuming you have Applied ML experience in production environments.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50896, "question": "How do you get into a field like that? Assuming you have Applied ML experience in production environments.", "aSentId": 50897, "answer": "With good ML experience you should have good results just sending your resume out to companies you're interested in. There aren't a ton of competent ML people out there, and most shops hire as much as they can find. Applied ML experience is the hard part; learning the finance is pretty easy. \n\nI got into the field a different way though, I started out in software and then switched to the ML/modeling side after. This probably only would work at a small fund though - we were pretty small when I started. Once funds get big, the lines between software and research become more distinct.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50898, "question": "With good ML experience you should have good results just sending your resume out to companies you're interested in. There aren't a ton of competent ML people out there, and most shops hire as much as they can find. Applied ML experience is the hard part; learning the finance is pretty easy. \n\nI got into the field a different way though, I started out in software and then switched to the ML/modeling side after. This probably only would work at a small fund though - we were pretty small when I started. Once funds get big, the lines between software and research become more distinct.", "aSentId": 50899, "answer": "what makes one a \"competent machine learning\" person?  does scoring well on kaggle count..? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50900, "question": "what makes one a \"competent machine learning\" person?  does scoring well on kaggle count..? ", "aSentId": 50901, "answer": "Kaggle is good, especially success in multiple competitions. However production ML experience is better, because there's a lot that happens before and after the \"kaggle stage\". Financial data is quite unique in that it's almost entirely noise, so often classical ML techniques need to be amended to work. For this reason it helps to know that you know how to adapt the standard ML tools to better fit the scenario at hand, which happens sometimes in Kaggle.\n\nSo...I'd highly recommend Kaggle, but you should also have experience with end-to-end systems (data gathering to using the model after it's built).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50902, "question": "Kaggle is good, especially success in multiple competitions. However production ML experience is better, because there's a lot that happens before and after the \"kaggle stage\". Financial data is quite unique in that it's almost entirely noise, so often classical ML techniques need to be amended to work. For this reason it helps to know that you know how to adapt the standard ML tools to better fit the scenario at hand, which happens sometimes in Kaggle.\n\nSo...I'd highly recommend Kaggle, but you should also have experience with end-to-end systems (data gathering to using the model after it's built).", "aSentId": 50903, "answer": "cool. well rigfht now im using ML for marketing and some other business demands but feel like im not getting the type of experience i really need. ..ie not being used in 'production' and more or less just being used to drive strategy..still interesting but its more 'amateurish', in my eyes \n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50898, "question": "With good ML experience you should have good results just sending your resume out to companies you're interested in. There aren't a ton of competent ML people out there, and most shops hire as much as they can find. Applied ML experience is the hard part; learning the finance is pretty easy. \n\nI got into the field a different way though, I started out in software and then switched to the ML/modeling side after. This probably only would work at a small fund though - we were pretty small when I started. Once funds get big, the lines between software and research become more distinct.", "aSentId": 50905, "answer": "Thanks for the detailed answer! Appreciate it!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50874, "question": "I work at a hedge fund, and most of what I do is machine learning. AMA if you want, I can't promise I'll answer everything.", "aSentId": 50907, "answer": "Did you find any benefit in using deep or recurrent neural networks for financial prediction? Apart from sentiment analysis on text.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50908, "question": "Did you find any benefit in using deep or recurrent neural networks for financial prediction? Apart from sentiment analysis on text.", "aSentId": 50909, "answer": "Maybe...maybe not. \n\nOne of the cool things about financial prediction is that all the data you need to get started is available online for free from a ton of sources. Download a bunch and give it a shot.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50908, "question": "Did you find any benefit in using deep or recurrent neural networks for financial prediction? Apart from sentiment analysis on text.", "aSentId": 50911, "answer": "I've used recurrent neural networks for time series forecasting, and they work well, but I think that they require a lot of data to work well.  It's also difficult to pick the right granularity to model if you have a continuous time series (i.e. weekly granularity is fast but makes it hard to model daily changes).  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50912, "question": "I've used recurrent neural networks for time series forecasting, and they work well, but I think that they require a lot of data to work well.  It's also difficult to pick the right granularity to model if you have a continuous time series (i.e. weekly granularity is fast but makes it hard to model daily changes).  ", "aSentId": 50913, "answer": "If you want to see what finance is like, take your targets y and define a new target y' = 0.97*u + 0.03*y, where u is a gaussian with the same mean/std as y. Of course, we also have huge outliers but that's another story.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50915, "question": "1. How does salary compare to positions in non-financial companies that hire people with similar background? x1.5? Bonuses?\n2. Is there room to grow or you pretty much have same job, salary and title for the rest of your career?\n3. I guess the companies are very secretive, so if you want to move to other place, can you actually show/tell what you have been doing?", "aSentId": 50916, "answer": "1) I've only worked in finance. What's normal for ML people not in NYC or SF? 100k? Salary will likely be the same, with bonus coming in at 1-10x salary depending on performance.\n\n2) Definitely room to grow. The most common path is to start your own fund, which can be done within a parent company or separately.\n\n3) Very secretive. Noncompetes and NDA's are everywhere. I suspect information transfer happens mostly by poaching employees, but we haven't done that yet so I'm not sure.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50920, "question": "This isnt anything new. Lots of shops out there trade using Machine Learning / AI. ", "aSentId": 50921, "answer": "A lot of the \"old guard\" of hedge funds, consisting of ivy-educated old guys who think they can outsmart the market, are getting their asses kicked by pure systematic quant shops (e.g. Rentech). They're starting to get over their pride and follow the money.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50922, "question": "A lot of the \"old guard\" of hedge funds, consisting of ivy-educated old guys who think they can outsmart the market, are getting their asses kicked by pure systematic quant shops (e.g. Rentech). They're starting to get over their pride and follow the money.", "aSentId": 50923, "answer": "Rentech is pretty much the best hedge fund ever and DE Shaw was in this game for along time too.\n\ni think the value addition to big data wont be neccessarily from reading the markets, but taking strucured/unstructured data from other sources and mixing it up with market data...which is pretty much what rentech did/does..but..thats another story...IMO...AI/ML on markets could be another way for the big dogs to disguise insider trading ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50925, "question": "Looking for ML-flavored propagation-styled graph algorithms", "aSentId": 50926, "answer": "hey, \n\nI guess that you mean message passing in general. is that right? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50927, "question": "hey, \n\nI guess that you mean message passing in general. is that right? ", "aSentId": 50928, "answer": "&gt; message passing in general\n\nDoes message passing generalize beyond probabilistic graphical models?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50925, "question": "Looking for ML-flavored propagation-styled graph algorithms", "aSentId": 50930, "answer": "Look up message passing and a graph's partition function.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50931, "question": "Look up message passing and a graph's partition function.", "aSentId": 50932, "answer": "I'd say that the graphs partition function is irrelevant if he tends to use somewhat like a log sum-product algorithm for bayesian networks.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50934, "question": "Has anyone replicated AlexNet in Theano?", "aSentId": 50935, "answer": "Gavin from our lab at the university of guelph did it, with support for multiple gpus: https://github.com/uoguelph-mlrg/theano_alexnet", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50936, "question": "Gavin from our lab at the university of guelph did it, with support for multiple gpus: https://github.com/uoguelph-mlrg/theano_alexnet", "aSentId": 50937, "answer": "Have you guys considered using something like lmdb like Caffe to get better throughput of images rather than having a separate process to load the data on the GPU. That seems somewhat expensive for GPU memory as you'll have to use 2x the memory for this process.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50938, "question": "Have you guys considered using something like lmdb like Caffe to get better throughput of images rather than having a separate process to load the data on the GPU. That seems somewhat expensive for GPU memory as you'll have to use 2x the memory for this process.", "aSentId": 50939, "answer": "In terms of wasting GPU memory, it won't be 2x, because there's only duplication of the input batch. So it shouldn't be a huge waste. A batch containing 256 float32 256x256x3 images have roughly 200M. For a GPU with 4GB+ memory, this shouldn't be a issue.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50934, "question": "Has anyone replicated AlexNet in Theano?", "aSentId": 50941, "answer": " Alexnet uses some convolutional structures that theano doesn't have, such as grouping of convolutional params. So you can't do it without making your own layers.\n\nare you looking to train alexnet from scratch, or do you need a trained alexnet in theano?\n\nif it's the latter, for a class project I made a repository that converts trained caffe models into equivalent theano models, you can use it if you want: https://github.com/kitofans/caffe-theano-conversion. I have made my own layers to support grouping/other parameters that are in caffe (and alexnet) that aren't in theano.\n\nit's admittedly got not amazing documentation, but you can probably figure it out. the convert function returns a \"BaseModel\" object which is just a wrapper around lasagne layers. You don't need caffe installed to use it, but without caffe installed you can't confirm the accuracy of the conversion.\n\nIf you want to train it from scratch, you can use the convert model definition to get a model and then train it yourself, but the repo doesn't make this too easy to do, you'll have to go deep into the code.\n\nlet me know if you need help using the repo, i'm happy to provide support. I've planned on making it a more open-source contribution to maybe lasagne (it's written in lasagne basically) because I think it is very useful (allowing transfer learning in theano) but haven't had the time.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50942, "question": " Alexnet uses some convolutional structures that theano doesn't have, such as grouping of convolutional params. So you can't do it without making your own layers.\n\nare you looking to train alexnet from scratch, or do you need a trained alexnet in theano?\n\nif it's the latter, for a class project I made a repository that converts trained caffe models into equivalent theano models, you can use it if you want: https://github.com/kitofans/caffe-theano-conversion. I have made my own layers to support grouping/other parameters that are in caffe (and alexnet) that aren't in theano.\n\nit's admittedly got not amazing documentation, but you can probably figure it out. the convert function returns a \"BaseModel\" object which is just a wrapper around lasagne layers. You don't need caffe installed to use it, but without caffe installed you can't confirm the accuracy of the conversion.\n\nIf you want to train it from scratch, you can use the convert model definition to get a model and then train it yourself, but the repo doesn't make this too easy to do, you'll have to go deep into the code.\n\nlet me know if you need help using the repo, i'm happy to provide support. I've planned on making it a more open-source contribution to maybe lasagne (it's written in lasagne basically) because I think it is very useful (allowing transfer learning in theano) but haven't had the time.", "aSentId": 50943, "answer": "that's awesome, I've been thinking about writing something similar but I'm glad someone did it already.\n\nthere's now a place on the lasange wiki for related external code, please consider adding a link to your project there:\n\nhttps://github.com/benanne/Lasagne/wiki/3rd-party-extensions-and-code", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50944, "question": "that's awesome, I've been thinking about writing something similar but I'm glad someone did it already.\n\nthere's now a place on the lasange wiki for related external code, please consider adding a link to your project there:\n\nhttps://github.com/benanne/Lasagne/wiki/3rd-party-extensions-and-code", "aSentId": 50945, "answer": "oh,  didn't know that existed. I actually just e-mailed /u/benanne on the email he lists on github and asked him for advice on what he thinks would be the best way to give this to the open source community, maybe he'll just tell me that.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50934, "question": "Has anyone replicated AlexNet in Theano?", "aSentId": 50947, "answer": "[This](http://arxiv.org/pdf/1412.2302v3.pdf) is a pretty recent paper that did 2-gpu Alexnet in theano, unfortunately they needed PyCuda to glue some of it together as theano's multi-gpu is still a work in progress.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 50949, "question": "The Automatic Statistician", "aSentId": 50950, "answer": "This was a really interesting one. That guys job sounds fun.", "corpus": "reddit"}]