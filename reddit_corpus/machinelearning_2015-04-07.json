[{"docID": "t5_2r3gv", "qSentId": 60707, "question": "UPDATE: Andrew Ng and Adam Coates will be doing an AMA in /r/MachineLearning on April 14 9AM PST", "aSentId": 60708, "answer": "GREAT! thanks", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60707, "question": "UPDATE: Andrew Ng and Adam Coates will be doing an AMA in /r/MachineLearning on April 14 9AM PST", "aSentId": 60710, "answer": "this is amazing!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60707, "question": "UPDATE: Andrew Ng and Adam Coates will be doing an AMA in /r/MachineLearning on April 14 9AM PST", "aSentId": 60712, "answer": "Sorry, I'm new to reddit, what's an AMA?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60713, "question": "Sorry, I'm new to reddit, what's an AMA?", "aSentId": 60714, "answer": "AMA means \"Ask me anything.\"\nYou can ask questions and, in this case, Andrew Ng and Adam Coates will answer them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60715, "question": "AMA means \"Ask me anything.\"\nYou can ask questions and, in this case, Andrew Ng and Adam Coates will answer them.", "aSentId": 60716, "answer": "Andrew Ng and Adam Coates... Exciting!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60715, "question": "AMA means \"Ask me anything.\"\nYou can ask questions and, in this case, Andrew Ng and Adam Coates will answer them.", "aSentId": 60718, "answer": "Is there a way to reminder to when the AMA starts?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60719, "question": "Is there a way to reminder to when the AMA starts?", "aSentId": 60720, "answer": "I don't think Reddit has a reminder feature. However setting the AMA time on your Google calendar can help.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60724, "question": "[1504.00941] A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "aSentId": 60725, "answer": "Slightly off-topic question.  This paper refers to the billion-word-dataset in 1312.3005 and when I looked at that paper I found the the following rather incredulous claim:\n\n&gt;We cut down training times by a factor of 20-50\n&gt;for large problems using a number of techniques,\n&gt;which allow RNN training in typically 1-10 days\n&gt;with billions of words, &gt; 1M vocabularies and up to\n&gt;20B parameters on a single standard machine without\n&gt;GPUs.\n\nThey say they achieve this using some tricks they will describe in a follow up paper but I didn't have any luck finding it.  Anyone know any more about this?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60726, "question": "Slightly off-topic question.  This paper refers to the billion-word-dataset in 1312.3005 and when I looked at that paper I found the the following rather incredulous claim:\n\n&gt;We cut down training times by a factor of 20-50\n&gt;for large problems using a number of techniques,\n&gt;which allow RNN training in typically 1-10 days\n&gt;with billions of words, &gt; 1M vocabularies and up to\n&gt;20B parameters on a single standard machine without\n&gt;GPUs.\n\nThey say they achieve this using some tricks they will describe in a follow up paper but I didn't have any luck finding it.  Anyone know any more about this?", "aSentId": 60727, "answer": "&gt; These techniques were in order of importance: a) Parallelization of training across available CPU threads, b) Making use of SIMD instructions where possible, ...\n\nDon't OpenBLAS/MKL already do this for you (among other things) ?!\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60724, "question": "[1504.00941] A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "aSentId": 60729, "answer": "It's unclear to me how they initialize the hidden states. Did I miss it in the paper?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60730, "question": "It's unclear to me how they initialize the hidden states. Did I miss it in the paper?", "aSentId": 60731, "answer": "Yeah I didn't see anything in the paper about this, either.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60735, "question": "[Question] With L1/L2 Regularization in a neural network, why are the weights regularized, but not the biases?", "aSentId": 60736, "answer": "It's not typical to regularize the biases, the probable reason being that doing so directly limits the amount of nonlinearity you can learn (*edit: in a sigmoidal net, anyway*). If you do regularize them it would make sense to have a much smaller coefficient than for your weights.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60737, "question": "It's not typical to regularize the biases, the probable reason being that doing so directly limits the amount of nonlinearity you can learn (*edit: in a sigmoidal net, anyway*). If you do regularize them it would make sense to have a much smaller coefficient than for your weights.", "aSentId": 60738, "answer": "Can you elaborate on how regularizing biases \"limits the amount of nonlinearity you can learn\"? If you clamped all biases to zero, the neural net would still encode a nonlinear function. \n\nIt's also typical not to regularize the bias even in linear models, like SVMs. My understanding is that this is mostly because a) there's no reason to expect the separating hyperplane to be particularly close to the origin, and b) leaving this one parameter unregularized doesn't have a significant effect on model complexity. But I've never heard a really satisfying explanation.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60739, "question": "Can you elaborate on how regularizing biases \"limits the amount of nonlinearity you can learn\"? If you clamped all biases to zero, the neural net would still encode a nonlinear function. \n\nIt's also typical not to regularize the bias even in linear models, like SVMs. My understanding is that this is mostly because a) there's no reason to expect the separating hyperplane to be particularly close to the origin, and b) leaving this one parameter unregularized doesn't have a significant effect on model complexity. But I've never heard a really satisfying explanation.", "aSentId": 60740, "answer": "I guess my thinking was limited to a sigmoidal activation, that if you penalize the weights and the biases very strongly then you're forcing the bias close to zero and forcing each sigmoid to be close to the linear regime. With ReLUs things get weirder, the bias can't be really negative but it also can't be really positive, so it seems like you're placing much more responsibility on the weights to decide whether a unit should be on or off.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60737, "question": "It's not typical to regularize the biases, the probable reason being that doing so directly limits the amount of nonlinearity you can learn (*edit: in a sigmoidal net, anyway*). If you do regularize them it would make sense to have a much smaller coefficient than for your weights.", "aSentId": 60742, "answer": "Well, you are optimizing the biases as well right?, I think I remember reading that regularizing the biases will only end up in a different bias, but the effect of it is going to be the same.\n\nExample: Optimal bias is 10, if you regularize with 0.1 lambda andL1, the final bias is still going to be the same, you are just going to take more to reach the answer. The bias does not control the degrees of freedom like the rest of the coefs, it just controls how free is the surface to go around.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60743, "question": "Well, you are optimizing the biases as well right?, I think I remember reading that regularizing the biases will only end up in a different bias, but the effect of it is going to be the same.\n\nExample: Optimal bias is 10, if you regularize with 0.1 lambda andL1, the final bias is still going to be the same, you are just going to take more to reach the answer. The bias does not control the degrees of freedom like the rest of the coefs, it just controls how free is the surface to go around.", "aSentId": 60744, "answer": "I don't see why that would be the case in general. Depending on the strength of lambda versus the loss, you may get more bang for your buck pushing the penalty term down instead of the loss.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60735, "question": "[Question] With L1/L2 Regularization in a neural network, why are the weights regularized, but not the biases?", "aSentId": 60746, "answer": "You can if you want. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60735, "question": "[Question] With L1/L2 Regularization in a neural network, why are the weights regularized, but not the biases?", "aSentId": 60748, "answer": "&gt; With L1/L2 Regularization in a neural network, why are the weights regularized, but not the biases?\n\nWho says that? You can regularize the biases in the same way as you do the weights.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60749, "question": "&gt; With L1/L2 Regularization in a neural network, why are the weights regularized, but not the biases?\n\nWho says that? You can regularize the biases in the same way as you do the weights.", "aSentId": 60750, "answer": "it's quite common not to do so, however.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60752, "question": "Introduction to Pandas (Slides from Pydata 2015)", "aSentId": 60753, "answer": "Doh. Four oh four. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60754, "question": "Doh. Four oh four. ", "aSentId": 60755, "answer": "working for me", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60752, "question": "Introduction to Pandas (Slides from Pydata 2015)", "aSentId": 60757, "answer": "Didn't tell me much about Pandas, tbh, but now I'm interested in IPython. Is that a useful environment?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60758, "question": "Didn't tell me much about Pandas, tbh, but now I'm interested in IPython. Is that a useful environment?", "aSentId": 60759, "answer": "Well, yes. It is a quite big project actually. I personally use the REPL (which is embedded in Spyder or on remote servers), the notebook and the (distributed) parallel computing package that is built into it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60760, "question": "Well, yes. It is a quite big project actually. I personally use the REPL (which is embedded in Spyder or on remote servers), the notebook and the (distributed) parallel computing package that is built into it.", "aSentId": 60761, "answer": "Can't get ipython notebook to fire up unfortunately. I'll keep trying.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60762, "question": "Can't get ipython notebook to fire up unfortunately. I'll keep trying.", "aSentId": 60763, "answer": "It works out of the box when using the [anaconda distribution](http://continuum.io/downloads). Maybe you are missing dependencies, like zeromq or there is something blocked on your browser...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60764, "question": "It works out of the box when using the [anaconda distribution](http://continuum.io/downloads). Maybe you are missing dependencies, like zeromq or there is something blocked on your browser...", "aSentId": 60765, "answer": "&gt;    from ._cffi import C, c_constant_names\n  File \"/usr/lib/python2.7/dist-packages/zmq/backend/cffi/_cffi.py\", line 20, in &lt;module&gt;\n    from cffi import FFI\nImportError: No module named cffi  \n\nLooked around, unsure. I'll keep cracking away.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60766, "question": "&gt;    from ._cffi import C, c_constant_names\n  File \"/usr/lib/python2.7/dist-packages/zmq/backend/cffi/_cffi.py\", line 20, in &lt;module&gt;\n    from cffi import FFI\nImportError: No module named cffi  \n\nLooked around, unsure. I'll keep cracking away.\n", "aSentId": 60767, "answer": "Looks like you need to install that missing library. I think it is called python-cffi on debian (probably the same on ubuntu)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60772, "question": "Has there ever been an attempt to create a nondeterministic RNN? (or perhaps a CTRNN)", "aSentId": 60773, "answer": "I think that by \"nondeterministic\" you mean \"stochastic\". I.e. the connection weight between two units is, say, a gaussian random variable, parametrized by mean and and variance (which we tweak via backprop). Using dropout is somewhat like having a nondeterministic RNN (connection is weight times a bernoulli random variable). I think other types of RNN would simply function as regularizers as well. \n\nThe computer science notion of \"nondeterminism\" would be interesting too. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60775, "question": "Minibatching prevents neural networks from learning on nonseparable data?", "aSentId": 60776, "answer": "learning rate will need to be adjusted going from single to minibatch, but assuming you were properly exploring these things it sounds like you have buggy code.\n\nhave you verified that predicting with your correctly trained network in minibatch mode produces the proper results?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60777, "question": "learning rate will need to be adjusted going from single to minibatch, but assuming you were properly exploring these things it sounds like you have buggy code.\n\nhave you verified that predicting with your correctly trained network in minibatch mode produces the proper results?", "aSentId": 60778, "answer": "Yeah, if we train with SGD then test a large batch, we get the identical test results to testing individually. We've also tried the minibatch fitting on other data (both separable classification and regression problems) and it works with no issue. We've tried making up synthetic nonseparable data to reproduce the effect, and had mixed results, for classification problems some seeds will generate the same effect and some won't. For regression making a nonseparable set seems to produce garbage however we train. \n\nSo we think there's something real there, but we're digging at it on overhead now, and wanted to see if there was a simple answer we just weren't aware of before we spent a lot of time rebuilding the wheel. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60779, "question": "Yeah, if we train with SGD then test a large batch, we get the identical test results to testing individually. We've also tried the minibatch fitting on other data (both separable classification and regression problems) and it works with no issue. We've tried making up synthetic nonseparable data to reproduce the effect, and had mixed results, for classification problems some seeds will generate the same effect and some won't. For regression making a nonseparable set seems to produce garbage however we train. \n\nSo we think there's something real there, but we're digging at it on overhead now, and wanted to see if there was a simple answer we just weren't aware of before we spent a lot of time rebuilding the wheel. ", "aSentId": 60780, "answer": "can you post some of your synthetic data? \n\nI'm curious about this - I haven't read anything like it and it doesn't seem intuitive to me that this would be a problem.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60775, "question": "Minibatching prevents neural networks from learning on nonseparable data?", "aSentId": 60782, "answer": "Interesting. My intuition would be that it does not have anything to do with the non-separability. After all, it will just add up gradients. If you are using the Bernoulli cross entropy error function, adding up the gradients will even do the right thing.\n\nApart from that, mini batches can introduce a large amount of noise into the training, but since you seem to have tried adagrad, I guess you are on the save side.\n\nNevertheless, it should be easy to test this. Remove any \"dubious\" training examples and see if learning suddenly works much better.\n\n\nIn general, if you are in such a situation you have to perform pretty scientific steps. Come up with a hypothesis why training does not work. Think about a way to test this. Perform the test.\n\nThe problem with machine learning is that debugging does not work as with other methods: you don't get a definite sign from a compiler or sth, instead you get stochastic error numbers or qualitative results like an error curve. You should try *not to rely on your intuition* there but instead find a formal way to reject/accept hypotheses.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60775, "question": "Minibatching prevents neural networks from learning on nonseparable data?", "aSentId": 60784, "answer": "Identical points with different labels can be a sign of (serious?) problems with data:\na) data or measurement errors b) inadequate feature representation\n\nDid you tried to analyze your dataset, look for QC, etc? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60785, "question": "Identical points with different labels can be a sign of (serious?) problems with data:\na) data or measurement errors b) inadequate feature representation\n\nDid you tried to analyze your dataset, look for QC, etc? ", "aSentId": 60786, "answer": "It's the latter; we know that there are a few missing features that would separate the points, unfortunately they were unavailable and apparently way too expensive to collect. On the other hand we weren't asked to get it perfect, just as close to Bayes error as possible. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60790, "question": "A Theory of Feature Learning", "aSentId": 60791, "answer": "Please try to link to the paper's [arxiv landing page](http://arxiv.org/abs/1504.00083) instead of directly linking to the PDF.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60790, "question": "A Theory of Feature Learning", "aSentId": 60793, "answer": "They explained in a statistical formulation why reconstruction is necessary for unsupervised learning if the learnt features are to be used by some unknown loss function in an unknown task.\n\nAlthough that does not add much to the intuition we long had for feature learning, it is still nice to see a formal formulation.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60794, "question": "They explained in a statistical formulation why reconstruction is necessary for unsupervised learning if the learnt features are to be used by some unknown loss function in an unknown task.\n\nAlthough that does not add much to the intuition we long had for feature learning, it is still nice to see a formal formulation.", "aSentId": 60795, "answer": "Geoff Hinton had an interesting theory about why (stacked) RBMs should be better than AEs: AEs try to ~~reconstruct~~ *represent* even random bits *in higher layers*, RBMs don't.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60796, "question": "Geoff Hinton had an interesting theory about why (stacked) RBMs should be better than AEs: AEs try to ~~reconstruct~~ *represent* even random bits *in higher layers*, RBMs don't.", "aSentId": 60797, "answer": "Is this from one of his 'Capsule' talks or a paper? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60800, "question": "(How) Should I bin categorical features to reduce dimensionality after one hot encoding?", "aSentId": 60801, "answer": "&gt;One hot encoding would be ok for small N, but this seems like a fairly bad idea to me if N were around 1000.  \n\nNot clear to me it's true if you have plenty of data. If you are data-starved this may be true.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60806, "question": "[x-post /r/mlquestions] Subsequence Identification in Time-Series Data?", "aSentId": 60807, "answer": "First question: are you able to identify take off and landing by looking at the data yourself?  \n\nWithout being able to actually see the data (I assume it's not public), my guess is that pressure first drops as the plane takes off, then pressure goes back up when the plane descends.  You might be able to get decent results with a hand-crafted rule.  Are there non-takeoff / landing reasons for pressure to change?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60808, "question": "First question: are you able to identify take off and landing by looking at the data yourself?  \n\nWithout being able to actually see the data (I assume it's not public), my guess is that pressure first drops as the plane takes off, then pressure goes back up when the plane descends.  You might be able to get decent results with a hand-crafted rule.  Are there non-takeoff / landing reasons for pressure to change?  ", "aSentId": 60809, "answer": "Yes, I am able to idenify by 'eye'. And you're correct, it's not public.\n\nYeah, a simple rule would be nice, but plane flights turn out are slightly more complicated. Sometimes they ascend a few times making the flight look like a staircase.\n\nStill, I think a rule-based approach would be doable, but the rule(s) would have to be a little more complex.\n\nLike if F = flat, D = descend, and A = ascend, then a ~~flight~~ shipment might work like\n\nFAFAFAFDFAFDF\n\nAnd we could pick out\n\nF | **AFAFAFD** | F | **AFD** | F\n\nas the flights. Turning the readings into the states is something I'm a little unclear on, though.\n\nEDIT: changed a word", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60810, "question": "Yes, I am able to idenify by 'eye'. And you're correct, it's not public.\n\nYeah, a simple rule would be nice, but plane flights turn out are slightly more complicated. Sometimes they ascend a few times making the flight look like a staircase.\n\nStill, I think a rule-based approach would be doable, but the rule(s) would have to be a little more complex.\n\nLike if F = flat, D = descend, and A = ascend, then a ~~flight~~ shipment might work like\n\nFAFAFAFDFAFDF\n\nAnd we could pick out\n\nF | **AFAFAFD** | F | **AFD** | F\n\nas the flights. Turning the readings into the states is something I'm a little unclear on, though.\n\nEDIT: changed a word", "aSentId": 60811, "answer": "How much variance is there in ground pressue?  Does ground pressure ever go low enough to match the pressure observed in the air?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60812, "question": "How much variance is there in ground pressue?  Does ground pressure ever go low enough to match the pressure observed in the air?  ", "aSentId": 60813, "answer": "Since I'm still hand-labeling flights vs. tarmac, I unfortunately don't have the stats on that yet.\n\nHowever, if I recall, the ground pressure in places like Denver are comparable to cabin pressure.\n\nStill, that would a *super* easy rule to implement, so my fingers are crossed.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60806, "question": "[x-post /r/mlquestions] Subsequence Identification in Time-Series Data?", "aSentId": 60815, "answer": "I think DTW is a good approach, and should be fairly efficient. Are you writing your own code? There are super-optimized algorithms and libraries available for this.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60816, "question": "I think DTW is a good approach, and should be fairly efficient. Are you writing your own code? There are super-optimized algorithms and libraries available for this.", "aSentId": 60817, "answer": "I'm writing parts of it, but I'll be using libraries for delicate parts like calculating the DTW.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60806, "question": "[x-post /r/mlquestions] Subsequence Identification in Time-Series Data?", "aSentId": 60819, "answer": "Just a thought, but wouldn't the 1st and 2nd derivative of pressure be much more interesting than the pressure itself? It seems like you want times where the change itself increased. \n\nAlso, is time-series really needed? it seems like windows of average pressure change velocities and accelerations should make the problem a pretty straightforward clustering problem, or jeez maybe even just a simple heuristic classifier.\n\nEDIT: Another thought it may turn out that a DFT could be used to produce a filter that correlates with landings and takeoffs\n\nIn general ML is best suited for areas where heuristics are difficult to determine, highly error prone / noisy, dynamics are not well understood or relationships are complex. I'm not sure a single stream of sampled pressure will fall into it. I'd bet the physical model is pretty straightforward for post-hoc event labeling.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60820, "question": "Just a thought, but wouldn't the 1st and 2nd derivative of pressure be much more interesting than the pressure itself? It seems like you want times where the change itself increased. \n\nAlso, is time-series really needed? it seems like windows of average pressure change velocities and accelerations should make the problem a pretty straightforward clustering problem, or jeez maybe even just a simple heuristic classifier.\n\nEDIT: Another thought it may turn out that a DFT could be used to produce a filter that correlates with landings and takeoffs\n\nIn general ML is best suited for areas where heuristics are difficult to determine, highly error prone / noisy, dynamics are not well understood or relationships are complex. I'm not sure a single stream of sampled pressure will fall into it. I'd bet the physical model is pretty straightforward for post-hoc event labeling.", "aSentId": 60821, "answer": "Whoops, just saw the edit.\n\nYes, Fourier was definitely also on my list of things to try. I should bump that up.\n\nAlso, could you elaborate a bit on the 'physical model' bit? Thanks.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60823, "question": "Based on similar problems I have worked on before, my initial intuition is to use a Profile HMM.  This is a specific configuration used pretty often in bioinformatics to perform multiple sequence alignment and motif discovery.\n\n", "aSentId": 60824, "answer": "I'd briefly looked into using HMM's, but I'm pretty unfamiliar with them and they seemed like somewhat of a complex model (not that my approach is necessarily simpler).\n\nI'll be coming back to them if I'm not getting the results I want, though, because HMM's seem well suited on paper.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60825, "question": "I'd briefly looked into using HMM's, but I'm pretty unfamiliar with them and they seemed like somewhat of a complex model (not that my approach is necessarily simpler).\n\nI'll be coming back to them if I'm not getting the results I want, though, because HMM's seem well suited on paper.", "aSentId": 60826, "answer": "Depending on the specifics of your data, something like GHMM would be a good library to implement it with.  http://ghmm.org/\n\nThe Profile HMM approach may be more difficult if you data is continuous, but a slight variation in the emission distributions would probably compensate.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60827, "question": "Depending on the specifics of your data, something like GHMM would be a good library to implement it with.  http://ghmm.org/\n\nThe Profile HMM approach may be more difficult if you data is continuous, but a slight variation in the emission distributions would probably compensate.", "aSentId": 60828, "answer": "Thanks for that. I'm glad to see the python wrappers because that's what I'm writing/analyzing in.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60830, "question": "My first impression is that given that the signal you're dealing with is time-dependent and non-stationary (pressure changes in time), the first step could be to extract statistical characteristics of a time-window and then spot when these stats change. A related problem is found in the speech processing tasks: the speech signal is firmed by pressure waves that change rapidly. In this case, mel-frequency ceptrum coeffs work well. I guess your specific field may suggest a similar feature.", "aSentId": 60831, "answer": "Are you referring to change-point analysis?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60832, "question": "Are you referring to change-point analysis?", "aSentId": 60833, "answer": "I am not familiar with this change-point technique, but the overall concept seems to be closely related.\n\nMy suggestion is to:\n1) Take reasonable time window\n2) Assume normal distribution of data (why not?)\n3) Calculate moments (mu, sigma)\n4) Slide window over signal (as time goes on)\n5) Detect significant changes in the signal continuum: t-test with previous window\n6) Given a point of change, you can take the two chunks and model them at will for further analysis", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60834, "question": "I am not familiar with this change-point technique, but the overall concept seems to be closely related.\n\nMy suggestion is to:\n1) Take reasonable time window\n2) Assume normal distribution of data (why not?)\n3) Calculate moments (mu, sigma)\n4) Slide window over signal (as time goes on)\n5) Detect significant changes in the signal continuum: t-test with previous window\n6) Given a point of change, you can take the two chunks and model them at will for further analysis", "aSentId": 60835, "answer": "Yes, this idea is similar to change-point analysis which is concerned with finding points where the probability distribution generating the time series changes (at least, that's my understanding).\n\nThe issue with this approach, I think, is that it only helps me segment the time series into the different 'chunks'. It doesn't help me model them or tell me how to find similar chunks in later shipments. Since I'll be doing the labeling by hand (probably, unless the automatic template construction pointed out above works out), I don't think this helps me out too much.\n\nStill, thanks for your suggestion. And let me know if I misintrepted it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60836, "question": "Yes, this idea is similar to change-point analysis which is concerned with finding points where the probability distribution generating the time series changes (at least, that's my understanding).\n\nThe issue with this approach, I think, is that it only helps me segment the time series into the different 'chunks'. It doesn't help me model them or tell me how to find similar chunks in later shipments. Since I'll be doing the labeling by hand (probably, unless the automatic template construction pointed out above works out), I don't think this helps me out too much.\n\nStill, thanks for your suggestion. And let me know if I misintrepted it.", "aSentId": 60837, "answer": "In a sense, the parameters of a stats distrib can be used as a model. If you have the point when a change is occurring, at least you know  that the part on the left is different (in a statistically significant sense) from the part on the right, as far as you can tell by the parameters you've chosen. Wouldn't that be two different models? Model 1 has u1 and s1, and model 2 has u2 and s2, and you know that u1 is significantly different from u2. If normal distrib is not powerful enough, the chunks can in turn be trained on more complex models.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60838, "question": "In a sense, the parameters of a stats distrib can be used as a model. If you have the point when a change is occurring, at least you know  that the part on the left is different (in a statistically significant sense) from the part on the right, as far as you can tell by the parameters you've chosen. Wouldn't that be two different models? Model 1 has u1 and s1, and model 2 has u2 and s2, and you know that u1 is significantly different from u2. If normal distrib is not powerful enough, the chunks can in turn be trained on more complex models.", "aSentId": 60839, "answer": "You're correct. I misspoke when I said this method couldn't help me model the different chunks.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60842, "question": "Neural network backpropagation help", "aSentId": 60843, "answer": "The weights are usually updated during backprop. Which is done for each pattern. Here's my train function that I use in my personal NeuralNetwork.py.\n\n    def train(self, patterns, iterations=1000, N=0.4, M=0.1):\n            # N: learning rate\n            # M: momentum factor\n            for i in range(iterations):\n                error = 0.0\n                for p in patterns:\n                    inputs = p[0]\n                    targets = p[1]\n                    self.update(inputs)\n                    error = error + self.backPropagate(targets, N, M)\n                if i % 100 == 0:\n                    print('error %-.5f' % error)\n\nI have a [simple multi-layer neural net python class](https://github.com/kirmani/NeuralNetwork/blob/master/NeuralNetwork.py) if you're interested. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60844, "question": "The weights are usually updated during backprop. Which is done for each pattern. Here's my train function that I use in my personal NeuralNetwork.py.\n\n    def train(self, patterns, iterations=1000, N=0.4, M=0.1):\n            # N: learning rate\n            # M: momentum factor\n            for i in range(iterations):\n                error = 0.0\n                for p in patterns:\n                    inputs = p[0]\n                    targets = p[1]\n                    self.update(inputs)\n                    error = error + self.backPropagate(targets, N, M)\n                if i % 100 == 0:\n                    print('error %-.5f' % error)\n\nI have a [simple multi-layer neural net python class](https://github.com/kirmani/NeuralNetwork/blob/master/NeuralNetwork.py) if you're interested. ", "aSentId": 60845, "answer": "Thanks.\n\nFor me, it felt like if you update them after each pattern, you do not really consider how to other patterns will act, and thus you do not necessarily take the correct action.\n\nWould you mind taking a look at my code when I'm done?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60842, "question": "Neural network backpropagation help", "aSentId": 60847, "answer": "The more formally correct way to compute errors is over the full dataset (i.e. both lines in your example) because then you move the network in a direction that minimizes the sum over the error of all (training) datapoints.  This is called batch learning and generally the learning method is gradient descent (GD).  For various reasons however its usually preferable to update the weights after only sampling some small number of data points (128 seems to be popular but it depends on a lot of factors).  This is often better as it takes less time and it adds a degree of \"noise\" to the data from the sampling.  Thus it is referred to as stochastic gradient decent (SGD).\n\nIf you haven't already you might want to check out Hinton's neural networks course on coursera or maybe some textbook on neural networks as I'm sure they all cover this.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60848, "question": "The more formally correct way to compute errors is over the full dataset (i.e. both lines in your example) because then you move the network in a direction that minimizes the sum over the error of all (training) datapoints.  This is called batch learning and generally the learning method is gradient descent (GD).  For various reasons however its usually preferable to update the weights after only sampling some small number of data points (128 seems to be popular but it depends on a lot of factors).  This is often better as it takes less time and it adds a degree of \"noise\" to the data from the sampling.  Thus it is referred to as stochastic gradient decent (SGD).\n\nIf you haven't already you might want to check out Hinton's neural networks course on coursera or maybe some textbook on neural networks as I'm sure they all cover this.", "aSentId": 60849, "answer": "Doing it over the whole dataset feels better to me. Though, I have read papers suggesting that it should rather be done on-line.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60850, "question": "Doing it over the whole dataset feels better to me. Though, I have read papers suggesting that it should rather be done on-line.", "aSentId": 60851, "answer": "It depends a lot on your dataset. For very large datasets (and large models), full-batch learning and online learning are both impractical. Minibatches are a compromise between the two that seems to hit a \"sweet spot\" in performance.\n\nDon't rely on what \"feels better\" though - try both! And see which gives you the best results. That's the best way to learn IMHO.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60853, "question": "Bug Prediction at Google (2011)", "aSentId": 60854, "answer": "I don't get why this is called \"bug prediction\". It seems to be more of a \"bug hotspot analysis\".", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60853, "question": "Bug Prediction at Google (2011)", "aSentId": 60856, "answer": "So how would this detect if a new piece of code (e.g. a new repository) has bugs?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60853, "question": "Bug Prediction at Google (2011)", "aSentId": 60858, "answer": "Well, if they have labeled data for what pieces of code are \"buggy\", then it might be interesting to release it as a labeled dataset.  \n\nIt might actually be more tractable to build a system that predicts whether or not a piece of code will have a certain type of exception thrown, since this is more concrete then whether a code has a bug, and it's more actionable for a user.  \n\nPerhaps an ML algorithm could learn to recognize certain common failiure motifs?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60861, "question": "Help with feature selection for supervised learning classification?", "aSentId": 60862, "answer": "For a simple \"throw an algorithm at it and pray\" result, random forests are usually pretty accurate and robust, and do not usually need hand-engineered features.  Though for sklearn you will want to encode categorical features into one-hot form.  As long as you use axis-aligned stumps (which is either the default or the only way sklearn does it, forget which), ordinal features don't need special treatment, assuming you have any.\n\nFeed everything to it with a few hundred trees and see what happens; they have the singular virtue of failing spectacularly and obviously when they don't work.\n\nBeyond that, without more information about your problem it's pretty hard to guess what might actually work.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60864, "question": "Which license should I use for the Machine Learning framework?", "aSentId": 60865, "answer": "From my commercial experience, BSD or Apache are favoured because they offer a lot less risk of 'IP leakage' caused by building on top / around the code.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60864, "question": "Which license should I use for the Machine Learning framework?", "aSentId": 60867, "answer": "I think GPL is just fine :P \n\nFor me personally, If I was going to put mine under a less restrictive one, I would do Apache. It does a good job of making a lot of rights explicit that are only implied under BSD/MIT. Its also a more common license in Java land (IMHO) which will make it easier for you to share code with projects under the same license. \n\nIf you like the GPL (as I do) and don't want to completely go away from it, you could also use the LGPL. It basically says they don't have to open source their project if all they do is import it. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60868, "question": "I think GPL is just fine :P \n\nFor me personally, If I was going to put mine under a less restrictive one, I would do Apache. It does a good job of making a lot of rights explicit that are only implied under BSD/MIT. Its also a more common license in Java land (IMHO) which will make it easier for you to share code with projects under the same license. \n\nIf you like the GPL (as I do) and don't want to completely go away from it, you could also use the LGPL. It basically says they don't have to open source their project if all they do is import it. ", "aSentId": 60869, "answer": "IIRC, INAL, ETC :) ..  The GPL is great for SaaS but it limits your options on commercial software in some edge cases.  One good example is if you wanted to use a GPL framework with modifications that your company have written and deploy it on some hardware that you sell, it technically falls under the definition of distribution and you end up having to release your modifications as GPL.  It's great for the open source community but if companies want to have patented solutions using a framework they'll pick Apache over GPL.  I do know that GPLv3 had some fixes to these, and LGPL did as well so this info may not even be true anymore.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60870, "question": "IIRC, INAL, ETC :) ..  The GPL is great for SaaS but it limits your options on commercial software in some edge cases.  One good example is if you wanted to use a GPL framework with modifications that your company have written and deploy it on some hardware that you sell, it technically falls under the definition of distribution and you end up having to release your modifications as GPL.  It's great for the open source community but if companies want to have patented solutions using a framework they'll pick Apache over GPL.  I do know that GPLv3 had some fixes to these, and LGPL did as well so this info may not even be true anymore.", "aSentId": 60871, "answer": "the GPLv3 was to prevent exactly that situation (companies stealing GPL software and using it as a selling point for their hardware).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60872, "question": "the GPLv3 was to prevent exactly that situation (companies stealing GPL software and using it as a selling point for their hardware).", "aSentId": 60873, "answer": "Sort of, but you can get around those limitations but simply making everything SaaS and not selling physical systems with the software on it.  The restrictions don't really stop companies from, as you said, \"stealing GPL software and using it as a selling point\" as you can still \"steal\" the software and sell it as a service.  But then you end up restricting the use of it for things like mobile and black box solutions.\n\nBlack box solutions are sometimes needed as you could have an SaaS solution for something, but security or strict compliance scenarios like HIPAA could make SaaS not feasible.  I work with voice recognition systems and even if the SaaS is cheaper and faster than a deployed solution for clients,  a lot of companies require installations on premises because they have to have their own security procedures be applied to the systems.  Licenses like GPL just make scenarios like these more complicated.\n\nMobile has had a bunch of problems with the LGPL.  You can't statically link to libraries with mobile, you have to compile the library into your executable so it's technically illegal for any smartphone app to uses an LGPL library.\n\nFYI I put quotes around \"steal\" because if you have a single license open source codebase you are giving companies permission to use your code and selling hardware with open source software on it isn't really stealing.  As a copyright holder you always have the option of running a dual license for commercial use and make money from it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60874, "question": "Sort of, but you can get around those limitations but simply making everything SaaS and not selling physical systems with the software on it.  The restrictions don't really stop companies from, as you said, \"stealing GPL software and using it as a selling point\" as you can still \"steal\" the software and sell it as a service.  But then you end up restricting the use of it for things like mobile and black box solutions.\n\nBlack box solutions are sometimes needed as you could have an SaaS solution for something, but security or strict compliance scenarios like HIPAA could make SaaS not feasible.  I work with voice recognition systems and even if the SaaS is cheaper and faster than a deployed solution for clients,  a lot of companies require installations on premises because they have to have their own security procedures be applied to the systems.  Licenses like GPL just make scenarios like these more complicated.\n\nMobile has had a bunch of problems with the LGPL.  You can't statically link to libraries with mobile, you have to compile the library into your executable so it's technically illegal for any smartphone app to uses an LGPL library.\n\nFYI I put quotes around \"steal\" because if you have a single license open source codebase you are giving companies permission to use your code and selling hardware with open source software on it isn't really stealing.  As a copyright holder you always have the option of running a dual license for commercial use and make money from it.", "aSentId": 60875, "answer": "&gt;  if you have a single license open source codebase you are giving companies permission to use your code and selling hardware with open source software on it isn't really stealing.\n\nactually, a lot of people consider that it is. it's subverting the spirit of GPL which is releasing code modifications, and preventing that was the motivation for GPLv3.\n\nclosing the SaaS loophole will have to wait for GPLv4 (or the affero GPL).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60876, "question": "&gt;  if you have a single license open source codebase you are giving companies permission to use your code and selling hardware with open source software on it isn't really stealing.\n\nactually, a lot of people consider that it is. it's subverting the spirit of GPL which is releasing code modifications, and preventing that was the motivation for GPLv3.\n\nclosing the SaaS loophole will have to wait for GPLv4 (or the affero GPL).", "aSentId": 60877, "answer": "Whether or not they consider it stealing doesn't matter, what matters is the legal restrictions by the GPL :) but regardless of that, companies contribute to open source projects all the time.  Limiting commercial use on software basically limits it to academia only.  If there's no viable commercial use for a piece of software companies will be less likely to pay into it's development.\n\nI feel that this mentality actually hurts poor entrepreneurs and bolsters big businesses that can afford to develop proprietary software.  If open source software didn't exist for corporate use, IBM and Microsoft would still be the only tech companies.  By allowing corporate use you end up having a common, shared resource that everyone gets to use and improve.  By limiting commercial use with today's tech market you are pretty much digging a grave for your software :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60878, "question": "Whether or not they consider it stealing doesn't matter, what matters is the legal restrictions by the GPL :) but regardless of that, companies contribute to open source projects all the time.  Limiting commercial use on software basically limits it to academia only.  If there's no viable commercial use for a piece of software companies will be less likely to pay into it's development.\n\nI feel that this mentality actually hurts poor entrepreneurs and bolsters big businesses that can afford to develop proprietary software.  If open source software didn't exist for corporate use, IBM and Microsoft would still be the only tech companies.  By allowing corporate use you end up having a common, shared resource that everyone gets to use and improve.  By limiting commercial use with today's tech market you are pretty much digging a grave for your software :)", "aSentId": 60879, "answer": "no shit, that's why we needed GPLv3: companies suck and only act nice when legally required to.\n\nyou only have to look at the relative success of Linux and BSD to see which open source strategy is superior.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60880, "question": "no shit, that's why we needed GPLv3: companies suck and only act nice when legally required to.\n\nyou only have to look at the relative success of Linux and BSD to see which open source strategy is superior.", "aSentId": 60881, "answer": "Maybe, but Linux also has driver problems due to companies not being able to release binary drivers in some instances.  The few non-free drivers that do exist for video cards and whatnot are technically illegal as they are considered via the GPL as derivative works and I'm curious to see what happens when Valve/Steam tries to release their linux based Steambox gaming console as it will require having \"illegal\" binaries on it to work.  (though I'm not sure why video card manufacturers don't just open source their drivers under GPL anyways)\n\nLinux being more popular than BSD is an aftereffect of AT&amp;T's lawsuit against UC Berkeley over their kernel code moreso than which license was used.  Even if UC Berkeley tried to GPL BSD they would have still gotten sued by AT&amp;T.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60874, "question": "Sort of, but you can get around those limitations but simply making everything SaaS and not selling physical systems with the software on it.  The restrictions don't really stop companies from, as you said, \"stealing GPL software and using it as a selling point\" as you can still \"steal\" the software and sell it as a service.  But then you end up restricting the use of it for things like mobile and black box solutions.\n\nBlack box solutions are sometimes needed as you could have an SaaS solution for something, but security or strict compliance scenarios like HIPAA could make SaaS not feasible.  I work with voice recognition systems and even if the SaaS is cheaper and faster than a deployed solution for clients,  a lot of companies require installations on premises because they have to have their own security procedures be applied to the systems.  Licenses like GPL just make scenarios like these more complicated.\n\nMobile has had a bunch of problems with the LGPL.  You can't statically link to libraries with mobile, you have to compile the library into your executable so it's technically illegal for any smartphone app to uses an LGPL library.\n\nFYI I put quotes around \"steal\" because if you have a single license open source codebase you are giving companies permission to use your code and selling hardware with open source software on it isn't really stealing.  As a copyright holder you always have the option of running a dual license for commercial use and make money from it.", "aSentId": 60883, "answer": "&gt;  Licenses like GPL just make scenarios like these more complicated.\n\nyou seem to think the point of open source is to make life awesome for companies. It's not.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60884, "question": "&gt;  Licenses like GPL just make scenarios like these more complicated.\n\nyou seem to think the point of open source is to make life awesome for companies. It's not.", "aSentId": 60885, "answer": "Sorry but you are wrong on both counts, I do not think the point of open source is to make life awesome for companies.  I do think it CAN be a point of SOME open source software.  That's determined by the author's wishes which goes into his decision for picking a license.  This is why I'm explaining how GPL can limit some uses.  As I said in another post here.. companies contribute to open source projects even when they are not required to.  IMO I think that GPL was written with a lot of cynicism at heart and a lot of modern tech companies are totally willing to contribute to open source software as long as it's with things that don't give them a crazy edge on a target market.  I believe that allowing unrestricted commercial use on open source software helps small businesses and entrepreneurs enter the market against big businesses.  It also creates a common, shared resource between companies, scholars and hobbyists so that everyone involved can help improve the software.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 60864, "question": "Which license should I use for the Machine Learning framework?", "aSentId": 60887, "answer": "I use three clause BSD, MIT and Apache are similar. These to me make a lot of sense for analytical software as the user can go in and customize or modify and not be required to open source.\n\nGPL (including GPL affero) can make sense if you want to prevent someone else from doing a commercial fork of your work. ", "corpus": "reddit"}]