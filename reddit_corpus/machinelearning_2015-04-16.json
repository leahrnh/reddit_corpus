[{"docID": "t5_2r3gv", "qSentId": 62853, "question": "AMA Andrew Ng and Adam Coates", "aSentId": 62854, "answer": "What motivates some big companies to publish their ML tricks, like e.g. the recent Batch Normalization from Google? Aren't they giving away their secret sauce to competitors?\n\nDo you think the published results are just the tip of the iceberg, and the very best findings are kept secret?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62855, "question": "What motivates some big companies to publish their ML tricks, like e.g. the recent Batch Normalization from Google? Aren't they giving away their secret sauce to competitors?\n\nDo you think the published results are just the tip of the iceberg, and the very best findings are kept secret?", "aSentId": 62856, "answer": "As a research organization, Baidu Research and others want to be part of the community, and we want to learn from as well as contribute to it.  Of course, publishing also helps us attract talent, and also give our team better internal and external visibility.  But underlying this is that we're researchers and just want to invent ideas that help make the world a better place!  \n\nHaving said that, the mission of the Baidu's AI Lab is to develop hard AI technologies that let us impact hundreds of millions of users.  Thus our focus is on developing and shipping technologies.  It's just that we're pretty open and transparent and are happy to publish a lot of what we learn along the way.  \n\n(By the way, Adam Coates and I are sitting together, so you should assume all these answers are written by both of us.) ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62855, "question": "What motivates some big companies to publish their ML tricks, like e.g. the recent Batch Normalization from Google? Aren't they giving away their secret sauce to competitors?\n\nDo you think the published results are just the tip of the iceberg, and the very best findings are kept secret?", "aSentId": 62858, "answer": "To hire the best researchers they have to demonstrate how world class their research is, which in turn requires publishing lots of good papers. \n\nGoogle publish papers about the majority of advancements in ML.  The thing they rarely talk about is which specific services within Google use ML.   For example,  there are no papers about machine learning in web search.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62855, "question": "What motivates some big companies to publish their ML tricks, like e.g. the recent Batch Normalization from Google? Aren't they giving away their secret sauce to competitors?\n\nDo you think the published results are just the tip of the iceberg, and the very best findings are kept secret?", "aSentId": 62860, "answer": "I wonder whether these techniques are actually patented so that Google profits if others build upon them (because they can demand licensing fees).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62861, "question": "I wonder whether these techniques are actually patented so that Google profits if others build upon them (because they can demand licensing fees).", "aSentId": 62862, "answer": "To my knowledge there aren't any credible patents in deep learning.  \n\nThis is unlike much of computer vision, which has a minefield of patents holding back progress.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62863, "question": "To my knowledge there aren't any credible patents in deep learning.  \n\nThis is unlike much of computer vision, which has a minefield of patents holding back progress.  ", "aSentId": 62864, "answer": "Patents take a year or two to go through the system.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62865, "question": "Patents take a year or two to go through the system.", "aSentId": 62866, "answer": "Neural networks have been around for decades and I haven't seen any credible patents.  The really industrially relevant work in deep learning is more than 1-2 years old.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62867, "question": "Neural networks have been around for decades and I haven't seen any credible patents.  The really industrially relevant work in deep learning is more than 1-2 years old.  ", "aSentId": 62868, "answer": "The question was specifically about newer work from Google, Facebook etc.\n\n\"What motivates some big companies to publish their ML tricks, like e.g. the recent Batch Normalization from Google?\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62853, "question": "AMA Andrew Ng and Adam Coates", "aSentId": 62871, "answer": "I am a big fan of your work Dr. Ng, your coursera course was what introduced me to Machine Learning. My question is do you think a PhD or Masters degree is a strong requirement for those who wish to do ML research in industry or can a Bachelors and independent learning be enough? Thanks.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62872, "question": "I am a big fan of your work Dr. Ng, your coursera course was what introduced me to Machine Learning. My question is do you think a PhD or Masters degree is a strong requirement for those who wish to do ML research in industry or can a Bachelors and independent learning be enough? Thanks.", "aSentId": 62873, "answer": "This question is very common. I would try to help answer this question. It depends what company you are trying to work for. The bigger the company the more they want to see a Masters or PhD degree although that is not always the case. The smaller/newer companies are more willing to accept Bachelors with independent learning. I was looking for entry jobs in the data science field one day and notice a company write out in the job description \"Online degree or certificate can replace 1 year of relevant job experience.\" Just think about this, there are more and more companies wanting people with data analytic/mining skills to get an edge in their respective industries. I believe online learning is also on the rise. Sooner or later companies will accept online certificate (free or not). Masters/PhD are for people wanting to focusing on a area within ML. Bachelors and independent learning is for people who want to get their feet in the door then get the small company pay for your Masters/PhD. Some smaller company pay competitively and raises are based on performance of the company/your contribution.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62874, "question": "This question is very common. I would try to help answer this question. It depends what company you are trying to work for. The bigger the company the more they want to see a Masters or PhD degree although that is not always the case. The smaller/newer companies are more willing to accept Bachelors with independent learning. I was looking for entry jobs in the data science field one day and notice a company write out in the job description \"Online degree or certificate can replace 1 year of relevant job experience.\" Just think about this, there are more and more companies wanting people with data analytic/mining skills to get an edge in their respective industries. I believe online learning is also on the rise. Sooner or later companies will accept online certificate (free or not). Masters/PhD are for people wanting to focusing on a area within ML. Bachelors and independent learning is for people who want to get their feet in the door then get the small company pay for your Masters/PhD. Some smaller company pay competitively and raises are based on performance of the company/your contribution.", "aSentId": 62875, "answer": "I agree that the newer companies---ones that know how to evaluate machine learning talent---care more about your ability, and less about the credential (such as MS or PhD).  For example, at Baidu Research, we do hire top machine learning researchers and machine learning engineers that don't have a graduate degree, but have great software skills and have knowledge of ML from elsewhere. \n\nOver time, companies are also increasingly valuing certificates earned from MOOCs.  \n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62874, "question": "This question is very common. I would try to help answer this question. It depends what company you are trying to work for. The bigger the company the more they want to see a Masters or PhD degree although that is not always the case. The smaller/newer companies are more willing to accept Bachelors with independent learning. I was looking for entry jobs in the data science field one day and notice a company write out in the job description \"Online degree or certificate can replace 1 year of relevant job experience.\" Just think about this, there are more and more companies wanting people with data analytic/mining skills to get an edge in their respective industries. I believe online learning is also on the rise. Sooner or later companies will accept online certificate (free or not). Masters/PhD are for people wanting to focusing on a area within ML. Bachelors and independent learning is for people who want to get their feet in the door then get the small company pay for your Masters/PhD. Some smaller company pay competitively and raises are based on performance of the company/your contribution.", "aSentId": 62877, "answer": "There really isn't much specialization within ML at the Masters level, even at the Top 10 schools - that's mostly reserved for PhDs.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62878, "question": "There really isn't much specialization within ML at the Masters level, even at the Top 10 schools - that's mostly reserved for PhDs.", "aSentId": 62879, "answer": "respected Dr. Ng , can you please share your perspective on Online MS in Data Science from Berkely and Online MS with Specilization in MS from Georgia Tech ? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62878, "question": "There really isn't much specialization within ML at the Masters level, even at the Top 10 schools - that's mostly reserved for PhDs.", "aSentId": 62881, "answer": "That's the entire point of getting a PhD in anything. A masters is too general and you want to specialize on a very specific topic.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62872, "question": "I am a big fan of your work Dr. Ng, your coursera course was what introduced me to Machine Learning. My question is do you think a PhD or Masters degree is a strong requirement for those who wish to do ML research in industry or can a Bachelors and independent learning be enough? Thanks.", "aSentId": 62883, "answer": "Same question. Very relevant because a huge number of students are taking your ML courses to start with.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62872, "question": "I am a big fan of your work Dr. Ng, your coursera course was what introduced me to Machine Learning. My question is do you think a PhD or Masters degree is a strong requirement for those who wish to do ML research in industry or can a Bachelors and independent learning be enough? Thanks.", "aSentId": 62885, "answer": "If you want to do ML research, where research means developing new algorithms, methods, anything not in the already present \"ML cookbooks\" - you need a PhD (or a Master's and significant experience in research).\n\nFrom what I've heard from a few friends currently working in Facebook &amp; Google - there they don't let you touch research (or even ML) without a PhD in the field.\n\nFrom my personal experience, I got a few job opportunities (NLP, ML), and on each of the interviews, the company / team leader already had a set plan for which methods will be used for the problem.\n\nThe only options I see possible, as /u/hachidan05 already wrote, is aiming for startups/smaller companies, which usually have lower standards - and finding one that will allow you to do research for them.\n\nAlternatively, set up a strong GitHub account. Employers often check those things, and if you have coded SVM's, regression models, clustering from scratch (and applied it successfully to some known datasets), that could be proof enough of your skill.\n\n*My background - MSc in Computer Science - ML+NLP &amp; 1 year of work in the field*\n\nSince I'm kind of late to the party, I'll piggyback off of your comment and try to ask prof. Ng a question as well -\n\n1. Which European universities do you consider best for ML? Or, more specific, which professors do you consider the best in Europe?\n\nMy \"field of expertise\" is ML + NLP with a bit of information retrieval.\n\nI'm planning to apply for a PhD and this would be significant help - thank you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62886, "question": "If you want to do ML research, where research means developing new algorithms, methods, anything not in the already present \"ML cookbooks\" - you need a PhD (or a Master's and significant experience in research).\n\nFrom what I've heard from a few friends currently working in Facebook &amp; Google - there they don't let you touch research (or even ML) without a PhD in the field.\n\nFrom my personal experience, I got a few job opportunities (NLP, ML), and on each of the interviews, the company / team leader already had a set plan for which methods will be used for the problem.\n\nThe only options I see possible, as /u/hachidan05 already wrote, is aiming for startups/smaller companies, which usually have lower standards - and finding one that will allow you to do research for them.\n\nAlternatively, set up a strong GitHub account. Employers often check those things, and if you have coded SVM's, regression models, clustering from scratch (and applied it successfully to some known datasets), that could be proof enough of your skill.\n\n*My background - MSc in Computer Science - ML+NLP &amp; 1 year of work in the field*\n\nSince I'm kind of late to the party, I'll piggyback off of your comment and try to ask prof. Ng a question as well -\n\n1. Which European universities do you consider best for ML? Or, more specific, which professors do you consider the best in Europe?\n\nMy \"field of expertise\" is ML + NLP with a bit of information retrieval.\n\nI'm planning to apply for a PhD and this would be significant help - thank you.", "aSentId": 62887, "answer": "PHD opens doors, but not everyone has that requirement. I personally try to run a selection process that is as resume blind as possible. Using PHD as an argument for or against either hiring or utilizing someone, is something I have never said. But I have heard it from others. For example, my recruiters are much more likely to pass me a PHD, even though I tell them to stop it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62853, "question": "AMA Andrew Ng and Adam Coates", "aSentId": 62890, "answer": "Hinton seems to think that the next neural abstraction after the layer is the artificial cortical column. Have you done any work toward this end goal? \n\nAlso what are your thoughts on HTM and the CLA (Numenta)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62891, "question": "Hinton seems to think that the next neural abstraction after the layer is the artificial cortical column. Have you done any work toward this end goal? \n\nAlso what are your thoughts on HTM and the CLA (Numenta)", "aSentId": 62892, "answer": "Why did they ignore this question? It seems like plenty of others wanted to hear the answer.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62891, "question": "Hinton seems to think that the next neural abstraction after the layer is the artificial cortical column. Have you done any work toward this end goal? \n\nAlso what are your thoughts on HTM and the CLA (Numenta)", "aSentId": 62894, "answer": "Do you have any links to Geoff speaking on this?\n\nI'd like to understand it before the answering starts", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62891, "question": "Hinton seems to think that the next neural abstraction after the layer is the artificial cortical column. Have you done any work toward this end goal? \n\nAlso what are your thoughts on HTM and the CLA (Numenta)", "aSentId": 62896, "answer": "That sounds a lot more like Jeff Hawkins than Geoff Hinton. Are you sure you're not getting them mixed up?\n\nEdit: It looks like you did actually mean Hinton after all. Thanks /u/tabacof for clearing up the confusion. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62897, "question": "That sounds a lot more like Jeff Hawkins than Geoff Hinton. Are you sure you're not getting them mixed up?\n\nEdit: It looks like you did actually mean Hinton after all. Thanks /u/tabacof for clearing up the confusion. ", "aSentId": 62898, "answer": "I'm not sure why you're being downvoted without explanation. I'm confused about this as well. I read Jeff Hawkins book and thought this was his work. Can anybody explain? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62900, "question": "@andrewyng \n\nWhat kind of self projects and follow up courses would you recommend after the Coursera ML course?", "aSentId": 62901, "answer": "There seems to be some \"general ML wisdom\" which is not taught in courses like yours or Daphne Kollers PGM, but it enables people (experts) in the field to understand each other's research/presentations. How/where can one acquire this knowledge?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62905, "question": "Hi Andrew and Adam! Many thanks for taking the time for this!\n\n(1) What are your thoughts on the role that theory is to play in the future of ML, particularly as models grow in complexity? It often seems like that the gap between theory and practice is widening.\n\n(2) What are your thoughts on the future of unsupervised learning, especially now that (properly initialized and regularized) supervised techniques are leading the pack? Will layer-by-layer pretraining end up as a historical footnote?", "aSentId": 62906, "answer": "Hi Eldeemon, \n\nGreat question.  I think that 50 years ago, CS theory was really driving progress in CS practice. For example, the theoretical work figuring out that sorting is O(n log n), and Don Knuth's early books, really helped advance the field.  Today, there're some areas of theory that're still driving practice, such as computer security: If you find a flaw in crypto and publish a theoretical paper about it, this can cause code to be written all around the world. \n\nBut in machine learning, progress is increasingly driven by empirical work rather than theory.  Both still remain important (for example, I'm inspired by a lot of Yoshua Bengio's theoretical work), but in the future I hope we can do a better job connecting theory and practice. \n\nAs for unsupervised learning, I remain optimistic about it, but just have no idea what the right algorithm is.  I think layer-by-layer pretraining was a good first attempt.  But it really remains to be seen if researchers come up with something dramatically different in the coming years!  (I'm seeing some early signs of this.) ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62907, "question": "Hi Eldeemon, \n\nGreat question.  I think that 50 years ago, CS theory was really driving progress in CS practice. For example, the theoretical work figuring out that sorting is O(n log n), and Don Knuth's early books, really helped advance the field.  Today, there're some areas of theory that're still driving practice, such as computer security: If you find a flaw in crypto and publish a theoretical paper about it, this can cause code to be written all around the world. \n\nBut in machine learning, progress is increasingly driven by empirical work rather than theory.  Both still remain important (for example, I'm inspired by a lot of Yoshua Bengio's theoretical work), but in the future I hope we can do a better job connecting theory and practice. \n\nAs for unsupervised learning, I remain optimistic about it, but just have no idea what the right algorithm is.  I think layer-by-layer pretraining was a good first attempt.  But it really remains to be seen if researchers come up with something dramatically different in the coming years!  (I'm seeing some early signs of this.) ", "aSentId": 62908, "answer": "&gt; As for unsupervised learning, I remain optimistic about it, but just have no idea what the right algorithm is. I think layer-by-layer pretraining was a good first attempt. But it really remains to be seen if researchers come up with something dramatically different in the coming years! (I'm seeing some early signs of this.)\n\nCan you share those early signs with the rest of us?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62910, "question": "What are your thoughts on ML competitions (the most well-known example being kaggle)? And more generally, do you think gamification is beneficial to (ML) research?", "aSentId": 62911, "answer": "Is it just me that fears this? I am worried it will lead to talented people giving their skills and time away and driving their worth down. Why hire an expert when you can run a competition for the fraction of the cost?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62912, "question": "Is it just me that fears this? I am worried it will lead to talented people giving their skills and time away and driving their worth down. Why hire an expert when you can run a competition for the fraction of the cost?", "aSentId": 62913, "answer": "Isn't it just the reality of a market economy? If people are willing to give it away for free then the expert isn't worth her high fee.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62914, "question": "Isn't it just the reality of a market economy? If people are willing to give it away for free then the expert isn't worth her high fee.", "aSentId": 62915, "answer": "it's a dumb fear anyway. You can get to 90% of a winning kaggle score with sklearn or other off the shelf software, but you'll still need a professional to build a robust pipeline.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62916, "question": "it's a dumb fear anyway. You can get to 90% of a winning kaggle score with sklearn or other off the shelf software, but you'll still need a professional to build a robust pipeline.", "aSentId": 62917, "answer": "That's irrelevant - if the professional is willing to give it away it devalues everyone ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62918, "question": "That's irrelevant - if the professional is willing to give it away it devalues everyone ", "aSentId": 62919, "answer": "give what away?\n\nkaggle competitions are about taking preprocessed data and building an ungodly ensemble to squeeze every last drop from a performance metric.\n\nthe solutions are very different from a usable product.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62920, "question": "give what away?\n\nkaggle competitions are about taking preprocessed data and building an ungodly ensemble to squeeze every last drop from a performance metric.\n\nthe solutions are very different from a usable product.", "aSentId": 62921, "answer": "Couldn't agree with you more. Typically, the competition results provide a benchmark --- given the current data, what performance is feasible? --- and often, winners are invited to discuss their solutions with the company in question.\n\nI don't think it promotes 'research', because as you said, ungodly ensembles are often the winner. However, the gamification/competition aspect does seem to lead to the creation of new 'tricks': classifier ensembles that haven't been used before, hacks that reduce runtime, smart sampling or featurespace-reduction, etc.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62914, "question": "Isn't it just the reality of a market economy? If people are willing to give it away for free then the expert isn't worth her high fee.", "aSentId": 62923, "answer": "Exactly my thought yeah", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62925, "question": "Can you touch on what makes Baidu, in terms of the work being done (applications of the research etc.) and the work environment (autonomy, benefits, career growth / mobility etc.), a more attractive workplace for machine learning and AI engineers vs. other companies like Google or Facebook?", "aSentId": 62926, "answer": "I think Baidu, Google and Facebook are all great places to work! \n\nI don't want to compare Baidu against any other company (since I think they're all great).  But Baidu Research is very much a startup environment.  With ~40 people in our Silicon Valley team, we tend to act with the nimbleness of a startup of a commensurate size (albeit with the access to computational power and data of a $75B company).  We also invest a lot in employee development, and so I see that people here are all working hard and learning rapidly about deep learning, HPC, etc.  I think these things make the best possible combination for driving machine learning research, which is why both of us (Adam &amp; Andrew) had decided to join Baidu.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62930, "question": "Hey Andrew, huge fan of your work, mainly Machine Learning Coursera course that basically started my interest in ML area. \n\nQuestion: I have seen that your work is focused in DL, however I have not seen or read any work of yours focusing on Recurrent Neural Networks (RNN). Works in this area like the one that has been done by Schmidhuber with Long Short-Term Memories (LSTM)  are very famous and started to win some contests. Have you never thought about working and researching with RNNs? With your experience, can you point some pros and cons of RNNs?\n\nThanks a lot!", "aSentId": 62931, "answer": "I think RNNs are an exciting class of models for temporal data!  In fact, our recent breakthrough in speech recognition used bi-directional RNNs.  See http://bit.ly/deepspeech   We also considered LSTMs.  For our particular application, we found that the simplicity of RNNs (compared to LSTMs) allowed us to scale up to larger models, and thus we were able to get RNNs to perform better.  But at Baidu we are also applying LSTMs to a few problems were there is are longer-range dependencies in the temporal data. \n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62934, "question": "What do you wish you had known at the start of your career?", "aSentId": 62935, "answer": "One of the things both of us (Adam &amp; Andrew) talk about frequently is the impact of research.  At Baidu, our goal is to develop hard AI technologies that impact hundreds of millions of users.  Over time, I think we've both learned to be more strategic, and to learn to see more steps out ahead--beyond just writing a paper--to plot a path to seeing our technology benefit huge numbers of people.  These days, this is one of the things that really excite us about our work! ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62939, "question": "J\u00fcrgen Schmidhuber QUOTE: *\"Since BP was 3-5 decades old by then, and pattern deformations 2 decades, these results seemed to suggest that advances in exploiting modern computing hardware were more important than advances in algorithms.\"* [1]\n\nYann LeCun QUOTE: *\"Basically we limited by computational power. So, the faster, you know, the next generation of Nvidia GPU will be the more progress we'll make.\"* [2]\n\nWhat is your opinion about the matter?\n\n[1] Juergen Schmidhuber, 2014, Deep Learning in Neural Networks: An Overview\n\n[2] Yann LeCun, 2014, Convolutional Networks- Machine Learning for Computer Perception (Nvidia webinar, 2014) ", "aSentId": 62940, "answer": "&gt; [1] Juergen Schmidhuber, 2014, Deep Learning in Neural Networks: An Overview\n\nFor those interested, reference [1] points to section 5.18 (page 23)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62942, "question": "Do you think neural networks will continue to be the dominant paradigm in ML, or will we see a swing back to greater diversity, with things like Bayesian nonparametrics and deep architectures constructed out of non-NN layers?", "aSentId": 62943, "answer": "Linear/logistic regression and k-means clustering are probably the dominant paradigms in ML, and likely will always be. There's just too much bang for the buck.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62942, "question": "Do you think neural networks will continue to be the dominant paradigm in ML, or will we see a swing back to greater diversity, with things like Bayesian nonparametrics and deep architectures constructed out of non-NN layers?", "aSentId": 62945, "answer": "Neural networks are certainly not the dominant paradigm in ML.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62946, "question": "Neural networks are certainly not the dominant paradigm in ML.", "aSentId": 62947, "answer": "aren't they in terms of state-of-the-art progress on numerous tasks?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62948, "question": "aren't they in terms of state-of-the-art progress on numerous tasks?", "aSentId": 62949, "answer": "They are state-of-the-art on a few supervised tasks for which there is a large amount of data.  While that set of tasks is growing, machine learning is a large field and deep learning is still a relatively small part of it in both industry and academia.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62950, "question": "They are state-of-the-art on a few supervised tasks for which there is a large amount of data.  While that set of tasks is growing, machine learning is a large field and deep learning is still a relatively small part of it in both industry and academia.", "aSentId": 62951, "answer": "Computer vision, reinforcement learning and language processing are very broad subjects though.. Of course, DL is often used in combination with some other method, but it is very prominent nowadays. \n\nI'm not saying that you should just forget about Bayesian methods or manually constructed features, but, in my opinion, it is not unlikely that CNNs will be taught in CV courses soon.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62952, "question": "Computer vision, reinforcement learning and language processing are very broad subjects though.. Of course, DL is often used in combination with some other method, but it is very prominent nowadays. \n\nI'm not saying that you should just forget about Bayesian methods or manually constructed features, but, in my opinion, it is not unlikely that CNNs will be taught in CV courses soon.", "aSentId": 62953, "answer": "Sure.  Note that I did not say that neural networks were unimportant.  I said that it is incorrect to call them the dominant paradigm in machine learning.  I'd agree that CNNs will be taught in computer vision courses soon, and they obviously should be, but that doesn't change my point.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62954, "question": "Sure.  Note that I did not say that neural networks were unimportant.  I said that it is incorrect to call them the dominant paradigm in machine learning.  I'd agree that CNNs will be taught in computer vision courses soon, and they obviously should be, but that doesn't change my point.", "aSentId": 62955, "answer": "fair enough.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62952, "question": "Computer vision, reinforcement learning and language processing are very broad subjects though.. Of course, DL is often used in combination with some other method, but it is very prominent nowadays. \n\nI'm not saying that you should just forget about Bayesian methods or manually constructed features, but, in my opinion, it is not unlikely that CNNs will be taught in CV courses soon.", "aSentId": 62957, "answer": "There's a course at Stanford that was offered for the first time this year that is actually called \"Convolutional Neural Networks for Computer Vision\". I'm inclined to agree with you that NN is quickly becoming the dominant paradigm. I can think of a number of models in NLP that can be represented as one-hidden-layer neural networks even if they aren't taught as such, and I wouldn't be surprised if that was the case (implicit NNs) in other fields.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62946, "question": "Neural networks are certainly not the dominant paradigm in ML.", "aSentId": 62959, "answer": "Thank you! You'd never know it from this sub, since every other post is on some deep learning NN. In my field (computational biology) I never see enough data that would justify this approach. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62966, "question": "Is contrastive divergence still useful for training or has it been supplanted by other methods?", "aSentId": 62967, "answer": "In the early days of deep learning, Hinton had developed a few probabilistic deep learning algorithms such as Restricted Boltzmann Machines, which trained using contrastive divergence.  But these models were really complicated, and computing the normalization constant (partition function) was intractable, leading to really complex MCMC and other algorithms for training them. \n\nOver the next few years, we realized that these probabilistic formalisms didn't offer any advantage in most settings, but just added a lot of complexity.  Thus, almost all of deep learning has since moved away from these probabilistic formalisms, to instead use neural networks with deterministic computations.  One notable exception is that there're still a few groups (such as Ruslan Salakhutdinov's) doing very cool work on generative models using RBMs; but this is a minority.  Most of deep learning is now done using backpropagation, and contrastive divergence is very rarely used.  \n\nAs an aside, most of deep learning's successes today are due to supervised learning (trained with backprop).  Looking a little further out, I'm still very excited about the potential of unsupervised learning, since we have a lot more unlabeled data than labeled data; it's just that we just don't know what are the right algorithms are for unsupervised, and lots more research is needed here! ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62968, "question": "In the early days of deep learning, Hinton had developed a few probabilistic deep learning algorithms such as Restricted Boltzmann Machines, which trained using contrastive divergence.  But these models were really complicated, and computing the normalization constant (partition function) was intractable, leading to really complex MCMC and other algorithms for training them. \n\nOver the next few years, we realized that these probabilistic formalisms didn't offer any advantage in most settings, but just added a lot of complexity.  Thus, almost all of deep learning has since moved away from these probabilistic formalisms, to instead use neural networks with deterministic computations.  One notable exception is that there're still a few groups (such as Ruslan Salakhutdinov's) doing very cool work on generative models using RBMs; but this is a minority.  Most of deep learning is now done using backpropagation, and contrastive divergence is very rarely used.  \n\nAs an aside, most of deep learning's successes today are due to supervised learning (trained with backprop).  Looking a little further out, I'm still very excited about the potential of unsupervised learning, since we have a lot more unlabeled data than labeled data; it's just that we just don't know what are the right algorithms are for unsupervised, and lots more research is needed here! ", "aSentId": 62969, "answer": "Thanks! So are RBMs still the best for making generative models or even there auto-encoders, etc. are ahead?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62970, "question": "Thanks! So are RBMs still the best for making generative models or even there auto-encoders, etc. are ahead?", "aSentId": 62971, "answer": "I think that variational autoencoders have been getting the best results for generative modeling.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62972, "question": "I think that variational autoencoders have been getting the best results for generative modeling.  ", "aSentId": 62973, "answer": "How do you judge performance at generative modeling? Like, if the task is image recognition and you train the model on cats and dogs, and you ask for a cat, it spits something out, and then what? Does some person say \"yep that looks like a cat\"?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62974, "question": "How do you judge performance at generative modeling? Like, if the task is image recognition and you train the model on cats and dogs, and you ask for a cat, it spits something out, and then what? Does some person say \"yep that looks like a cat\"?", "aSentId": 62975, "answer": "So typically the model doesn't just give samples from the distribution p(x), it also lets you evaluate p(x).  So one evaluation metric is the observed values p(x) on the test data.  \n\nThis is actually kind of weak because: \n  -No one knows what a good likelihood is.  It's hard to interpret.  \n  -A model could make really good generative samples and not be good at estimating likelihood.  \n\nEvaluation metrics for generative models is definitely an area that could use work.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62980, "question": "@andrewyng\nFirst of all I wanted to say that your ML course on Coursera was amazing. Thank you!\n\n(1)\nHow much learning others helped you to develop your own skills in ML? You definitely put a lot of effort to prepare your online materials. Do you do this only to help others or maybe while preparing your materials you have also learned a lot - for example maybe you often investigated some concepts more deeply than you knew them before only because you wanted to explain them to others as clearly as possible.\n\n(2)\nYou have both outstanding academic and commercial experience.\nAre there any ML concepts or intuitions which are easier or faster to learn when you work for companies? And inversely - are there things which are easier / faster to learn in the academic world? I'm asking because lot of ML engineers seems to have PhD. So how is it helpful? Are those paths (commercial vs academic) somehow different?\n\n(3)\nWhich set of skills you find the most important in the ML field - is it practical application of ML, statistics or maybe domain knowledge of a particular problem?\nFor example lets assume that I want to develop a speech recognition system and I'm an expert in ML, but I do know nothing about audio processing. Do I have a chance to be successful?", "aSentId": 62981, "answer": "Thank you for taking the Coursera ML MOOC! \n\n(1) The old saw that teaching others helps you to learn really is true.  FWIW though I think one of the reasons I've had a few successes in research is because I'm a decent teacher.  This helps me to build a great team, and it's usually the team (not me) that comes up with many of the great ideas you see us publish and write about.  I think innovation often requires the combination of dozens of ideas from multiple team members, so I spend a lot of time trying to build that great team that can have those ideas.\n\n(2) A lot of deep learning progress is driven by computational scale, and by data.  For example, I think the bleeding edge of deep learning is shifting to HPC (high performance computing aka supercomputers), which is what we're working on at Baidu.  I've found it easier to build new HPC technologies and access huge amounts of data in a corporate context.  I hope that governments will increase funding of basic research, so as to make these resources easier for universities all around the world to get. \n\n(3) The skillset needed for different problems is different.  But broadly, the two sources of \"knowledge\" a program can have about a problem are (i) what you hand-engineer, and (ii) what it learns by itself from data.  In some fields (such as computer vision; and I predict increasingly so speech recognition and NLP in the future), the rapidly rising flood of data means that (ii) is now the dominant force, and thus the domain knowledge and the ability to hand-engineer little features is becoming less and less important.  5 years ago, it was really difficult to get involved in computer vision or speech recognition research, because there was a lot of domain knowledge you had to acquire.  But thanks to the rise of deep learning and the rise of data, I think the learning curve is now easier/shallower, because what's driving progress is machine learning+data, and it's now less critical to know about and be able to hand-engineer as many corner cases for these domains.  I'm probably over-simplifying a bit, but now the winning approach is increasingly to code up a learning algorithm, using only a modest amount of domain knowledge, and then to give it a ton of data, and let the algorithm figure things out from the data. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62985, "question": "A common weakness of Coursera and edX MOOCs is that they are watered down superficial versions of live courses. Students are not asked to solve any hard problems for fear of losing the audience, but as a result are not able to really learn the content of the course in a way that will allow them to apply it in real life scenarios. There are very few exceptions like Daphne Koller's PGM course or the ML course from Caltech on edX.\n\nDo you see any place for advanced Masters or PhD level courses on the Coursera platform, and if so, what steps are you taking to encourage their creation?", "aSentId": 62986, "answer": "My experience is the opposite. I have much more interaction with the material on Coursera and understand it better than I do in offline universities because of Coursera's automation.\n\nI see a very high correlation between making a course computer-based and my results in it. For me, barriers are lowered through the direct feedback of the quizzes and the automated direct assessment of your own code. Being able to selectively pause, rewind, and re-watch lectures has not been offered to me by offline universities and vastly improves understanding for me as well. When you're sitting in a hall with 100 students, I've found that fellow students *don't* like it when questions are asked, because everyone has different aspects they get stuck on, and what is unclear for you may well be clear to others. That's demotivating, but doesn't apply to Coursera.\n\nIn offline universities, it usually takes weeks for your results to get back to you, and by that time you've been put on new assignments already.\n\nI don't know if MOOCs will overtake offline universities, but I do know that they are more effective for me.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62987, "question": "My experience is the opposite. I have much more interaction with the material on Coursera and understand it better than I do in offline universities because of Coursera's automation.\n\nI see a very high correlation between making a course computer-based and my results in it. For me, barriers are lowered through the direct feedback of the quizzes and the automated direct assessment of your own code. Being able to selectively pause, rewind, and re-watch lectures has not been offered to me by offline universities and vastly improves understanding for me as well. When you're sitting in a hall with 100 students, I've found that fellow students *don't* like it when questions are asked, because everyone has different aspects they get stuck on, and what is unclear for you may well be clear to others. That's demotivating, but doesn't apply to Coursera.\n\nIn offline universities, it usually takes weeks for your results to get back to you, and by that time you've been put on new assignments already.\n\nI don't know if MOOCs will overtake offline universities, but I do know that they are more effective for me.", "aSentId": 62988, "answer": "Thanks for weighing in. I don't think we actually disagree. I also like all the things you mentioned about MOOCs. My comment relates to the level of the material that is presented and the difficulty of the homeworks. I've completed close to two dozen Coursera and edX courses now and only a couple come anywhere near the level of complexity of higher level undergrad or graduate courses. This has mostly to do with the fact that a typical homework takes the shape of a multiple choice quiz that gives you 100 tries and can be completed in half an hour with only a vague understanding of the material. An upper level university course on the other hand involves independent problem solving and development of ideas - activities that require you to incorporate course concepts into your working memory.\n\nI see the same Intro to Stats/Data Science or Single Variable Calculus popping up over and over again on Coursera but not a single Bayesian Inference, or Group Theory, or any other \"Insert Advanced Subject Here\". Having these would be quite nice as many on Coursera already have university degrees and are not well served by the innumerable introductory courses. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62989, "question": "Thanks for weighing in. I don't think we actually disagree. I also like all the things you mentioned about MOOCs. My comment relates to the level of the material that is presented and the difficulty of the homeworks. I've completed close to two dozen Coursera and edX courses now and only a couple come anywhere near the level of complexity of higher level undergrad or graduate courses. This has mostly to do with the fact that a typical homework takes the shape of a multiple choice quiz that gives you 100 tries and can be completed in half an hour with only a vague understanding of the material. An upper level university course on the other hand involves independent problem solving and development of ideas - activities that require you to incorporate course concepts into your working memory.\n\nI see the same Intro to Stats/Data Science or Single Variable Calculus popping up over and over again on Coursera but not a single Bayesian Inference, or Group Theory, or any other \"Insert Advanced Subject Here\". Having these would be quite nice as many on Coursera already have university degrees and are not well served by the innumerable introductory courses. ", "aSentId": 62990, "answer": "Well, our experiences certainly don't match up, but note that I'm not necessarily arguing from an objective basis, just relaying my own experiences.\n\nWhile there are ample courses that are less advanced on Coursera, the fact that they have the interaction I never got in university more than makes up for it for me. I'm fairly certain I simply learn more in any given Coursera course than offline course because of the reasons I've given.\n\n&gt; This has mostly to do with the fact that a typical homework takes the shape of a multiple choice quiz that gives you 100 tries and can be completed in half an hour with only a vague understanding of the material.\n\nTry a course where you do need to do so, especially the courses where you're encouraged to code up solutions to problems. Try guessing a number then or getting to the answer with only partial understanding...\n\n&gt; I see the same Intro to Stats/Data Science or Single Variable Calculus popping up over and over again on Coursera but not a single Bayesian Inference, or Group Theory, or any other \"Insert Advanced Subject Here\".\n\nI haven't even seen those (but I haven't looked since I'm supposed to be past that level anyway). Try cryptography. Algorithms. Ng's machine learning (did you do that one already?). Electrical engineering.\n\nI do agree that more advanced courses would be better though.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62989, "question": "Thanks for weighing in. I don't think we actually disagree. I also like all the things you mentioned about MOOCs. My comment relates to the level of the material that is presented and the difficulty of the homeworks. I've completed close to two dozen Coursera and edX courses now and only a couple come anywhere near the level of complexity of higher level undergrad or graduate courses. This has mostly to do with the fact that a typical homework takes the shape of a multiple choice quiz that gives you 100 tries and can be completed in half an hour with only a vague understanding of the material. An upper level university course on the other hand involves independent problem solving and development of ideas - activities that require you to incorporate course concepts into your working memory.\n\nI see the same Intro to Stats/Data Science or Single Variable Calculus popping up over and over again on Coursera but not a single Bayesian Inference, or Group Theory, or any other \"Insert Advanced Subject Here\". Having these would be quite nice as many on Coursera already have university degrees and are not well served by the innumerable introductory courses. ", "aSentId": 62992, "answer": "I think Udacity has some Bayesian inference stuff. \n\nThe absence of advanced mathematics beyond basic linear algebra is pretty disappointing, though my hope is that some recent books that have combined functional programming and discrete mathematics, logic, and proofs might translate into a MOOC within the next few years. And Sussman's written two books on using Scheme to learn classical mechanics and basic differential geometry. \n\nAnd edx has some graduate level physics courses like in Effective Field Theory and I think there was a basic functional analysis course offered. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 62985, "question": "A common weakness of Coursera and edX MOOCs is that they are watered down superficial versions of live courses. Students are not asked to solve any hard problems for fear of losing the audience, but as a result are not able to really learn the content of the course in a way that will allow them to apply it in real life scenarios. There are very few exceptions like Daphne Koller's PGM course or the ML course from Caltech on edX.\n\nDo you see any place for advanced Masters or PhD level courses on the Coursera platform, and if so, what steps are you taking to encourage their creation?", "aSentId": 62994, "answer": "I agree, one can see a great gap between Andrew's course taught in Stanford and the Coursera version. A lot of math is ommited, theories are simplified - that's surely a big disadvantage of MOOC (i've taken a few other courses and all of them had that lack-of-deep-math problem).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63006, "question": "This one is for Adam.\n\nYour work that I'm most familiar with was exploring/describing single layer networks that performed better than the more complex/deep learning learning methods of the time on the CIFAR dataset.\n\nDo you think that simpler configurations are possible that can compete with todays large network performance? Would it only be for certain dataset configurations that are difficult for large networks and their variants?\n\nThanks!", "aSentId": 63007, "answer": "One of the reasons we looked at single layer networks was so that we could rapidly explore a lot of characteristics that we felt could influence how these models performed without a lot of the complexity that deep networks brought at the time (e.g., needing to train layer-by-layer).  There is lots of evidence (empirical and theoretical) today, however, that deep networks can represent far more complex functions than shallow ones and, thus, to make use of the very large training datasets available, it is probably important to continue using large/deep networks for these problems.\n\nThankfully, while deep networks can be tricky to get working compared to some of the simplest models in 2011, today we have the benefit of much better tools and faster computers --- this lets us iterate quickly and explore in a way that we couldn't do in 2011.  In some sense, building better systems for DL has enabled us to explore large, deep models at a pace similar to what we could do in 2011 only for very simple models.  This is one of the reasons we invest a lot in systems research for deep learning here in the AI Lab:  the faster we are able to run experiments, the more rapidly we can learn, and the easier it is to find models that are successful and understand all of the trade-offs.   \n\nSometimes the \"best\" model ends up being a bit more complex than we want, but the good news is that the *process* of finding these models has been simplified a lot!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63008, "question": "One of the reasons we looked at single layer networks was so that we could rapidly explore a lot of characteristics that we felt could influence how these models performed without a lot of the complexity that deep networks brought at the time (e.g., needing to train layer-by-layer).  There is lots of evidence (empirical and theoretical) today, however, that deep networks can represent far more complex functions than shallow ones and, thus, to make use of the very large training datasets available, it is probably important to continue using large/deep networks for these problems.\n\nThankfully, while deep networks can be tricky to get working compared to some of the simplest models in 2011, today we have the benefit of much better tools and faster computers --- this lets us iterate quickly and explore in a way that we couldn't do in 2011.  In some sense, building better systems for DL has enabled us to explore large, deep models at a pace similar to what we could do in 2011 only for very simple models.  This is one of the reasons we invest a lot in systems research for deep learning here in the AI Lab:  the faster we are able to run experiments, the more rapidly we can learn, and the easier it is to find models that are successful and understand all of the trade-offs.   \n\nSometimes the \"best\" model ends up being a bit more complex than we want, but the good news is that the *process* of finding these models has been simplified a lot!", "aSentId": 63009, "answer": "Hi Adam!\nI have a follow up question regarding your answer. Do you have any recommended reading for the *process* of finding models? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63012, "question": "If you would have 1000 times the memory (disk/ram) available compared to what you've used so far, what technique would become viable that is currently not, if any? What about 1000000?\n\nIf you would have 1000 times the processing power (in parallel) available compared to what you've used so far, what technique would become viable that is currently not, if any? What about 1000000?\n\nIf you would have 1000 times the processing power (not in parallel, so pure speed/hz) available compared to what you've used so far, what technique would become viable that is currently not, if any? What about 1000000?", "aSentId": 63013, "answer": "ITT: Same techniques, bigger nets.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63017, "question": "Hi Andrew, I have followed your work with interest and audited a few of your machine learning courses online.  They have been an incredible resource.  I actually made use of your homework exercises on the sparse autoencoder in my research on neural activity. So thanks for your dedication to education!  I wanted to ask: when you are confronted with a large/high-dimensional/complex data set, what are the main early considerations that you use in determining what family of learning algorithms you will try with it?  Do you have a recommended standard approach (e.g. start simple and linear and move to more complex techniques if those fail?) or are there things that you might notice in a data set that suggest that particular types of algorithms might be really well suited?", "aSentId": 63018, "answer": "He mentions in the Coursera course that he does start simple, plotting learning curves and such.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63017, "question": "Hi Andrew, I have followed your work with interest and audited a few of your machine learning courses online.  They have been an incredible resource.  I actually made use of your homework exercises on the sparse autoencoder in my research on neural activity. So thanks for your dedication to education!  I wanted to ask: when you are confronted with a large/high-dimensional/complex data set, what are the main early considerations that you use in determining what family of learning algorithms you will try with it?  Do you have a recommended standard approach (e.g. start simple and linear and move to more complex techniques if those fail?) or are there things that you might notice in a data set that suggest that particular types of algorithms might be really well suited?", "aSentId": 63020, "answer": "Pick the simplest model that can actually learn your function.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63050, "question": "Dear Dr. Ng and Dr. Coates,\nWould you please give suggestions on what are the next coursera classes (or other online resources) to take, after Dr. Ng's ML class, for beginners to become more proficient in ML? I understand it probably depends on the learning purpose. I want to use ML to handle some new metabolomics data, where the features are largely unknown.\nThanks, Jen", "aSentId": 63051, "answer": "Dear Andrew, \nOn a separate note, would you please tell us how to correctly pronounce your last name? :) Thanks!\nJen", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63066, "question": "Hi Adam\n\nWhich framework, e.g. Hadoop Map Reduce, Spark, did you use for distributing the tasks between CPUs/GPUs for training billions of connections size neural net?", "aSentId": 63067, "answer": "Many of the current frameworks out there for large scale computation are very successful for problems involving huge amounts of data and relatively less computation.  One of the things I worked on with Bryan Catanzaro and Andrew was how to do distributed computation for deep learning using tools/techniques that are specifically meant to handle very intense computational problems (like MPI/CUDA that come from the HPC/supercomputing world).  There's more in our paper on that topic here:  http://stanford.io/1JHzBwx\n\nSince a lot of the HPC tools ecosystem isn't as well developed for our problems, in the AI Lab / at Baidu Research the systems team is building a platform that let's DL researchers build experiments rapidly (like Hadoop/Spark do for cloud systems) but that run much faster!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63068, "question": "Many of the current frameworks out there for large scale computation are very successful for problems involving huge amounts of data and relatively less computation.  One of the things I worked on with Bryan Catanzaro and Andrew was how to do distributed computation for deep learning using tools/techniques that are specifically meant to handle very intense computational problems (like MPI/CUDA that come from the HPC/supercomputing world).  There's more in our paper on that topic here:  http://stanford.io/1JHzBwx\n\nSince a lot of the HPC tools ecosystem isn't as well developed for our problems, in the AI Lab / at Baidu Research the systems team is building a platform that let's DL researchers build experiments rapidly (like Hadoop/Spark do for cloud systems) but that run much faster!", "aSentId": 63069, "answer": "Thanks!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63107, "question": "Did you feel uncomfortable working for Baidu during their involvement in the GitHub ddos?", "aSentId": 63108, "answer": "Why would he answer that, it contributes nothing. \n\nThey weren't even involved. You should [get informed](http://krebsonsecurity.com/2015/04/dont-be-fodder-for-chinas-great-cannon/) before you make yourself look very dumb.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63109, "question": "Why would he answer that, it contributes nothing. \n\nThey weren't even involved. You should [get informed](http://krebsonsecurity.com/2015/04/dont-be-fodder-for-chinas-great-cannon/) before you make yourself look very dumb.", "aSentId": 63110, "answer": "Hadn't read that content. I appreciate the link.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63109, "question": "Why would he answer that, it contributes nothing. \n\nThey weren't even involved. You should [get informed](http://krebsonsecurity.com/2015/04/dont-be-fodder-for-chinas-great-cannon/) before you make yourself look very dumb.", "aSentId": 63112, "answer": "In all fairness, Baidu were at least negligent, but definitely not responsible.\n\nTheir code should have been served over HTTPS, making a man-in-the-middle attack at least another level of difficulty. As it is, their inaction allowed their code to be subverted.\n\nBut yes, possibly off-topic for this thread.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63146, "question": "A new Favourite Machine Learning Paper: Autoencoders as Probabilistic Models", "aSentId": 63147, "answer": "That hack of throwing computational power at a problem instead of modelling the priors and invariances is a pretty good one for being just a hack. If I can solve problems with less of my time and more CPU time then isn't that a good thing? Isn't that what Machine Learning is all about? Instead of coding something from scratch, we have a computer try to learn the problem.\n\nP.S. It was a great article.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63148, "question": "That hack of throwing computational power at a problem instead of modelling the priors and invariances is a pretty good one for being just a hack. If I can solve problems with less of my time and more CPU time then isn't that a good thing? Isn't that what Machine Learning is all about? Instead of coding something from scratch, we have a computer try to learn the problem.\n\nP.S. It was a great article.", "aSentId": 63149, "answer": "Not necessarily, imagine if back in Newton's day, the were analyzing data from physical random variables with deep nets.  Sure, they might get great prediction accuracy on how far a ball will go given measurements of its weight, initial force/angle, and some other irrelevant variables, but would this really be the best approach to discover all of the useful laws of physics such as f = ma and the conversion from potential to kinetic energy via the gravitational constant?  Probably not, in fact the predictions might be in some sense \"too good\" incorporating other confounding effects such as air drag and the shape / spin of the ball which obfuscate the desired law.\nIn many settings where an interpretation of what is going on in the data is desired, a clear model is necessary with simple knobs that have clear effects when turned.   This may also be a requirement not only for human interpretation, but an also AI system which is able to learn and combine facts about the world (rather than only storing the complex functions which represent the relationships between things as inferred by a deep-net).  \n\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63150, "question": "Not necessarily, imagine if back in Newton's day, the were analyzing data from physical random variables with deep nets.  Sure, they might get great prediction accuracy on how far a ball will go given measurements of its weight, initial force/angle, and some other irrelevant variables, but would this really be the best approach to discover all of the useful laws of physics such as f = ma and the conversion from potential to kinetic energy via the gravitational constant?  Probably not, in fact the predictions might be in some sense \"too good\" incorporating other confounding effects such as air drag and the shape / spin of the ball which obfuscate the desired law.\nIn many settings where an interpretation of what is going on in the data is desired, a clear model is necessary with simple knobs that have clear effects when turned.   This may also be a requirement not only for human interpretation, but an also AI system which is able to learn and combine facts about the world (rather than only storing the complex functions which represent the relationships between things as inferred by a deep-net).  \n\n\n", "aSentId": 63151, "answer": "This is true insofar as one seeks a declarative understanding of the world. But each of us has an internal model of how physical processes like gravity behave (we know to anticipate an impact when something is traveling towards us, we can trace out the likely trajectory of a ball hurtling through the air given its previous velocity and position), and it's one that we have learned long before we had the help of declarative understanding imparted in high school physics classes.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63152, "question": "This is true insofar as one seeks a declarative understanding of the world. But each of us has an internal model of how physical processes like gravity behave (we know to anticipate an impact when something is traveling towards us, we can trace out the likely trajectory of a ball hurtling through the air given its previous velocity and position), and it's one that we have learned long before we had the help of declarative understanding imparted in high school physics classes.", "aSentId": 63153, "answer": "I don't think it's only useful for declarative understanding. The human brain has priors that constrain what it can learn. Visual illusions are good examples - some of these are result of learning, some of them a result of evolution.\n\nThese priors certainly help children learn from the information they have at hand, that they might not be able to learn otherwise.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63150, "question": "Not necessarily, imagine if back in Newton's day, the were analyzing data from physical random variables with deep nets.  Sure, they might get great prediction accuracy on how far a ball will go given measurements of its weight, initial force/angle, and some other irrelevant variables, but would this really be the best approach to discover all of the useful laws of physics such as f = ma and the conversion from potential to kinetic energy via the gravitational constant?  Probably not, in fact the predictions might be in some sense \"too good\" incorporating other confounding effects such as air drag and the shape / spin of the ball which obfuscate the desired law.\nIn many settings where an interpretation of what is going on in the data is desired, a clear model is necessary with simple knobs that have clear effects when turned.   This may also be a requirement not only for human interpretation, but an also AI system which is able to learn and combine facts about the world (rather than only storing the complex functions which represent the relationships between things as inferred by a deep-net).  \n\n\n", "aSentId": 63155, "answer": "Devil's advocate here... Instead of me trying to learn the laws of physics and incorporate it into a model, why don't I have a computer learn it for me? Again, isn't this what Machine Learning is all about? .... Granted, it take a mighty powerful computer, and might require new algorithms that have not been invented yet.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63156, "question": "Devil's advocate here... Instead of me trying to learn the laws of physics and incorporate it into a model, why don't I have a computer learn it for me? Again, isn't this what Machine Learning is all about? .... Granted, it take a mighty powerful computer, and might require new algorithms that have not been invented yet.", "aSentId": 63157, "answer": "No, this I don't think this is what machine learning is about. There is no such thing as learning without priors. In the simplest form, the objective function of the optimisation is a prior - you tell the machine that it's goal is to minimise mean squared error for example. The machine solves the optimisation problem (typically) you tell it to solve, and good machine learning is about figuring out what that problem is. Priors are part of that.\n\nSecondly, if you think about it, it is actually a tiny portion of machine learning problems where you actually have enough data to get away without engineering better priors or architectures by just using a model which is highly flexible. Today, you can do this in visual, audio, video domain because you can collect and learn from tonnes of examples and particularly because you can use unsupervised or semi-supervised learning to learn natural invariances. An example is chemistry: if you want to predict certain properties of chemicals, it almost doesn't make sense to use data only to make the machine learn what a chemical is, and what the invariances are - doing that would be less accurate and a lot harder than giving it the required context. Un- and semi-supervised learning doesn't make sense because in many cases learning about the natural distribution of chemicals (even if you had a large dataset of this) may be uninformative of the prediction tasks you want to solve.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63146, "question": "A new Favourite Machine Learning Paper: Autoencoders as Probabilistic Models", "aSentId": 63159, "answer": "Some of its formulas are broken?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63160, "question": "Some of its formulas are broken?", "aSentId": 63161, "answer": "Thanks, fixed now.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63163, "question": "Benefit of Unsupervised learning related to Supervised learning", "aSentId": 63164, "answer": "Interesting formulation. In reality, I'd just try both and see which approach empirically does better on a holdout truth set made from a small sample of the supervised data.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63166, "question": "A Group Theoretic Perspective on Unsupervised Deep Learning", "aSentId": 63167, "answer": "The group theoretic perspective is surprisingly terse.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63168, "question": "The group theoretic perspective is surprisingly terse.  ", "aSentId": 63169, "answer": "That's because OP linked to the extended abstract instead of the [actual paper](http://arxiv.org/abs/1412.6621).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63166, "question": "A Group Theoretic Perspective on Unsupervised Deep Learning", "aSentId": 63171, "answer": "If this is true that would be amazing.  Group theory is one of the most abstract areas of pure math, and as I understand  has had very little use in science so far, so to think that it would play a key role in neural network learning would be pretty interesting.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63172, "question": "If this is true that would be amazing.  Group theory is one of the most abstract areas of pure math, and as I understand  has had very little use in science so far, so to think that it would play a key role in neural network learning would be pretty interesting.", "aSentId": 63173, "answer": "um it's kinda the basis of particle physics, not to mention solid state physics and chemistry.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63175, "question": "oxnn - Oxford NN Library", "aSentId": 63176, "answer": "thank you ! ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63182, "question": "Two Python deep learning Theano-based toolkits you haven't heard of", "aSentId": 63183, "answer": "I wish these frameworks would spend more time on making it obvious how to load your own data versus wrapping a bunch of benchmark datasets.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63182, "question": "Two Python deep learning Theano-based toolkits you haven't heard of", "aSentId": 63185, "answer": "You also probably have not heard of http://github.com/breze-no-salt/breze, even though it is 3 years old already.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63186, "question": "You also probably have not heard of http://github.com/breze-no-salt/breze, even though it is 3 years old already.", "aSentId": 63187, "answer": "README and some docs, in addition to the notebooks, would help.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63182, "question": "Two Python deep learning Theano-based toolkits you haven't heard of", "aSentId": 63189, "answer": "Boy, we really don't suffer from a lack of theano based libraries these days.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63190, "question": "Boy, we really don't suffer from a lack of theano based libraries these days.", "aSentId": 63191, "answer": "I'd like to see the most popular 3-5 compared and contrasted, the maturity especially.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63192, "question": "I'd like to see the most popular 3-5 compared and contrasted, the maturity especially.", "aSentId": 63193, "answer": "Yea hopefully fastml jumps on that.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63190, "question": "Boy, we really don't suffer from a lack of theano based libraries these days.", "aSentId": 63195, "answer": "It reminds me of the early Python web framework days. There was an explosion in new web frameworks, which lasted a few years. It helped the community a lot to figure out how things are done best, because different frameworks did things quite different.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63190, "question": "Boy, we really don't suffer from a lack of theano based libraries these days.", "aSentId": 63197, "answer": "I found writing a theano-based NN library to be a nice way to learn theano. But yeah, it sounds like there are way more theano-based NN libraries than we need. Why not just use the pylearn2 lib straight from the source? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63199, "question": "Save me from Theano... \n[/Headaches making the damn thing work/compile on a windows PC]", "aSentId": 63200, "answer": "Sounds like you need someone to save you from Windows.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63201, "question": "Sounds like you need someone to save you from Windows.", "aSentId": 63202, "answer": "Yeah, since the solution to \"This is convoluted and complicated, compared to the rest of the options\" is \"get a seperate operating system\". \nAdding Python 2.7 (instead of 3+ is one thing. Installing some common sense dependencies another. Working out what type of visual studio 12+ compilation or where to add the .theano.rc file, or various weird hacks that might help with weird errors is beyond my patience or knowledge, alass). ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63203, "question": "Yeah, since the solution to \"This is convoluted and complicated, compared to the rest of the options\" is \"get a seperate operating system\". \nAdding Python 2.7 (instead of 3+ is one thing. Installing some common sense dependencies another. Working out what type of visual studio 12+ compilation or where to add the .theano.rc file, or various weird hacks that might help with weird errors is beyond my patience or knowledge, alass). ", "aSentId": 63204, "answer": "if you want to do real computing, use a real operating system.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63199, "question": "Save me from Theano... \n[/Headaches making the damn thing work/compile on a windows PC]", "aSentId": 63206, "answer": "I've gotten cuda/theano up and running on two win7x64 machines in the last week, there are a couple bugaboos but nothing too bad. What problems are you having?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63207, "question": "I've gotten cuda/theano up and running on two win7x64 machines in the last week, there are a couple bugaboos but nothing too bad. What problems are you having?", "aSentId": 63208, "answer": "The darn thing refuses to work. \nI managed to get it working on one pc after a few days (the missing magic to get it working was installing \"libpython\", after I'd tried all the acronym soup of minGW, Msys, gcc, etc' ) .  Said PC has an AMD GPU, so the whole thing is slow as molasses (as compared to what i'm used to from scikit-learn's multicore speedup..)\n\nOn my laptop which actually has a Nvidia GPU (and CUDA), it gives various errors when I try importing it. (Yes, I have installed Visual studio, gcc, etc'). \n\nI'm tired of the massive amount of time I put into trying to get it working, It's just a shame that so many nice libraries (Keras, Lasagna) are built on it. \n(I'm also not a \"native programmer\". e.g. \"Add to system path\" when trying to add cuDNN for example is just chinese to me when it comes to windows). ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63209, "question": "The darn thing refuses to work. \nI managed to get it working on one pc after a few days (the missing magic to get it working was installing \"libpython\", after I'd tried all the acronym soup of minGW, Msys, gcc, etc' ) .  Said PC has an AMD GPU, so the whole thing is slow as molasses (as compared to what i'm used to from scikit-learn's multicore speedup..)\n\nOn my laptop which actually has a Nvidia GPU (and CUDA), it gives various errors when I try importing it. (Yes, I have installed Visual studio, gcc, etc'). \n\nI'm tired of the massive amount of time I put into trying to get it working, It's just a shame that so many nice libraries (Keras, Lasagna) are built on it. \n(I'm also not a \"native programmer\". e.g. \"Add to system path\" when trying to add cuDNN for example is just chinese to me when it comes to windows). ", "aSentId": 63210, "answer": "how surprising, a python library requires python to be installed..", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63213, "question": "ArkReddit: Is anyone doing serious research using python3?", "aSentId": 63214, "answer": "scikit-learn is fully python3. Theano is under 2to3, and working toward full. I am pretty sure scipy is fully python 3.\n\nI use python 3 exclusively, though I make sure my code works in both as much as possible. It is 2015 - as python devs we should be pushing forward, not holding things back IMO. And for most scientific code, the changes are really easy. Like print(x) instead of print x, no xrange, float division by default. cPickle imports, urllib2, a few others.\n\nReally, there is no reason not to just learn to code python 3. Same exact language (give up print x already!), with a lot of upside. Moving old packages is obviously a chore, but so is all programming! And you get a chance to clean up code, strengthen tests, etc. along the way.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63215, "question": "scikit-learn is fully python3. Theano is under 2to3, and working toward full. I am pretty sure scipy is fully python 3.\n\nI use python 3 exclusively, though I make sure my code works in both as much as possible. It is 2015 - as python devs we should be pushing forward, not holding things back IMO. And for most scientific code, the changes are really easy. Like print(x) instead of print x, no xrange, float division by default. cPickle imports, urllib2, a few others.\n\nReally, there is no reason not to just learn to code python 3. Same exact language (give up print x already!), with a lot of upside. Moving old packages is obviously a chore, but so is all programming! And you get a chance to clean up code, strengthen tests, etc. along the way.", "aSentId": 63216, "answer": "right, exactly the sort of code that 2to3 will fix, and it will work no questions asked.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63217, "question": "right, exactly the sort of code that 2to3 will fix, and it will work no questions asked.", "aSentId": 63218, "answer": "Well 2to3 makes it really annoying to do development from Python 3 (see Theano dev under 3.4). You *can* do it, but it is much more of a hassle than it needs to be. It also isn't 100% perfect - some things need to fixed or handled manually.\n\nIt is much better to have a codebase that supports both natively (with the addition of six), and unless you are dealing with string support it is pretty easy.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63215, "question": "scikit-learn is fully python3. Theano is under 2to3, and working toward full. I am pretty sure scipy is fully python 3.\n\nI use python 3 exclusively, though I make sure my code works in both as much as possible. It is 2015 - as python devs we should be pushing forward, not holding things back IMO. And for most scientific code, the changes are really easy. Like print(x) instead of print x, no xrange, float division by default. cPickle imports, urllib2, a few others.\n\nReally, there is no reason not to just learn to code python 3. Same exact language (give up print x already!), with a lot of upside. Moving old packages is obviously a chore, but so is all programming! And you get a chance to clean up code, strengthen tests, etc. along the way.", "aSentId": 63220, "answer": "&gt;(give up print x already!)  \n\nMOLON LABE", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63221, "question": "&gt;(give up print x already!)  \n\nMOLON LABE", "aSentId": 63222, "answer": "I would probably switch, if not for print x. Seriously. There should be \n`from past import print` or something like that.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63215, "question": "scikit-learn is fully python3. Theano is under 2to3, and working toward full. I am pretty sure scipy is fully python 3.\n\nI use python 3 exclusively, though I make sure my code works in both as much as possible. It is 2015 - as python devs we should be pushing forward, not holding things back IMO. And for most scientific code, the changes are really easy. Like print(x) instead of print x, no xrange, float division by default. cPickle imports, urllib2, a few others.\n\nReally, there is no reason not to just learn to code python 3. Same exact language (give up print x already!), with a lot of upside. Moving old packages is obviously a chore, but so is all programming! And you get a chance to clean up code, strengthen tests, etc. along the way.", "aSentId": 63224, "answer": "&gt; (give up print x already!)\n\nI write a ton of small scripts that end up being predominantly print statements. fuck python 3.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63225, "question": "&gt; (give up print x already!)\n\nI write a ton of small scripts that end up being predominantly print statements. fuck python 3.", "aSentId": 63226, "answer": "Really? It is a one day switch to write your future code the right way - and probably one call to 2to3 for the old. I respect your feelings on the matter, but I also think you are dead wrong.\n\nTo be frank, if it bothers you that much another language might be a good head start - python 3 as default is coming (see Debian), it is just a matter of time. Both Torch (Lua) and Julia seem like promising alternatives for machine learning.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63227, "question": "Really? It is a one day switch to write your future code the right way - and probably one call to 2to3 for the old. I respect your feelings on the matter, but I also think you are dead wrong.\n\nTo be frank, if it bothers you that much another language might be a good head start - python 3 as default is coming (see Debian), it is just a matter of time. Both Torch (Lua) and Julia seem like promising alternatives for machine learning.", "aSentId": 63228, "answer": "I know what python 3 print statements look like, I prefer the painless 2 alternative. I still use % over format as well.\n\n&gt; Both Torch (Lua) and Julia seem like promising alternatives for machine learning.\n\nFuck any language that uses 1-based indexing and doesn't have integers.\n\nI've been looking for an alternative, and I think it might be rust or nim.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63229, "question": "I know what python 3 print statements look like, I prefer the painless 2 alternative. I still use % over format as well.\n\n&gt; Both Torch (Lua) and Julia seem like promising alternatives for machine learning.\n\nFuck any language that uses 1-based indexing and doesn't have integers.\n\nI've been looking for an alternative, and I think it might be rust or nim.", "aSentId": 63230, "answer": "julia doesn't have integers?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63213, "question": "ArkReddit: Is anyone doing serious research using python3?", "aSentId": 63232, "answer": "I'm using Python3 exclusively and I make extensive use of numpy, scipy, scikit-learn, pandas, pycuda, scikits.cuda, ipython, matplotlib and a couple of others all the time. I can't think of a single dependency that is still stuck at 2.x.  Taking the plunge to switch to py3 was actually a huge lot less painful than I initially thought. It was actually very... uneventful. Read up on the few language changes, set up the environment and start `ipython3` instead of `ipython`, and you're done. `2to3` does most of the work for you if you need to bring any code over (I personally found `python-modernize` to be a much better tool for the job, as it makes code both py2 and py3 compatible in one go). You'll need to form a few new habits, which takes a week or two: the parenthesis around print, the new division (which I think is hugely better than the old one), and if you do much string handling, then there's that . Occasionally you'll stumble over py2 pickle files that contain strings and need a bit of extra care, but that's it.\n\nI have to agree with /u/kkastner: There's no real reason not to use Py3 right now, and it's goddamn time for everyone to switch. It's also extremely easy to keep all your work both py2 and py3 compatible (just start each new file with `from __future__ import division, print_function` and always pretend you're writing py3. In the few cases when that's not enough, `six` helps), and that's what I do just in case I need to run my code in a py2 environment; One of our older machines only has python 2.6 and all of my code that I've developed in py3 still runs flawlessly there.\n\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63213, "question": "ArkReddit: Is anyone doing serious research using python3?", "aSentId": 63234, "answer": "I'm using Python 3 too, mainly with pandas, numpy, sklearn and Seaborn at the moment. Haven't had any major issues: some less popular libraries don't work or only partially work, but these aren't ones I use regularly. The syntax is nicer and having better Unicode support is great. (And want to use Greek variable names in your notebook? Well, you can. :)\n\nI have found it better to install the Python 2 Anaconda distribution and make Python 3 virtualenvs, as installers for other software may use the default Anaconda Python and break otherwise. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63236, "question": "[Question] How to handle large (10GB) datasets?", "aSentId": 63237, "answer": "Chances are you don't need the full dataset at least at first. Randomly sample 10% of it and use that.\n\n10GB is also not that much, you can rent an AWS instances with up to 240GB RAM.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63238, "question": "Chances are you don't need the full dataset at least at first. Randomly sample 10% of it and use that.\n\n10GB is also not that much, you can rent an AWS instances with up to 240GB RAM.", "aSentId": 63239, "answer": "Subsampling is your friend! As a rule of thumb, any analysis you do on a good subsample should lead to similar results if you would have done it on the whole data set. \"Good\" is the tricky part -- subsampling well depends on the questions you plan to ask. For example, you might want to subsample 10% of unique players, or 10% of unique matches, rather than just 10% of the data. Also, if you have any cleaning steps that you take, you might want to run them *before* subsampling. For example, if you know that 25% of matches end prematurely due to a player disconnecting (I'm guessing here, not that experienced with LoL), you might want to clean these and then subsample.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63238, "question": "Chances are you don't need the full dataset at least at first. Randomly sample 10% of it and use that.\n\n10GB is also not that much, you can rent an AWS instances with up to 240GB RAM.", "aSentId": 63241, "answer": "&gt;AWS instances with up to 240GB RAM.  \n\nHuh. Should have realized this. Although my laptop has 32GB so I'm ok in almost all instances.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63236, "question": "[Question] How to handle large (10GB) datasets?", "aSentId": 63243, "answer": "hdf5 (using PyTables) is quite nice, especially in conjunction with compression filters. I also have an outstanding request from/u/bennane to show a simple example... will post back later today!\n\nalso, what /u/j1395010 said is very important - but ultimately for your final model you (probably) want to train on the whole dataset. So subsampling helps during model development, but at the end you probably have the same issue.\n\nWhat are you using for the machine learning part? There are ways in both scikit-learn and other libraries to use hdf5, memmaps and other things which are just iterators.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63236, "question": "[Question] How to handle large (10GB) datasets?", "aSentId": 63245, "answer": "Along with j1395010's comment, you may be able to store the info in a more efficient fashion than dicts and lists using numpy or pandas (depending on the data).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63250, "question": "Are there any rules of thumb for selecting the number of hidden layers and neurons per layer in a deep net?", "aSentId": 63251, "answer": "I think it's mostly empirical in modern practice. Geoff Hinton would advocate making the net as big as possible, using dropout, and then if the resulting net is too slow for deployment, using distilling to create a small net from it.\n\nStill, it's useful to have some idea as to how big a net one needs without regularization to fit the data well. I vaguely remember reading about this heuristic (but I forget where I saw it) :\n\n&gt; You should have about 1 (floating point) parameter per 1 bit of information in your training set\n\nFor example, the MNIST training set labels contain 60e3 * log2(10) = 2e5 bits of information. OTOH a 784-1000-10 net contains 8e5 parameters, so if this heuristic is correct, it's enough to start overfitting, unless you use regularization.\n\nIf anyone remembers where the above heuristic is from, or what the theoretical justifications are, I'd like to see them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63250, "question": "Are there any rules of thumb for selecting the number of hidden layers and neurons per layer in a deep net?", "aSentId": 63253, "answer": "The bias is decreasing along with the size of the net, however, the variance is increasing.\n\nTherefore, the answer depends on how much data do you have:\n\nIn case of large volume of data, the big networks with regularisation provide better results than the small ones.\n\nIn case of insufficient data, the big networks almost inevitably will face overfitting (if no regularisation).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63254, "question": "The bias is decreasing along with the size of the net, however, the variance is increasing.\n\nTherefore, the answer depends on how much data do you have:\n\nIn case of large volume of data, the big networks with regularisation provide better results than the small ones.\n\nIn case of insufficient data, the big networks almost inevitably will face overfitting (if no regularisation).", "aSentId": 63255, "answer": "These are some good principles, but I have no sense of what \"big\" is. If I have 500 input units and 1 output unit, how many layers should I have and how many per layer? Is it a good idea to have the first layer have more neurons than the input layer? etc...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63256, "question": "These are some good principles, but I have no sense of what \"big\" is. If I have 500 input units and 1 output unit, how many layers should I have and how many per layer? Is it a good idea to have the first layer have more neurons than the input layer? etc...", "aSentId": 63257, "answer": "From my point of view, the best way of action is to start from big model (as big ar your computational capacity allows) and then tune regularisation in order to minimise validation error.\nThen, learning curve will give the idea how large is the variation.\nIf it's huge - down to the simpler model and repeat.\nBut i wolud not suggest this way of actions for any data sets without some additional information.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63250, "question": "Are there any rules of thumb for selecting the number of hidden layers and neurons per layer in a deep net?", "aSentId": 63259, "answer": "Any time I work with any kind of NN, I usually start with the number of hidden nodes equal to the average of the input and output layer nodes, and then tweak and test from there.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63261, "question": "I've read that one hidden layer is as effective as many layers, so if one layer can simplify things why use multiple. The node count is what matters.", "aSentId": 63262, "answer": "I think you misunderstood. Given a nn with n layers there exists a nn with one layer that is equivalent. But the one layer nn will have many more nodes.\n\nThat's the gist anyway I'm sure the actual result is more technical.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63263, "question": "I think you misunderstood. Given a nn with n layers there exists a nn with one layer that is equivalent. But the one layer nn will have many more nodes.\n\nThat's the gist anyway I'm sure the actual result is more technical.", "aSentId": 63264, "answer": "Yep, you're totally right.  The \"you can do anything with a one layer net\" approach basically says we can represent any function with a one layer net because we can have a hidden node for every possible combinations of inputs.  True, yes.  Practical, no.  Going deeper lets us represent hierarchical representations so that we can get tractable approximations of these super complex functions.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63263, "question": "I think you misunderstood. Given a nn with n layers there exists a nn with one layer that is equivalent. But the one layer nn will have many more nodes.\n\nThat's the gist anyway I'm sure the actual result is more technical.", "aSentId": 63266, "answer": "Yeah, that's what I meant. Multiple layer solution is more complex, so I don't see reason to use it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63267, "question": "Yeah, that's what I meant. Multiple layer solution is more complex, so I don't see reason to use it.", "aSentId": 63268, "answer": "You're still not getting it.  Why would you use 1 million nodes in a single layer when you can use 3 thousand split over three layers?  (actual numbers are made up)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63270, "question": "Has anyone been able to reproduce the results in \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\"?", "aSentId": 63271, "answer": "Are you trying to replicate one result in particular? I gave iRNNs a shot on some stuff I was working on, they did ok(better than the same system initialized with 0 mean, .01 std normal initialization), but I can't really say if they were better than any other system. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63272, "question": "Are you trying to replicate one result in particular? I gave iRNNs a shot on some stuff I was working on, they did ok(better than the same system initialized with 0 mean, .01 std normal initialization), but I can't really say if they were better than any other system. ", "aSentId": 63273, "answer": "I'm trying the idea on my own sequence prediction task. I've found relu units to be better in general, but I'm not seeing a strong gain in performance with the Identity Matrix weight initialization they describe.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63274, "question": "I'm trying the idea on my own sequence prediction task. I've found relu units to be better in general, but I'm not seeing a strong gain in performance with the Identity Matrix weight initialization they describe.", "aSentId": 63275, "answer": "A few things I've done in an iRNN is use the PRelu activation, and scaled the identity with:\n\n    numpy.arange(0,1,(1/n_hidden))\n\nThe the intuition being that different parts of the hidden state will be more inclined to forget/remember, and it is easier than trying to find the best value to scale the identity by. I haven't done any extensive testing to see if it is always better, but it's trivial to implement. \n\nI also ran into problems with the hidden state getting super big and overflowing in some scenarios, so I added batch normalization to prevent that. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63276, "question": "A few things I've done in an iRNN is use the PRelu activation, and scaled the identity with:\n\n    numpy.arange(0,1,(1/n_hidden))\n\nThe the intuition being that different parts of the hidden state will be more inclined to forget/remember, and it is easier than trying to find the best value to scale the identity by. I haven't done any extensive testing to see if it is always better, but it's trivial to implement. \n\nI also ran into problems with the hidden state getting super big and overflowing in some scenarios, so I added batch normalization to prevent that. ", "aSentId": 63277, "answer": "I've found overflow issues are much more common when I use relu units. Batch normalization seems like a nice trick to fix this. Has PRelu been beneficial over Relu for your recurrent networks?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63278, "question": "I've found overflow issues are much more common when I use relu units. Batch normalization seems like a nice trick to fix this. Has PRelu been beneficial over Relu for your recurrent networks?", "aSentId": 63279, "answer": "I ran it in a couple scenarios and it seemed to do better, so I use it by default now. If you batch normalize you shift the activation to mean-0, with generic relu this would throw away a lot of information so PRelu handles this case better. \n\nI still evaluate these on a case by case basis though, since I haven't done an exhaustive study to see if one is better than the other.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63272, "question": "Are you trying to replicate one result in particular? I gave iRNNs a shot on some stuff I was working on, they did ok(better than the same system initialized with 0 mean, .01 std normal initialization), but I can't really say if they were better than any other system. ", "aSentId": 63281, "answer": "Orthogonalized random on hidden-to-hidden, or just normal 0.01? I have been planning to look into this (iRNN) but haven't had time yet.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63282, "question": "Orthogonalized random on hidden-to-hidden, or just normal 0.01? I have been planning to look into this (iRNN) but haven't had time yet.", "aSentId": 63283, "answer": "normal 0.01, orthogonal would be better but I haven't tried it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63284, "question": "normal 0.01, orthogonal would be better but I haven't tried it.", "aSentId": 63285, "answer": "OK, I will be interested to see how it compares. Maybe the fact that I is a special kind of orthogonal provides lots of the benefit? But either way it seems nice.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63286, "question": "OK, I will be interested to see how it compares. Maybe the fact that I is a special kind of orthogonal provides lots of the benefit? But either way it seems nice.", "aSentId": 63287, "answer": "The big benefit it that the iRNN is much quicker than a similarly sized LSTM, however you might need to make it larger to match performance.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63288, "question": "The big benefit it that the iRNN is much quicker than a similarly sized LSTM, however you might need to make it larger to match performance.", "aSentId": 63289, "answer": "Hmm... sounds like a partial step back toward echo state networks :) I really need to set aside some time and check this out", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63290, "question": "Hmm... sounds like a partial step back toward echo state networks :) I really need to set aside some time and check this out", "aSentId": 63291, "answer": "Its definitely easy to knock one together, I had it up and running in theano like 10 min after reading the paper. Given the success of relu for other deep learning stuff, it would be nice to see it for RNNs. I know Baidu's recent speech system used a relu RNN instead of LSTM because they could scale relu more.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63293, "question": "Training a mini-Convnet. My learning curve starts on a Plataeu. Need help understanding why", "aSentId": 63294, "answer": "Try different weight initialization, or use batch normalization. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63295, "question": "Try different weight initialization, or use batch normalization. ", "aSentId": 63296, "answer": "My input is coming from a previously trained conv-net, I was just feeding it as-is to my network. Dunno how to implement something like batch-normalization in this context...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63297, "question": "My input is coming from a previously trained conv-net, I was just feeding it as-is to my network. Dunno how to implement something like batch-normalization in this context...", "aSentId": 63298, "answer": "Like you take the output of the last layer and use that as the input to your convnet, while keeping the pre-trained weights the same? If so you would just batch normalize any non-linearity that has a set of weights that you are learning, and ignore the fixed weights.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63299, "question": "Like you take the output of the last layer and use that as the input to your convnet, while keeping the pre-trained weights the same? If so you would just batch normalize any non-linearity that has a set of weights that you are learning, and ignore the fixed weights.", "aSentId": 63300, "answer": "Will give that paper a read. I thought batch-normalization applied to doing stuff to the input (I guess I saw the x's in the equation and made assumptions)\n\nThanks", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63293, "question": "Training a mini-Convnet. My learning curve starts on a Plataeu. Need help understanding why", "aSentId": 63302, "answer": "Have you tried increasing/decreasing the learning rate in steps of 10x?\nfor example: 1, 0.1, 0.001, 0.0001\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63304, "question": "Repeat Buyers Prediction Competition [IJCAI 15]", "aSentId": 63305, "answer": "I can see the practical value of this problem. But having to write your code in Java for the 2nd stage, just so it can be deployed easily is ... uhh.... unusual. And their final part  (\"The top 3 teams at the second stage will have the opportuntiy to deploy their algorithms on Tmall.com for the ''Double-11'' promotion, 2015. [...] Participants at this stage would work onsite as interns for around two months.\")  Just screams exploitation", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63306, "question": "I can see the practical value of this problem. But having to write your code in Java for the 2nd stage, just so it can be deployed easily is ... uhh.... unusual. And their final part  (\"The top 3 teams at the second stage will have the opportuntiy to deploy their algorithms on Tmall.com for the ''Double-11'' promotion, 2015. [...] Participants at this stage would work onsite as interns for around two months.\")  Just screams exploitation", "aSentId": 63307, "answer": "I don't even think this technically counts as \"screaming exploitation.\" I'm pretty sure it's more like calm, pseudo-rational inside-voices explanation of their plans to blatantly exploit whoever has the misfortune of winning this thing. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63304, "question": "Repeat Buyers Prediction Competition [IJCAI 15]", "aSentId": 63309, "answer": "\"You will need to submit your code in JAVA\"\n\nYou couldn't pay me enough...  \n\nWay to go throwing away 90-99% of attainable machine learning performance AliBaba.  But I should probably be quiet about that, after all:\n\n\"Never interrupt your enemy when he is making a mistake.\" - Napoleon Bonaparte", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63310, "question": "\"You will need to submit your code in JAVA\"\n\nYou couldn't pay me enough...  \n\nWay to go throwing away 90-99% of attainable machine learning performance AliBaba.  But I should probably be quiet about that, after all:\n\n\"Never interrupt your enemy when he is making a mistake.\" - Napoleon Bonaparte", "aSentId": 63311, "answer": "This Java requirement is hilarious, if you step back a little bit and think about it.\n\n\"You will need to submit your code in JAVA, then the distributed computation will be handled by the cloud platform.\"\n\nSure, sure...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63304, "question": "Repeat Buyers Prediction Competition [IJCAI 15]", "aSentId": 63313, "answer": "I am not sure I am doing it the right way, but the registration seems to be through a form in Kanji. Also (from what I can guess), this form requires a +86 phone number.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63315, "question": "Any advice on good books about Deep Learning?", "aSentId": 63316, "answer": "Andrew Ng's has course notes (note his slides for the coursera class). Have Bengio's book drafts but have not read. All free.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63318, "question": "Question about Fractional Max-Pooling", "aSentId": 63319, "answer": "If I understand correctly, you're asking how you generate a string of N 1's and 2's so that the sum is another number M. I think you can just randomly generate 1s and 2s, calculate the sum S at the end, and then randomly flip (S-M) 2s to 1s if S&gt;M (or vice versa if S&lt;M.) This only works if N &lt;= M &lt;= 2N, but that seems to be fulfilled here.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63318, "question": "Question about Fractional Max-Pooling", "aSentId": 63321, "answer": "I have some work in progress theano code for FMP here: \nhttps://github.com/Lasagne/Lasagne/pull/171\n\nI've only implemented random overlapping mode so far.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63322, "question": "I have some work in progress theano code for FMP here: \nhttps://github.com/Lasagne/Lasagne/pull/171\n\nI've only implemented random overlapping mode so far.", "aSentId": 63323, "answer": "Awesome! I'll have to look into Theano. I've been playing around with Torch for my projects. \n\nI imagine the disjoint sets would be quite a bit harder to get set up. A lot of bounds checking. \n\nOT: Am I wrong in thinking the paper doesn't describe his method too well? Did I just not understand the notation used?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63318, "question": "Question about Fractional Max-Pooling", "aSentId": 63325, "answer": "Just checked, Ben Graham has updated SparseCNN with code for fractional pooling:\n\nhttps://github.com/btgraham/SparseConvNet/blob/master/MaxPoolingLayer.h", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63327, "question": "I had been thinking about doing something like he does in this paper, randomly varying the size of the pooling window or stride length.\n\nThe way I was thinking about doing it, in the case of 1's or 2's was:\n\n - Keep track of the N^in - N^out and N^in \n  - N^in - N^out is the number of 2's required \n  - If the number of 2's is greater than or equal to N^in - N^out, and we have all the 2's we need, then fill the rest with 1's.\n - Keep track of N^out - the number of 2's required, this is the number of 1's required. \n  - If the number of 1's in the list is greater than or equal to the number of 1's needed, then fill the slots with 2's. ", "aSentId": 63328, "answer": "I did it by making a list of (N^in - N^out ) 2's and (N^out - (N^in - N^out )) 1's, then shuffling it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63330, "question": "R script as a data collection tool - how does this work?", "aSentId": 63331, "answer": "Where does data come from? Database (SQL, Oracle)?, text files (CSV, XLS,...)? web api or web page?\n\nI think R can get data from these sources and more. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63332, "question": "Where does data come from? Database (SQL, Oracle)?, text files (CSV, XLS,...)? web api or web page?\n\nI think R can get data from these sources and more. ", "aSentId": 63333, "answer": "The date source is a series of word docs (official transcripts). would that make it easier than online sources, i suppose? \n\nDo you know any website/books that i can learn from? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63334, "question": "The date source is a series of word docs (official transcripts). would that make it easier than online sources, i suppose? \n\nDo you know any website/books that i can learn from? ", "aSentId": 63335, "answer": "what format of these docs?\n\nyou can upload some samples online and I will try to help you if I can", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63337, "question": "Format for training HMM with MFCCs", "aSentId": 63338, "answer": "You can try a different toolbox like HMMLearn (Python) or pmtk (never version of HMM toolbox IIRC). HMMs are notoriously difficult numerically!\n\nAlso try normalizing your inputs - take each feature channel and subtract mean, divide by standard deviation. With time series aka 3D tensor it is a little weird, but I normally calculate the mean over each feature, over all time slices. Same with std.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63340, "question": "Real applications of probabilistic programming", "aSentId": 63341, "answer": "PyMC is an example of a probabilistic programming system. So are other \"real-world\" Bayesian modeling tools like BUGS, Stan, etc. \n\nThere's a spectrum of probabilistic programming tools, from those like PyMC, BUGS, Stan, Infer.NET that allow you to describe (somewhat restricted) classes of graphical models and do efficient inference in them, to more ambitious tools that allow description of more sophisticated probability models (mixing discrete and continuous variables, potentially with unknown numbers of objects, potentially even defined recursively to include variables representing the results of inference on submodels, etc.) but can't necessarily do efficient inference for many of the models they can describe. The latter category is where most of the research effort is going, but it's certainly still an active research field, meaning that most of the tools (Venture, BLOG, Figaro, etc.) still don't work that well in practice and AFAIK the only current applications are carefully selected proofs-of-concept in research papers.\n\nUltimately the vision is to make it very easy to experiment with building novel probability models. This means supporting a wide range of models (beyond what tools like PyMC can easily express) with reasonably reliable inference, since complex models or large datasets will easily overload most current tools.\n\nAnother way to put it is that a *wide* range of current papers at NIPS, ICML, AAAI, CVPR, ACL, ICRA, etc. consist of \"we thought up a new graphical model, had a team of grad students spend 6-12 months implementing an inference engine, and it performs better on some task.\" The vision of probabilistic programming is that those months of grad-student time could be reduced to an afternoon of just specifying the model in a simple description language and then letting the machine execute it automatically. It's not necessarily the solution to all problems in machine learning -- a lot of real-world problems are solvable as simple regression/classification/whatever and don't require developing new models, and not even all valuable new techniques are naturally expressed as Bayesian inference in some probability model (e.g. convnets in vision -- yes there are probabilistic interpretations, but that's not usually how people conceive of these models) -- but a flexible, efficient probabilistic programming system would be a very big deal for people doing model-based AI research.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63342, "question": "PyMC is an example of a probabilistic programming system. So are other \"real-world\" Bayesian modeling tools like BUGS, Stan, etc. \n\nThere's a spectrum of probabilistic programming tools, from those like PyMC, BUGS, Stan, Infer.NET that allow you to describe (somewhat restricted) classes of graphical models and do efficient inference in them, to more ambitious tools that allow description of more sophisticated probability models (mixing discrete and continuous variables, potentially with unknown numbers of objects, potentially even defined recursively to include variables representing the results of inference on submodels, etc.) but can't necessarily do efficient inference for many of the models they can describe. The latter category is where most of the research effort is going, but it's certainly still an active research field, meaning that most of the tools (Venture, BLOG, Figaro, etc.) still don't work that well in practice and AFAIK the only current applications are carefully selected proofs-of-concept in research papers.\n\nUltimately the vision is to make it very easy to experiment with building novel probability models. This means supporting a wide range of models (beyond what tools like PyMC can easily express) with reasonably reliable inference, since complex models or large datasets will easily overload most current tools.\n\nAnother way to put it is that a *wide* range of current papers at NIPS, ICML, AAAI, CVPR, ACL, ICRA, etc. consist of \"we thought up a new graphical model, had a team of grad students spend 6-12 months implementing an inference engine, and it performs better on some task.\" The vision of probabilistic programming is that those months of grad-student time could be reduced to an afternoon of just specifying the model in a simple description language and then letting the machine execute it automatically. It's not necessarily the solution to all problems in machine learning -- a lot of real-world problems are solvable as simple regression/classification/whatever and don't require developing new models, and not even all valuable new techniques are naturally expressed as Bayesian inference in some probability model (e.g. convnets in vision -- yes there are probabilistic interpretations, but that's not usually how people conceive of these models) -- but a flexible, efficient probabilistic programming system would be a very big deal for people doing model-based AI research.", "aSentId": 63343, "answer": "Hmm yes, I can see what you're saying, and it makes sense. I guess I'm just a a little surprised there aren't more clear applications already around, even just of simple expression of complicated models, given the number of PPLs that are under development and the buzz it seems to have.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63345, "question": "How Airbnb uses machine learning to detect host preferences", "aSentId": 63346, "answer": "\"The project taught us that sometimes you have to roll up your sleeves and build a machine learning model tailored for your own application\"\nTotally agreed with that.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63345, "question": "How Airbnb uses machine learning to detect host preferences", "aSentId": 63348, "answer": "Why the hell were they using MSE as a metric for logistic regression?!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63349, "question": "Why the hell were they using MSE as a metric for logistic regression?!", "aSentId": 63350, "answer": "the article says they were interested in probabilities more so than correctly classifying. can you elaborate why MSE is wrong in this case? genuinely curious ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63351, "question": "the article says they were interested in probabilities more so than correctly classifying. can you elaborate why MSE is wrong in this case? genuinely curious ", "aSentId": 63352, "answer": "Yeah so why not logistic loss? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63353, "question": "Yeah so why not logistic loss? ", "aSentId": 63354, "answer": "In my view, MSE is usually not appropriate when thinking about probabilities.  Let's say that for a certain instance, p(y = 1 | w,x) = 1.  A model that gives a probability of 0.6 is better than a model that gives a model of 0.5, but a model that gives a probability of 0.1 is WAY BETTER than a model that gives a probability of 0.0.  Such a model would actually have a total data likelihood of 0 or an infinite negative log likelihood.  \n\nI associate square loss with mean estimators and with the likelihood of a normal distribution.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63355, "question": "In my view, MSE is usually not appropriate when thinking about probabilities.  Let's say that for a certain instance, p(y = 1 | w,x) = 1.  A model that gives a probability of 0.6 is better than a model that gives a model of 0.5, but a model that gives a probability of 0.1 is WAY BETTER than a model that gives a probability of 0.0.  Such a model would actually have a total data likelihood of 0 or an infinite negative log likelihood.  \n\nI associate square loss with mean estimators and with the likelihood of a normal distribution.  ", "aSentId": 63356, "answer": "Here's how I see things - ultimately, even if the'ye looking to improve the probability, it's still with the ultimate goal of performing better classification. Wouldn't it make more sense to set a cutoff, and then use traditional classification metrics? I would assume that using MSE, or any variant of such metrics usually used for regression problems, would be completely inappropriate?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63357, "question": "Here's how I see things - ultimately, even if the'ye looking to improve the probability, it's still with the ultimate goal of performing better classification. Wouldn't it make more sense to set a cutoff, and then use traditional classification metrics? I would assume that using MSE, or any variant of such metrics usually used for regression problems, would be completely inappropriate?", "aSentId": 63358, "answer": "The metric depends on your goals, but I think that precision/recall is often a natural metric.  I.e. with what precision can we identify 90% of successful hostings.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63359, "question": "The metric depends on your goals, but I think that precision/recall is often a natural metric.  I.e. with what precision can we identify 90% of successful hostings.  ", "aSentId": 63360, "answer": "I absolutely agree. But my question is (and, as a poster above said, genuinely curious) - does it make any sense to use MSE as a metric in the blog post's scenario? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 63361, "question": "I absolutely agree. But my question is (and, as a poster above said, genuinely curious) - does it make any sense to use MSE as a metric in the blog post's scenario? ", "aSentId": 63362, "answer": "I think their motivation to use MSE is because they're actually trying to compute a 'score' by which to rank entries. The fact that this score lies in the interval [0, 1] and has an interpretation as a probability is not really relevant for them.\n\nThat said, some kind of ranking loss might have been a better match (maybe mean average precision, or precision@k where k is the number of results displayed on the first page). But the MSE is really easy to compute, so that may be why they went with that.", "corpus": "reddit"}]