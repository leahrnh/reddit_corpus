[{"docID": "t5_2r3gv", "qSentId": 52604, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 52605, "answer": "You again!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52604, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 52607, "answer": "All I can say is _wow_! This shall be interesting.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52604, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 52609, "answer": "In case I don't make it: does the French guy get his bottle of exquisite French wine? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52616, "question": "Google DeepMind Nature Paper: Human-level control through deep reinforcement learning", "aSentId": 52617, "answer": "Here's a link to a publicly-accessible version of the full paper.\n\nhttp://rdcu.be/cdlg", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52618, "question": "Here's a link to a publicly-accessible version of the full paper.\n\nhttp://rdcu.be/cdlg", "aSentId": 52619, "answer": "I'm looking for videos of the software playing the games. I've seen some Space Invaders and Breakout, is there anything else out there?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52616, "question": "Google DeepMind Nature Paper: Human-level control through deep reinforcement learning", "aSentId": 52621, "answer": "Hey I read the paper but I only have the most basic knowledge of ML so I was wondering if somebody could explain to me exactly what's novel about this. So from what I gather the basic set up is a convolutional NN with 2 convolutional layers with multiple frames of video as inputs and 2 fully connected layers with game controls as outputs. Gradient descent (or ascent I guess) is then performed to maximize a so-called Q score which is a future-discounted score, with what looks like a pretty normal loss function? The whole learning procedure is unsupervised with randomized initial NN parameters and random actions sampled. We run this many times. Okay this is all pretty basic so far.\n\nWhat I don't understand is all the supposed modifications introduced that make this so good. What exactly is the experience replay aspect of this? It seems like what we do is store transitions then instead of performing gradient descent on one transition we sample a 'minimatch' of transitions from previously stored transitions? What good does this do exactly or did I misunderstand completely? The point of this is to solve the problems with certain correlations that we don't want? I don't quite follow this part.\n\nWhen sampling an action to perform what is the significance of the epsilon? With epsilon probability we uniformly sample a random action and with 1-epsilon probability we select the best action given the current Q function? What's the point of that exactly?\n\nFinally what the hell is the Q\\^hat function and what is its relationship to the Q function?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52622, "question": "Hey I read the paper but I only have the most basic knowledge of ML so I was wondering if somebody could explain to me exactly what's novel about this. So from what I gather the basic set up is a convolutional NN with 2 convolutional layers with multiple frames of video as inputs and 2 fully connected layers with game controls as outputs. Gradient descent (or ascent I guess) is then performed to maximize a so-called Q score which is a future-discounted score, with what looks like a pretty normal loss function? The whole learning procedure is unsupervised with randomized initial NN parameters and random actions sampled. We run this many times. Okay this is all pretty basic so far.\n\nWhat I don't understand is all the supposed modifications introduced that make this so good. What exactly is the experience replay aspect of this? It seems like what we do is store transitions then instead of performing gradient descent on one transition we sample a 'minimatch' of transitions from previously stored transitions? What good does this do exactly or did I misunderstand completely? The point of this is to solve the problems with certain correlations that we don't want? I don't quite follow this part.\n\nWhen sampling an action to perform what is the significance of the epsilon? With epsilon probability we uniformly sample a random action and with 1-epsilon probability we select the best action given the current Q function? What's the point of that exactly?\n\nFinally what the hell is the Q\\^hat function and what is its relationship to the Q function?", "aSentId": 52623, "answer": "&gt; What I don't understand is all the supposed modifications introduced that make this so good. What exactly is the experience replay aspect of this? It seems like what we do is store transitions then instead of performing gradient descent on one transition we sample a 'minimatch' of transitions from previously stored transitions? What good does this do exactly or did I misunderstand completely? The point of this is to solve the problems with certain correlations that we don't want? I don't quite follow this part.\n\nI can't explain why it works, but it's basically an array of N recently done moves. Every new move replaces a randomly chosen element in this array and training then uses minibatches randomly sampled from it. So, the NN always learns a random combination of recent and older moves. I did (almost) the same for my Nine Men's Morris playing ANN, because it was faster to reuse existing examples than to create novel ones. For some games, doing that doesn't seem to degrade performance, so it's a valid optimization, but I'm not aware of any satisfactory explanation of why it works.\n\n&gt; When sampling an action to perform what is the significance of the epsilon? With epsilon probability we uniformly sample a random action and with 1-epsilon probability we select the best action given the current Q function? What's the point of that exactly?\n\nExploration. It's done to prevent the AI from doing the same move over and over again even though there might be a better one available.\n\n&gt; Finally what the hell is the Q^hat function and what is its relationship to the Q function?\n\nQ is the neural network that's currently playing. Q\\^hat is the network after training.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52622, "question": "Hey I read the paper but I only have the most basic knowledge of ML so I was wondering if somebody could explain to me exactly what's novel about this. So from what I gather the basic set up is a convolutional NN with 2 convolutional layers with multiple frames of video as inputs and 2 fully connected layers with game controls as outputs. Gradient descent (or ascent I guess) is then performed to maximize a so-called Q score which is a future-discounted score, with what looks like a pretty normal loss function? The whole learning procedure is unsupervised with randomized initial NN parameters and random actions sampled. We run this many times. Okay this is all pretty basic so far.\n\nWhat I don't understand is all the supposed modifications introduced that make this so good. What exactly is the experience replay aspect of this? It seems like what we do is store transitions then instead of performing gradient descent on one transition we sample a 'minimatch' of transitions from previously stored transitions? What good does this do exactly or did I misunderstand completely? The point of this is to solve the problems with certain correlations that we don't want? I don't quite follow this part.\n\nWhen sampling an action to perform what is the significance of the epsilon? With epsilon probability we uniformly sample a random action and with 1-epsilon probability we select the best action given the current Q function? What's the point of that exactly?\n\nFinally what the hell is the Q\\^hat function and what is its relationship to the Q function?", "aSentId": 52625, "answer": "&gt;but I only have the most basic knowledge of ML\n\nNo you don't.  You know more than I do.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52616, "question": "Google DeepMind Nature Paper: Human-level control through deep reinforcement learning", "aSentId": 52627, "answer": "As is traditional with articles published in Nature, the title is extremely misleading.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52628, "question": "As is traditional with articles published in Nature, the title is extremely misleading.", "aSentId": 52629, "answer": "How so?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52630, "question": "How so?", "aSentId": 52631, "answer": "I would not call the Atari 2600 game set the benchmark for human control. Those games were designed around the limitations of the technology available in 1977, not the limitations of human control.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52632, "question": "I would not call the Atari 2600 game set the benchmark for human control. Those games were designed around the limitations of the technology available in 1977, not the limitations of human control.", "aSentId": 52633, "answer": "Human level control (of atari 2600 games) through deep reinforcement learning", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52634, "question": "Human level control (of atari 2600 games) through deep reinforcement learning", "aSentId": 52635, "answer": "Controlling an atari game has nothing to do with \"control\" as in control theory or robotics or science.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52636, "question": "Controlling an atari game has nothing to do with \"control\" as in control theory or robotics or science.", "aSentId": 52637, "answer": "Except for the fact that it's literally control theory.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52636, "question": "Controlling an atari game has nothing to do with \"control\" as in control theory or robotics or science.", "aSentId": 52639, "answer": "what do you mean? Are you saying that this isn't a SISO classical control problem from an undergraduate textbook? If so I agree, but there is a great deal more to control theory than pid controllers and bode plots.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52641, "question": "This really sounds cool! Sounds, because there's a paywall and I can't read what they did.\n\nAnyone know where we can read the paper?", "aSentId": 52642, "answer": "Use the link at the end of this [ArsTechnica article](http://arstechnica.com/science/2015/02/ai-masters-49-atari-2600-games-without-instructions/). It will redirect you to the paper if you follow it this way.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52641, "question": "This really sounds cool! Sounds, because there's a paywall and I can't read what they did.\n\nAnyone know where we can read the paper?", "aSentId": 52644, "answer": "I'm not quite sure if this paper is elsewhere, but [the precursory work behind this paper is on ArXiV](http://arxiv.org/pdf/1312.5602v1.pdf).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52645, "question": "I'm not quite sure if this paper is elsewhere, but [the precursory work behind this paper is on ArXiV](http://arxiv.org/pdf/1312.5602v1.pdf).", "aSentId": 52646, "answer": "I *thought* this sounded exactly like earlier work. Does anyone know how this expands on the earlier Atari paper?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52647, "question": "I *thought* this sounded exactly like earlier work. Does anyone know how this expands on the earlier Atari paper?", "aSentId": 52648, "answer": "It looks like the algorithm is the same, but they describe and analyze it in a bit more depth and they have now tested 49 games (used to be 6).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52653, "question": "I'm curious to know what the engineers behind this have to say about the fact that humans can learn to master these games with much, much less than 600+ training sets.\n\nThere is definitely something we're missing in how we train learning systems that they don't learn as quickly as a human would. I know humans have other experience we draw on, but I have to imagine there's an algorithm out there that will learn to master these games in a dozen training sets rather than hundreds.", "aSentId": 52654, "answer": "How many training samples needs a baby to walk? Maybe the key we are missing is to transfer knowledge from one task to another; once you know a lot if things, it's easier to learn others.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52655, "question": "How many training samples needs a baby to walk? Maybe the key we are missing is to transfer knowledge from one task to another; once you know a lot if things, it's easier to learn others.", "aSentId": 52656, "answer": "A baby is getting a huge amount of training data; every waking moment it's getting tons and tons and tons of sensory input and environmental feedback.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52655, "question": "How many training samples needs a baby to walk? Maybe the key we are missing is to transfer knowledge from one task to another; once you know a lot if things, it's easier to learn others.", "aSentId": 52658, "answer": "Anybody has suggested readings on the current approach of 'transferring knowledge from one task to another', if it's even done at all now?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52655, "question": "How many training samples needs a baby to walk? Maybe the key we are missing is to transfer knowledge from one task to another; once you know a lot if things, it's easier to learn others.", "aSentId": 52660, "answer": "In machine learning we use gradient following methods to perform learning. It's not certain that the brain is trained through gradient descent, or at least not just by gradient descent. Probably the brain uses a more sophisticated training system, gradient propagation is too sensitive and slow. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52653, "question": "I'm curious to know what the engineers behind this have to say about the fact that humans can learn to master these games with much, much less than 600+ training sets.\n\nThere is definitely something we're missing in how we train learning systems that they don't learn as quickly as a human would. I know humans have other experience we draw on, but I have to imagine there's an algorithm out there that will learn to master these games in a dozen training sets rather than hundreds.", "aSentId": 52663, "answer": "like alvaroggared said, Humans transfer their knowledge and make assumptions on observation from similar experiences.\n\nThe DeepMind AI is an academic piece, it stars each game like a newborn. \n\nIf I show you a game similar looking to pong, you will treat the game like pong in your first try.\nYou will now for a second think that the \"right arrow\" key will make you move left etc.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52653, "question": "I'm curious to know what the engineers behind this have to say about the fact that humans can learn to master these games with much, much less than 600+ training sets.\n\nThere is definitely something we're missing in how we train learning systems that they don't learn as quickly as a human would. I know humans have other experience we draw on, but I have to imagine there's an algorithm out there that will learn to master these games in a dozen training sets rather than hundreds.", "aSentId": 52665, "answer": "does some of this problem have to do with Q-learning itself? In basic Q-leaning values flow almost like heat, and you have to visit a state to update it...\n\nWith a model of the state transitions could you simulate how the values would flow, to do some of the updates without actually visiting them? Wait .... it sounds like that replay feature is an empirical model for doing this...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52667, "question": "Advice for getting into ML from a humanities PhD", "aSentId": 52668, "answer": "I would ask your professor for guidance, even if it's out of their field. Either have your professor or yourself contact a professor in charge of the machine learning or pattern recognition labs at your school. I know my lab does a lot of collaboration with other departments (although, mostly biology fields).  \n   \nFrom there, you can work on a collaborative project with students involved in machine learning just to get your foot in the door. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52667, "question": "Advice for getting into ML from a humanities PhD", "aSentId": 52670, "answer": "ive seen MLAPP posted elsewhere.  What does it stand for?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52675, "question": "(ML Beginner) - I want to implement an algorithm from a research paper, but not sure how to tackle it...", "aSentId": 52676, "answer": "&gt; Is all the information I need to reproduce this algorithm even contained within this paper and I just can't put it all together?\n\nSeems like it, yes. \n\n&gt;is it possible/normal for someone to reach out to the researchers and ask them for more details of their implementation? Or is that a no-no?\n\nYou always can, some people are more responsive than others. Since you don't seem to really understand what you are doing - your chances of getting a response probably drop a good bit. But they dont have time to teach you their whole field just so you can understand their one paper. \n\n\n&gt;I'm very much a beginner when it comes to Machine Learning topics and don't have a formal background in either Computer Science or Math (self taught web developer)\n\nThis is really your biggest issue. There is a lot of background material you don't have any training on at all. I sincerely doubt you *actually* understand what is going on when they say \"pairwise linear programming ranking model\". Just a cursory skim and I've got a good idea how to implement what they have described using a linear programming library. Their method is pretty straightforward and they step through a lot more of the details than most papers do. \n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52677, "question": "&gt; Is all the information I need to reproduce this algorithm even contained within this paper and I just can't put it all together?\n\nSeems like it, yes. \n\n&gt;is it possible/normal for someone to reach out to the researchers and ask them for more details of their implementation? Or is that a no-no?\n\nYou always can, some people are more responsive than others. Since you don't seem to really understand what you are doing - your chances of getting a response probably drop a good bit. But they dont have time to teach you their whole field just so you can understand their one paper. \n\n\n&gt;I'm very much a beginner when it comes to Machine Learning topics and don't have a formal background in either Computer Science or Math (self taught web developer)\n\nThis is really your biggest issue. There is a lot of background material you don't have any training on at all. I sincerely doubt you *actually* understand what is going on when they say \"pairwise linear programming ranking model\". Just a cursory skim and I've got a good idea how to implement what they have described using a linear programming library. Their method is pretty straightforward and they step through a lot more of the details than most papers do. \n\n", "aSentId": 52678, "answer": "Thanks for taking the time to reply. Like I said, I have no qualms about admitting my lack of knowledge in this field, but that's not going to stop me from asking and researching the topic thoroughly.\n\nI definitely don't have any experience implementing pairwise linear ranking models, but conceptually I understand what is happening in the sense that I know that the model compares each item against the others in pairs and summing up the results to determine rankings rather than looking at the rankings overall (hell, I readily admit that what I just said could be wrong, but that's at least my understanding of this system thus far).\n\nSince you point out that you have a good idea how to implement their system from a cursory skim, I have to ask: Can you give me a high level overview of how you would go about it? Is this something I could feasibly pull off in something like scikit-learn?\n\nAlso, while we're at it: Is the actual algorithm to be implemented contained in section III. B. 1. of the paper? And that algorithm would identify the weights needed for each ranking factor in this problem context?\n\nThanks again in advance!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52679, "question": "Thanks for taking the time to reply. Like I said, I have no qualms about admitting my lack of knowledge in this field, but that's not going to stop me from asking and researching the topic thoroughly.\n\nI definitely don't have any experience implementing pairwise linear ranking models, but conceptually I understand what is happening in the sense that I know that the model compares each item against the others in pairs and summing up the results to determine rankings rather than looking at the rankings overall (hell, I readily admit that what I just said could be wrong, but that's at least my understanding of this system thus far).\n\nSince you point out that you have a good idea how to implement their system from a cursory skim, I have to ask: Can you give me a high level overview of how you would go about it? Is this something I could feasibly pull off in something like scikit-learn?\n\nAlso, while we're at it: Is the actual algorithm to be implemented contained in section III. B. 1. of the paper? And that algorithm would identify the weights needed for each ranking factor in this problem context?\n\nThanks again in advance!", "aSentId": 52680, "answer": "As far as algorithms in research papers are concerned, this one doesn't seem bad at all.\n\nscipy.optimize has a linear program solver ('linprog'). The hardest part (in my opinion) will be understanding the linear programming formulation. I know a little (not a lot) about linear programming and have successfully used it to solve a few (simple) problems, and could not at a glance understanding exactly what they're doing.\n\nThe partitioning algorithm seems fairly straightforward, though.\n\nEdit: You may actually want to use the SVM approach rather than the linear program. There are libraries that provide a very high level of abstraction around training a SVM classifier. Linear programming is technically the simpler model, but you'll probably have to get your hands dirtier.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52681, "question": "As far as algorithms in research papers are concerned, this one doesn't seem bad at all.\n\nscipy.optimize has a linear program solver ('linprog'). The hardest part (in my opinion) will be understanding the linear programming formulation. I know a little (not a lot) about linear programming and have successfully used it to solve a few (simple) problems, and could not at a glance understanding exactly what they're doing.\n\nThe partitioning algorithm seems fairly straightforward, though.\n\nEdit: You may actually want to use the SVM approach rather than the linear program. There are libraries that provide a very high level of abstraction around training a SVM classifier. Linear programming is technically the simpler model, but you'll probably have to get your hands dirtier.", "aSentId": 52682, "answer": "Thanks for the reply! I'll start digging into these paths to see where it gets me.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52679, "question": "Thanks for taking the time to reply. Like I said, I have no qualms about admitting my lack of knowledge in this field, but that's not going to stop me from asking and researching the topic thoroughly.\n\nI definitely don't have any experience implementing pairwise linear ranking models, but conceptually I understand what is happening in the sense that I know that the model compares each item against the others in pairs and summing up the results to determine rankings rather than looking at the rankings overall (hell, I readily admit that what I just said could be wrong, but that's at least my understanding of this system thus far).\n\nSince you point out that you have a good idea how to implement their system from a cursory skim, I have to ask: Can you give me a high level overview of how you would go about it? Is this something I could feasibly pull off in something like scikit-learn?\n\nAlso, while we're at it: Is the actual algorithm to be implemented contained in section III. B. 1. of the paper? And that algorithm would identify the weights needed for each ranking factor in this problem context?\n\nThanks again in advance!", "aSentId": 52684, "answer": "&gt; conceptually I understand what is happening in the sense that I know that the model compares each item against the others in pairs and summing up the results to determine rankings rather than looking at the rankings overall\n\nWell, its not wrong. But I also wouldn't call that level of explanation an *understanding* in anyway. What you said is the high level jist, similarly - I could say Neural Networks are \"Just a repeated transformation of matrix vector products with a function applied after, where the largest value at the end indicates the class\". Is it wrong? No. Does actually display any understand of what is actually going on? Also no. It just means you know where the game is, not that you can play. \n\nJust to be clear, I'm not trying to be rude or mean about this. Its just that to go into more detail, I would have to teach you everything you are missing. If you had more background I could attempt one and see what is unclear, but the gap is too large. \n\n&gt;Can you give me a high level overview of how you would go about it?\n\nThere in lies the problem. The paper already provides an excellent high level overview of how to do it, it even goes a little lower and explains out step details. I don't think I could explain it any better than what is presented for you.\n\n\n&gt;Also, while we're at it: Is the actual algorithm to be implemented contained in section III. B. 1. of the paper?\n\nB.1 and B.3 combined make the final algorithm. B.3 uses B.1 \n\n&gt;And that algorithm would identify the weights needed for each ranking factor in this problem context?\n\nB.1 learns a set of ranks, B.3 learns a sequence of weights. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52675, "question": "(ML Beginner) - I want to implement an algorithm from a research paper, but not sure how to tackle it...", "aSentId": 52686, "answer": "You can do it. It's there but you'll need to study more.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52689, "question": "How necessary is an advanced degree to getting a job in the ML/AI field?", "aSentId": 52690, "answer": "I can't speak for the field at large, but here is my experience:\n\nI have my B.S. in C.S. and have moved more and more into the field of Machine Learning and Data Mining over the last 12 years. I found opportunities with start-ups who had no idea what they were doing and were impressed by the little I knew.  I grew.  They grew.\n\nThat said, if I were hiring right now for my team, I would certainly prefer an M.S. applicant.  Further, I am working hard to catch up on the breadth of techniques I might have learned through the graduate education process.  I may yet go after an advanced degree.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52689, "question": "How necessary is an advanced degree to getting a job in the ML/AI field?", "aSentId": 52692, "answer": "It really depends on the type of work you're going to be doing. ML and AI are broad generalizations for a field which contains a variety of jobs.\n\nAre you going to be implementing ML systems based on algorithms that have already been developed? In this case, you probably don't need an advanced degree. It'll be most software engineering. It'll be helpful and potentially necessary to have an understanding of how the algorithms work, but you won't need a complete knowledge of ML theory related to probability, statistics and complexity.\n\nConversely, if you're trying to develop new ML algorithms and work in a more research-oriented role, you probably do need an advanced degree. The ML/AI education given by undergraduate courses and even graduate courses (don't even get me started on most MOOCs) is almost completely superficial. The courses are designed to teach the structures of algorithms, but rarely handle the probabilistic principles behind many of them, nor the intuition of when to use certain approaches. Doing a large-scale ML project as part of a research thesis is the only way to get a good grasp many of these issues and an advanced degree is the best way of showing that you have that experience.\n\nThat's not to say that someone that doesn't have an advanced degree can't be successful in a research role. It's just a justification for why many employers prefer candidates that do have them. Employers also prefer candidates with advanced degrees for their software engineering positions on ML projects, but they tend to be less strict about these. Showing that you have experience with your own personal projects (e.g. Kaggle, KDD competitions) could be enough. Just doing these online competition probably wouldn't be enough for a research role because it's altogether a different problem. You're usually just applying existing methods to a data set, not creating a new method.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52693, "question": "It really depends on the type of work you're going to be doing. ML and AI are broad generalizations for a field which contains a variety of jobs.\n\nAre you going to be implementing ML systems based on algorithms that have already been developed? In this case, you probably don't need an advanced degree. It'll be most software engineering. It'll be helpful and potentially necessary to have an understanding of how the algorithms work, but you won't need a complete knowledge of ML theory related to probability, statistics and complexity.\n\nConversely, if you're trying to develop new ML algorithms and work in a more research-oriented role, you probably do need an advanced degree. The ML/AI education given by undergraduate courses and even graduate courses (don't even get me started on most MOOCs) is almost completely superficial. The courses are designed to teach the structures of algorithms, but rarely handle the probabilistic principles behind many of them, nor the intuition of when to use certain approaches. Doing a large-scale ML project as part of a research thesis is the only way to get a good grasp many of these issues and an advanced degree is the best way of showing that you have that experience.\n\nThat's not to say that someone that doesn't have an advanced degree can't be successful in a research role. It's just a justification for why many employers prefer candidates that do have them. Employers also prefer candidates with advanced degrees for their software engineering positions on ML projects, but they tend to be less strict about these. Showing that you have experience with your own personal projects (e.g. Kaggle, KDD competitions) could be enough. Just doing these online competition probably wouldn't be enough for a research role because it's altogether a different problem. You're usually just applying existing methods to a data set, not creating a new method.", "aSentId": 52694, "answer": "Definitely agree that most MOOCs are a joke.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52695, "question": "Definitely agree that most MOOCs are a joke.", "aSentId": 52696, "answer": "I wouldn't necessarily call them a joke. At the end of the day they accomplish what they set out to do. They provide an introduction to a  non-accessible field for someone who's never done any real machine learning work. But people tend to forget about these limitations. Taking Andrew Ng's coursera course does not make you an expert in machine learning (though he does mislead you into believing you are with a lot of his comments). \n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52697, "question": "I wouldn't necessarily call them a joke. At the end of the day they accomplish what they set out to do. They provide an introduction to a  non-accessible field for someone who's never done any real machine learning work. But people tend to forget about these limitations. Taking Andrew Ng's coursera course does not make you an expert in machine learning (though he does mislead you into believing you are with a lot of his comments). \n\n", "aSentId": 52698, "answer": "I dunno though - Koller's graphical model course is pretty good and on par with the courses I took at grad school. \n\nHinton's ANN course is frankly amazing and includes some tips you can't even find in the academic literature (or at least couldn't at the time the course was released). You need some background to understand it, but it's a solid course.\n\nThe same for the UFLDL and deeplearning.net tutorials.\n\nWorking on a PhD isn't magic - you do grad courses which are the same that Master's students take and aren't always that great and then you read papers and the resources I mentioned above - you might get lucky and have a really supportive supervisor who helps you, you might not.\n\nA dedicated individual could certainly learn it themselves - in terms of getting a job I'd say a Master's is worthwhile but a PhD can turn into some serious opportunity cost if you don't want to work in academia or on more theoretical topics.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52699, "question": "I dunno though - Koller's graphical model course is pretty good and on par with the courses I took at grad school. \n\nHinton's ANN course is frankly amazing and includes some tips you can't even find in the academic literature (or at least couldn't at the time the course was released). You need some background to understand it, but it's a solid course.\n\nThe same for the UFLDL and deeplearning.net tutorials.\n\nWorking on a PhD isn't magic - you do grad courses which are the same that Master's students take and aren't always that great and then you read papers and the resources I mentioned above - you might get lucky and have a really supportive supervisor who helps you, you might not.\n\nA dedicated individual could certainly learn it themselves - in terms of getting a job I'd say a Master's is worthwhile but a PhD can turn into some serious opportunity cost if you don't want to work in academia or on more theoretical topics.", "aSentId": 52700, "answer": "I think Koller's and Hinton's courses are more the exceptions than the rule. They also are not intended to give someone an introduction to the field. They are geared to people who already have solid foundations in probability, statistics, and machine learning. You cannot take Geoff Hinton's course as a first exposure.\n\nAs far as the opportunity cost of a PhD goes, I don't think it's fair to look at a PhD in terms of opportunity cost. Yes, you'll make less money. But you also get a lot of freedom (especially if you're at a good program) to pursue the problems you want and work on the projects that appeal to you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52701, "question": "I think Koller's and Hinton's courses are more the exceptions than the rule. They also are not intended to give someone an introduction to the field. They are geared to people who already have solid foundations in probability, statistics, and machine learning. You cannot take Geoff Hinton's course as a first exposure.\n\nAs far as the opportunity cost of a PhD goes, I don't think it's fair to look at a PhD in terms of opportunity cost. Yes, you'll make less money. But you also get a lot of freedom (especially if you're at a good program) to pursue the problems you want and work on the projects that appeal to you.", "aSentId": 52702, "answer": "I'm really glad I did Hinton's then...\n\nNGs had a serious lack.of rigor..", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52695, "question": "Definitely agree that most MOOCs are a joke.", "aSentId": 52704, "answer": "What's a MOOC?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52689, "question": "How necessary is an advanced degree to getting a job in the ML/AI field?", "aSentId": 52706, "answer": "Companies are looking for people that understand more than just slapping scikit-learn on a problem and calling it machine learning/data science. There's a ton of math/cs/engineering grads out there in advanced degrees looking to get into the field, and they know these techniques from the ground up. You'd need a very solid portfolio (or connections) if you hope to get in without a Master's+.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52710, "question": "I would HIGHLY recommend getting at least a master's degree. PhD is necessary if you want to be doing any pure research job (ie, developing brand new algorithms), but a master's is still required for most companies to consider you in the typical data science role. The road to VP of Analytics or Chief Data Scientist at a big company will be WAY smoother if you have a PhD. ", "aSentId": 52711, "answer": "Chief Data Scientist, yes, VP of Analytics in many cases a MS in business analysis or a data-focused MBA is a better bet.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52715, "question": "I wonder if the people here saying you should have a PhD in machine learning are saying so out of wishful thinking, because they themselves have one.\n\nIt's certainly true that in many places they're looking for people with advanced degrees to do cutting-edge work \u2013 from startups to R&amp;D to fraud detection at banks and so on.\n\nBut it's equally true that there's such a shortage of people who know anything at all about statistical learning, let alone machine learning, that people who know a little bit of Excel or SQL and can do a regression analysis manage to find jobs as well, and I have faith that these manage to meaningfully help out the organizations they join and have satisfying jobs, and perhaps learn some of the more advanced techniques (and caveats) on the job.\n\nNow, the latter kind of job might not be exactly what you're looking for (especially because you mention AI and don't mention statistics), just be aware that there's options at different levels.", "aSentId": 52716, "answer": "Places want experience - I started a PhD (in computational neuroscience) but mastered out after it appeared my project was hopeless and I became disillusioned with the academic career path.\n\nYou can learn the skills from reading the books and doing the difficult courses like Koller's and Hinton's and practicing on datasets - those courses were just as good as my ones in grad school.\n\nIt's hard work, but it's hard work in grad school too - there is no easy way and ultimately it doesn't matter _where_ you do the work just that you can demonstrate the skills.\n\nBut the skills are knowing things like probabilistic models and information theory and why these things matter. Not just 'import sklearn'", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52717, "question": "Places want experience - I started a PhD (in computational neuroscience) but mastered out after it appeared my project was hopeless and I became disillusioned with the academic career path.\n\nYou can learn the skills from reading the books and doing the difficult courses like Koller's and Hinton's and practicing on datasets - those courses were just as good as my ones in grad school.\n\nIt's hard work, but it's hard work in grad school too - there is no easy way and ultimately it doesn't matter _where_ you do the work just that you can demonstrate the skills.\n\nBut the skills are knowing things like probabilistic models and information theory and why these things matter. Not just 'import sklearn'", "aSentId": 52718, "answer": "I think the key item you gain from doing a PhD versus reading books and practicing on datasets is the people you are exposed to (faculty, post-docs, other phd students, collaborators, people in conferences etc). Simply doing things yourself you can be missing the most critical items in the field for years that someone with true experience can point out to you in minutes. I agree it's possible to find this kind of experience without going the academic route but it is far more than just read some books and do kaggle datasets with code from the internet, you really need to meet the right people.\n\nUnfortunately in research the many concepts are passed down orally, just trying going to a conference, people explain their complex papers with \"we showed this but ignore all this math that was just to get published, what really matters is blah\", try getting this type of knowledge from just reading dense papers that assume you are knowledgeable in the field. \n\nPersonally, I was one of two people doing ML type work at my company for nearly two years and was seen as performing acts of magic and infallible by the rest of the highly qualified programmers who were quite adapt at coding, nobody ever had even an ounce of core algorithm input into what I did. When I started grad school I realized I knew  very little ML and could have done so many things much better. Just my 2 cents.\n\n ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52723, "question": "Personally I think the field is changing so fast and will change so fast you should focus on learning how to constantly teach yourself. I would imagine if you spent a bunch of money on a fancy degree, a few years out of school they won't even be using the same techniques they taught you.", "aSentId": 52724, "answer": "I think that's a bit much.. it's not like Eigenvalues/vectors are going anywhere. The tech might change, but how can you hope to keep up with it if you don't understand the math and logic behind it?\n\nOf course one can learn how to do things without the education, but they're essentially just using memory to push buttons at that point and will be pretty lost when something goes actually goes wrong.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52725, "question": "I think that's a bit much.. it's not like Eigenvalues/vectors are going anywhere. The tech might change, but how can you hope to keep up with it if you don't understand the math and logic behind it?\n\nOf course one can learn how to do things without the education, but they're essentially just using memory to push buttons at that point and will be pretty lost when something goes actually goes wrong.\n", "aSentId": 52726, "answer": "I'm not exactly sure why learning \"math and logic\" is not included in the blanket term \"learning\". \n\nAnd, no it's not a bit much because I do it every day. Believe it or not some people have the self motivation to do things without other people telling them to. \n\nYou seem to have this idea that if anyone self teaches themselves they're inherently teaching themselves a shallow version of the concept and lack the ability to dig further. I'm not exactly sure why that's you're conclusion but I definitely disagree with it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52727, "question": "I'm not exactly sure why learning \"math and logic\" is not included in the blanket term \"learning\". \n\nAnd, no it's not a bit much because I do it every day. Believe it or not some people have the self motivation to do things without other people telling them to. \n\nYou seem to have this idea that if anyone self teaches themselves they're inherently teaching themselves a shallow version of the concept and lack the ability to dig further. I'm not exactly sure why that's you're conclusion but I definitely disagree with it.", "aSentId": 52728, "answer": "I agree, I probably did make a large assumption that that's not entirely correct. Some people certainly can learn effectively on their  own, that is true.\n\nHowever, it's been my experience that the majority of the self-taught professionals I've interacted with get to a point where they know button X does this, Y does that and move on. With respect to the job market, I think that's where the difference comes in. IMO, it's more likely that of two candidates having the same amount of work experience (self-taught vs classical education), the person who finished an MS or PhD would have a better understanding of the field than someone who's self taught. Of course there are exceptions, and that difference may diminish with years of experience,  but that's where my statement was coming from.\n\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52732, "question": "Implementing Reinforcement Learning", "aSentId": 52733, "answer": "What is described in this article is Dynamic Programming for control in Markov Decision Processes. This comes largely from the work of Bellman in the 1950s and has since been studied extensively in the fields of operations research and stochastic control. On the other hand, Reinforcement Learning (RL) grew mostly in the 1980s with Richad Sutton and Andrew Barto's work. RL uses the MDP formalism but generally assumes that we do not know the transition dynamics (the \"P\" matrix) or the reward function: it is inherently model-free and online. Estimating a model from data and applying value iteration or policy iteration is therefore not in the original spirit of RL.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52735, "question": "ICLR2015: Memory Networks (Facebook AI Research)", "aSentId": 52736, "answer": "They improved/updated model already, paper about more advanced one : http://arxiv.org/abs/1502.05698", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52735, "question": "ICLR2015: Memory Networks (Facebook AI Research)", "aSentId": 52738, "answer": "These guys work at Facebook. Does anybody see Facebook incorporating this kind of memory model into their system any time soon? It would be interesting to see what they can do with it using the data they have.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52740, "question": "I've just read Grammar as a Foreign Language Vinyals et al. and isn't this a possible solution for training deep nets on small data?", "aSentId": 52741, "answer": "This is done for speech sometimes by using pretrained HMMs go over unlabeled speech and give alignments/phoneme labels, then train your RNN recognizer over that. It is not great but it works - another approach is to predict HMM states directly using an MLP or the like.\n\nThe main problem is that you will only get incremental gains from this - deep learning people love to *smash* benchmarks :) When there is a huge amount of unused data that is what people go for first. Weak labels or automatically (but noisily) labeled data is another interesting area. But it is a good idea in general where there are many, many years of handcrafted engineering before you, and if you expect that more labeled data will be coming around the corner soon. Why throw out the last 50 years of engineering if you can use it to boost your score?\n\nIt is a similar idea to bootstrapping a compiler I think, cool idea but very time consuming especially in the era of Mechanical Turk. Much easier to pay a few bucks and build a large dataset.\n\nReally, almost anything that is an input-output system can be approximated with a neural net. The question is whether the extra complexity of an RNN was worth it compared to a fast hand tuned parser. I think it is, but NLP people might disagree.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52742, "question": "This is done for speech sometimes by using pretrained HMMs go over unlabeled speech and give alignments/phoneme labels, then train your RNN recognizer over that. It is not great but it works - another approach is to predict HMM states directly using an MLP or the like.\n\nThe main problem is that you will only get incremental gains from this - deep learning people love to *smash* benchmarks :) When there is a huge amount of unused data that is what people go for first. Weak labels or automatically (but noisily) labeled data is another interesting area. But it is a good idea in general where there are many, many years of handcrafted engineering before you, and if you expect that more labeled data will be coming around the corner soon. Why throw out the last 50 years of engineering if you can use it to boost your score?\n\nIt is a similar idea to bootstrapping a compiler I think, cool idea but very time consuming especially in the era of Mechanical Turk. Much easier to pay a few bucks and build a large dataset.\n\nReally, almost anything that is an input-output system can be approximated with a neural net. The question is whether the extra complexity of an RNN was worth it compared to a fast hand tuned parser. I think it is, but NLP people might disagree.", "aSentId": 52743, "answer": "One could not pay few bucks to MechTurks to label MRIs, CAT scans, mammograms etc etc. Or think about those microscopy images of ocean bacterias from Kaggle competition. \n\nSmall data is here and here to stay. And the problem of learning from little data is solvable. Vicarious claims that their CAPTCHA recognizer was trained on 1000 examples. I do **not** think they are lying. \n\nCould any state of the art convnet break ReCaptcha from Google if it would be trained only using 1k labeled examples? There is no freaking way to do this. At least not now. But with millions of examples Ian Goodfelow and whoever were his coauthors broke ReCaptcha with plain vanilla maxout convnet. \n\nI doubt Vicarious come up with something special or HTM got magical advantages over widely used models, if it were the case academia would had already picked HTM up and researched it. But setting HTM aside...\n\nIf there is few thousands(dozens k) examples of something it would still be possible to train deep model. Not a big one, or it would overfit. Then create synthetic weakly labeled dataset, pretrain bigger model on it and finetune it on labeled data. Repeat cycle using now better model to create even better one. \n\nWill this work? It might not. It might. In paper referred above they gone from 90.4 F-measure model to 91.6 one. Just doing this cycle once. In last 10 years state of the art on that dataset improved from 90.4 to 92.4... ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52740, "question": "I've just read Grammar as a Foreign Language Vinyals et al. and isn't this a possible solution for training deep nets on small data?", "aSentId": 52745, "answer": "Imagine trying to learn the names of animals from a small child who keeps telling you pictures of lions, panthers, and tigers are all \"kitty cat\". \n\nYou can't do much better than your teacher with supervised learning, no matter how much unlabelled data you \"label\".", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52749, "question": "how difficult would it be to build a math tutor ai that is useful (ie not useless simplistic)", "aSentId": 52750, "answer": "I think the best thing that a \"math tutor ai\" could do, is figure out what errors the student is making during what ever procedure they're learning (here I'm assuming you're talking primary-high school maths tutor) and correct those.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52749, "question": "how difficult would it be to build a math tutor ai that is useful (ie not useless simplistic)", "aSentId": 52752, "answer": "If the AI tutor were to do something more than output canned responses to the student, it would have to, at some level, understand the problem at hand enough to identify an error.  I could see an algebra tutor being built off ideas from [this paper](http://yoavartzi.com/pub/kazb-acl.2014.pdf), but it would be crude.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52756, "question": "Help a newbie? Train simple neural network on image.", "aSentId": 52757, "answer": "CNN is over kill for what you want to do.  You could just do KS on image histograms.  Or KS on wavelet coefficients.  Or KS on Fourier coefficients.  Or any number of simpler methods.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52759, "question": "Hands-On GPU-Accelerated Machine Learning Training at GTC 2015", "aSentId": 52760, "answer": "will these courses be freely available online?\nDo you have any other learning ressources for GPU machine learning beginners?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52762, "question": "Job Title disambiguation?", "aSentId": 52763, "answer": "Could attach some job description features (certain tech skills, typical degrees) to each job title? It increases dimensions but then you could cluster Job titles by ones with a certain level of similar skills.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52764, "question": "Could attach some job description features (certain tech skills, typical degrees) to each job title? It increases dimensions but then you could cluster Job titles by ones with a certain level of similar skills.", "aSentId": 52765, "answer": "unfortunately thats not an option :( just the title.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52767, "question": "[Question] Which metric to trust ?", "aSentId": 52768, "answer": "It depends; do your labels have meaning?  \n\nMSE isn't appropriate for classification since labels are artificial constructs we can set to {0,1} or {-1,+1} or {-500, 300}...whatever, as long as the classes are distinct.  The reason we often set them to {0,1} is because it allows for a probabilistic interpretation: the label is the outcome of a Bernoulli trial.  The classifier can then be seen as learning the parameter *p* in the Bernoulli pmf   p^x * (1-p)^1-x   where *x* is the label.  During training we want to minimize the negative log likelihood of the product of all the Bernoulli trials: Loss = -log[ (p)^x1 * (1-p)^1-x1 * p^x2 * (1-p)^1-x2 * ... * (p)^xN * (1-p)^1-xN ] = -sum( xi log p + (1-xi) log (1-p) ).  This is the well known *cross-entropy* error function, which is appropriate when the task is pure supervised classification.\n\nMSE also has a probabilistic interpretation.  It is the negative log likelihood of the Normal pdf: -log[ 1/sqrt(2pi) * exp{-1/2 * (x - m)^2 } ]  --&gt; 1/2 * (x - m)^2 where *x* is the label, *m* is the mean (which is the predictive model), and '---&gt;' means 'proportional to' since we can drop the 1/sqrt(2pi) because it doesn't depend on the data or model.  Computing this over all the data places a sum in front as it did for the Bernoulli above. \n\nHopefully that clears things up.  To summarize: if the labels represent just classes, negative log likelihood and/or accuracy are good evaluation metrics with the latter being the most interpretable and the former being better for gauging training progress.  If the labels have more meaning than class assignment and are continuous, then MSE is probably more appropriate.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52769, "question": "It depends; do your labels have meaning?  \n\nMSE isn't appropriate for classification since labels are artificial constructs we can set to {0,1} or {-1,+1} or {-500, 300}...whatever, as long as the classes are distinct.  The reason we often set them to {0,1} is because it allows for a probabilistic interpretation: the label is the outcome of a Bernoulli trial.  The classifier can then be seen as learning the parameter *p* in the Bernoulli pmf   p^x * (1-p)^1-x   where *x* is the label.  During training we want to minimize the negative log likelihood of the product of all the Bernoulli trials: Loss = -log[ (p)^x1 * (1-p)^1-x1 * p^x2 * (1-p)^1-x2 * ... * (p)^xN * (1-p)^1-xN ] = -sum( xi log p + (1-xi) log (1-p) ).  This is the well known *cross-entropy* error function, which is appropriate when the task is pure supervised classification.\n\nMSE also has a probabilistic interpretation.  It is the negative log likelihood of the Normal pdf: -log[ 1/sqrt(2pi) * exp{-1/2 * (x - m)^2 } ]  --&gt; 1/2 * (x - m)^2 where *x* is the label, *m* is the mean (which is the predictive model), and '---&gt;' means 'proportional to' since we can drop the 1/sqrt(2pi) because it doesn't depend on the data or model.  Computing this over all the data places a sum in front as it did for the Bernoulli above. \n\nHopefully that clears things up.  To summarize: if the labels represent just classes, negative log likelihood and/or accuracy are good evaluation metrics with the latter being the most interpretable and the former being better for gauging training progress.  If the labels have more meaning than class assignment and are continuous, then MSE is probably more appropriate.", "aSentId": 52770, "answer": "Curious! What do you mean by \"have meaning\", do you refer to something specific, to re-ask the same question in another way: how can we measure \"lavble have meaning and class assignement \"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52771, "question": "Curious! What do you mean by \"have meaning\", do you refer to something specific, to re-ask the same question in another way: how can we measure \"lavble have meaning and class assignement \"", "aSentId": 52772, "answer": "Maybe 'meaning' isn't the right word since class labels do have meaning in the sense that they represent class membership.  The key issue is that they are artificially constructed and interchangeable.  For example, in a two class problem where I want to discriminate cats from dogs, I can let cats be represented as '0' and dogs be '1' or vice versa.  However, if I want to learn a model to predict, say, the height of an animal based on its food consumption, the label (also called 'response') height has a fixed physical meaning and I must keep the relationship between the observed heights intact.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52774, "question": "Any one know any good survey papers on image classification ?", "aSentId": 52775, "answer": "The literature may be more focused than you expect. I don't think you will find a good survey paper on pros/cons of methods on a standard data set. Instead, it is much more common to find survey papers that are focused around a class of algorithms.\n\nThere are some good conferences that post their accepted papers online for anyone to access. For example, here are last years ICML papers: http://icml.cc/2014/index/article/15.htm\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 52776, "question": "The literature may be more focused than you expect. I don't think you will find a good survey paper on pros/cons of methods on a standard data set. Instead, it is much more common to find survey papers that are focused around a class of algorithms.\n\nThere are some good conferences that post their accepted papers online for anyone to access. For example, here are last years ICML papers: http://icml.cc/2014/index/article/15.htm\n", "aSentId": 52777, "answer": "&gt;I don't think you will find a good survey paper on pros/cons of methods on a standard data set. \n\nAny reason why ? To my noob mind that seems like a good paper idea. \n\nIsn't that what survey papers do ? Apply different algorithms to solve a similar problem and compare them ? \n\n", "corpus": "reddit"}]