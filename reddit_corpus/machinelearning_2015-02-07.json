[{"docID": "t5_2r3gv","qSentId": 30877,"question": "Andrej Karpathy's slides on Generating Image Description using Deep Learning","aSentId": 30878,"answer": "Very interesting. What ever happened to canonical correlation analysis? ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30880,"question": "Need help finding a talk: The Unreasonable Effectiveness of Deep Learning by Yann LeCun","aSentId": 30881,"answer": "He goes into sparse autoencoders [here](http://videolectures.net/sahd2014_lecun_deep_learning/?q=unreasonable%20effectiveness), but I don't know if that's what you're after. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30885,"question": "What do you do at work?","aSentId": 30886,"answer": "I spend a lot of my time arguing with workmates about whether or not the analysis they've done means anything, or if it has just produced a pretty picture.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30887,"question": "I spend a lot of my time arguing with workmates about whether or not the analysis they've done means anything, or if it has just produced a pretty picture.","aSentId": 30888,"answer": "...but...but.... Look at all of these pretty colors! This just HAS to be important! ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30885,"question": "What do you do at work?","aSentId": 30890,"answer": "The company I work for captures data from hospital network EMRs/EHRs, financial systems, etc. and does all sorts of cool analysis of that data. We make the deidentified data available to all customers, we take identifiable data across disparate systems and match patients so that the hospital network can have one virtual chart. We also develop risk scores, benchmarks that physicians should be hitting for their patients, among other things. Our goal is to help physicians and hospital networks identify issues for patients before it sends them to the ER or kills them. We present that data back to them through a few web applications we develop.\n\nWe also take the standardized data and make it available back to the customer so they can query their own data and do their own analysis on their patients. My job is to maintain that database and develop new features from the data, creating new denormalized datasets to make their analysis easier. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30891,"question": "The company I work for captures data from hospital network EMRs/EHRs, financial systems, etc. and does all sorts of cool analysis of that data. We make the deidentified data available to all customers, we take identifiable data across disparate systems and match patients so that the hospital network can have one virtual chart. We also develop risk scores, benchmarks that physicians should be hitting for their patients, among other things. Our goal is to help physicians and hospital networks identify issues for patients before it sends them to the ER or kills them. We present that data back to them through a few web applications we develop.\n\nWe also take the standardized data and make it available back to the customer so they can query their own data and do their own analysis on their patients. My job is to maintain that database and develop new features from the data, creating new denormalized datasets to make their analysis easier. ","aSentId": 30892,"answer": "I do much of the same. Minus making deidentified data available to everyone. Our predictive modeling centers around trying to identify issues before they happen (hospital/ER readmissions, acute metabolic syndrome, etc). We eventually end up with risk scores that are sent out to a network of care managers, heath educators, and relevant healthcare providers. I'm about to undertake the task of tweaking some of these models and building new ones now that we have new data streams. \n\nAs for the machine learning, we get data from countless different EMR's Some spit out beautifully clean data that has all of the medications, allergies, procedures, labs, etc mapped to standardized code sets. A lot of EMRs feed us crap. Unstandardized, free form text fields full of misspellings, irrelevant info, incomplete names of tests/medications, etc, and in order to do anything useful with this data it needs to be cleaned and mapped to standardized code sets. I use machine learning to complete this. When I first started it was done one by one, taking an incredible amount of man hours (read expensive). Now we are matching upwards of 90% of the data and constantly improving. \n\nMy next project is to mine several years worth of patient encounter summaries, use some ML to see if I can identify anything interesting, and the ultimately feed that into our predictive risk models. \n\nWith such messy data there can be a ton of grunt work just getting the data to be marginally useful, and I'd say that there are maybe 5 people in the entire company that actually understand what I do which as you can imagine leads to incredibly frustrating conversations when people ask for the impossible. But overall I love what I do and it's a blast to have the ability to constantly learn and explore new stuff. \n\nIf I wouldn't get canned for it (or sued into the ground by legal) I'd love to compare and contrast algorithms!  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30893,"question": "I do much of the same. Minus making deidentified data available to everyone. Our predictive modeling centers around trying to identify issues before they happen (hospital/ER readmissions, acute metabolic syndrome, etc). We eventually end up with risk scores that are sent out to a network of care managers, heath educators, and relevant healthcare providers. I'm about to undertake the task of tweaking some of these models and building new ones now that we have new data streams. \n\nAs for the machine learning, we get data from countless different EMR's Some spit out beautifully clean data that has all of the medications, allergies, procedures, labs, etc mapped to standardized code sets. A lot of EMRs feed us crap. Unstandardized, free form text fields full of misspellings, irrelevant info, incomplete names of tests/medications, etc, and in order to do anything useful with this data it needs to be cleaned and mapped to standardized code sets. I use machine learning to complete this. When I first started it was done one by one, taking an incredible amount of man hours (read expensive). Now we are matching upwards of 90% of the data and constantly improving. \n\nMy next project is to mine several years worth of patient encounter summaries, use some ML to see if I can identify anything interesting, and the ultimately feed that into our predictive risk models. \n\nWith such messy data there can be a ton of grunt work just getting the data to be marginally useful, and I'd say that there are maybe 5 people in the entire company that actually understand what I do which as you can imagine leads to incredibly frustrating conversations when people ask for the impossible. But overall I love what I do and it's a blast to have the ability to constantly learn and explore new stuff. \n\nIf I wouldn't get canned for it (or sued into the ground by legal) I'd love to compare and contrast algorithms!  ","aSentId": 30894,"answer": "Very interesting. Do you publish research on your findings, or plan to? I think there must be so much we could learn if we started doing real research on EHR data.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30891,"question": "The company I work for captures data from hospital network EMRs/EHRs, financial systems, etc. and does all sorts of cool analysis of that data. We make the deidentified data available to all customers, we take identifiable data across disparate systems and match patients so that the hospital network can have one virtual chart. We also develop risk scores, benchmarks that physicians should be hitting for their patients, among other things. Our goal is to help physicians and hospital networks identify issues for patients before it sends them to the ER or kills them. We present that data back to them through a few web applications we develop.\n\nWe also take the standardized data and make it available back to the customer so they can query their own data and do their own analysis on their patients. My job is to maintain that database and develop new features from the data, creating new denormalized datasets to make their analysis easier. ","aSentId": 30896,"answer": "1. What are your favorite tools?\n\n2. Do you plan to publish research on your findings?\n\n3. Where are you geographically? East Coast, West Coast? Boston? Bay Area?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30897,"question": "1. What are your favorite tools?\n\n2. Do you plan to publish research on your findings?\n\n3. Where are you geographically? East Coast, West Coast? Boston? Bay Area?","aSentId": 30898,"answer": "1) I don't actually do any of the analytic work myself. My role is getting the data from our hadoop grid to our columnar database for our customers and for our life sciences customers. I know informatics and analytics uses a lot of R and Python-- I assume scikit-learn, pandas, etc. I took this job in hopes of moving into the analytics somewhere down the road. Right now it's just a hobby for me. \n\n2) I'm not sure if we do publish or where we publish if we do. If I find out I'll be sure to come back and update.\n\n3) We're in Cleveland, OH.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30891,"question": "The company I work for captures data from hospital network EMRs/EHRs, financial systems, etc. and does all sorts of cool analysis of that data. We make the deidentified data available to all customers, we take identifiable data across disparate systems and match patients so that the hospital network can have one virtual chart. We also develop risk scores, benchmarks that physicians should be hitting for their patients, among other things. Our goal is to help physicians and hospital networks identify issues for patients before it sends them to the ER or kills them. We present that data back to them through a few web applications we develop.\n\nWe also take the standardized data and make it available back to the customer so they can query their own data and do their own analysis on their patients. My job is to maintain that database and develop new features from the data, creating new denormalized datasets to make their analysis easier. ","aSentId": 30900,"answer": "How do you deal with HIPAA?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30901,"question": "How do you deal with HIPAA?","aSentId": 30902,"answer": "We have very strict and tight policies depending on what we're doing with the data. The deidentified data that we make available to all doesn't have any HIPAA implications because you can never get back to an actual person. There are a lot of things we do on the application side and the back end to make sure of this and we have external auditors come in to validate that (not sure how often).","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30904,"question": "Cool! I had an opportunity to interview at a company like that but turned it down for something better ","aSentId": 30905,"answer": "&gt; Comment history: frequents /r/actuary and /r/TheRedPill","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30906,"question": "&gt; Comment history: frequents /r/actuary and /r/TheRedPill","aSentId": 30907,"answer": "damn straight","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30885,"question": "What do you do at work?","aSentId": 30909,"answer": "I'm a PhD student trying to subtype and predict human aggression based on genetic, brain imaging, and environmental data. The end goal is to find the best predictive features which may point to biologically relevant areas and provide targets for treatment.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30910,"question": "I'm a PhD student trying to subtype and predict human aggression based on genetic, brain imaging, and environmental data. The end goal is to find the best predictive features which may point to biologically relevant areas and provide targets for treatment.","aSentId": 30911,"answer": "so..basically you're trying to create an algorithmic prison for society and imprison them before they do anything.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30913,"question": "I build credit risk and fraud models using public records data. Due to regulation regarding disparate impact this is usually done through boring logistic regression. But lately we have been using GBM and randomForests to good effect.","aSentId": 30914,"answer": "sounds good.  why do so many people hate on logistic regression? dont get me wrong..im a decision tree guy myself but think logistic regression is useful when dealing with large amounts of features ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30915,"question": "sounds good.  why do so many people hate on logistic regression? dont get me wrong..im a decision tree guy myself but think logistic regression is useful when dealing with large amounts of features ","aSentId": 30916,"answer": "Logistic regression won't learn complex interactions or feature transformations unless you already know what they are and can put them in the model.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30918,"question": "I'm a Master's student working on extracting identifying features from pictures of humpback whale flukes (e.g. [this](http://www.alaska-in-pictures.com/data/media/3/humpback-whale-fluke_6446.jpg), but usually way worse quality) so they can be tracked by ecologists.\n\nI'm also finishing my undergrad so it's not my main focus, but it is my machine learning work.","aSentId": 30919,"answer": "This is pretty cool. What techniques are you using to compare the markings?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30923,"question": "I work in the legal industry, eDiscovery, where we use ML to classify, cluster, and perform topic modeling of million of text documents (scanned and OCR'd, extracted). We also use data mining techniques to identify near duplicate documents, reassemble email chains, analysis of email network graphs, and that's just the text portion of it.\n\nThere's very few companies doing anything with all the (typically) images, even scanned documents that OCR is useless on. \n\nBasically, any and everything you can do to help lawyers find what they're looking for among the sea of crap. Typical cases are 5-15% relevant, out of MILLIONS of documents, and the goal is to find just those.\n\nAnd it's interesting and fascinating because your performance translates almost directly to a sizeable sum of money. If we can reduce 1 million documents to 100,000 (80% recall or similar), that means contract lawyers that typically do these reviews have to look at 900k less documents. At quoted rates of a couple dollars to as high as $8 per document in a review, that translates to millions of dollars.","aSentId": 30924,"answer": "Awesome! \n\nWhat would you say are the key skills for being a good data scientist in text mining? I'm a data scientist focusing more on health and marketing data, I don't get much text (yet).","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30925,"question": "Awesome! \n\nWhat would you say are the key skills for being a good data scientist in text mining? I'm a data scientist focusing more on health and marketing data, I don't get much text (yet).","aSentId": 30926,"answer": "Judgement, careful analysis, an insatiable appetite for learning, and experience :)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30927,"question": "Judgement, careful analysis, an insatiable appetite for learning, and experience :)","aSentId": 30928,"answer": "Hah... this one doesn't kiss and tell!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30931,"question": "Training a neural net; an intuition for weight variation","aSentId": 30932,"answer": "&gt; Yet, I am struck how local minima with similar performance and loss can have such dramatically different weights. \n\nI think you want to look into unidentifiability. A lot of statistical models have many symmetries in their parameters, so you can reorder params/negate params/rescale params/etc and see no difference in the result. This means that a single optimal configuration can manifest as a huge number of separate minima.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30933,"question": "&gt; Yet, I am struck how local minima with similar performance and loss can have such dramatically different weights. \n\nI think you want to look into unidentifiability. A lot of statistical models have many symmetries in their parameters, so you can reorder params/negate params/rescale params/etc and see no difference in the result. This means that a single optimal configuration can manifest as a huge number of separate minima.","aSentId": 30934,"answer": "&gt; I am working with some feedforward neural network models. Repeatedly training a network (using backprobagation) on the same data, I get a different set of weights. The loss function output (MSE) and predictive performance of each network are very similar. The different models clearly represent different but comparably performant local minima.\n&gt;\n\nWould this suggest that if /u/osazuwa wants to find a optima, or at least minimize redundant, equally performing local optima, they should either \n\n* constrain enough parameters of their model such that there is no more scaling that could result in equivalent effective model? \n* represent all the params in their current model is a convex function on a smaller, identifiable params?\n\n(just double checking my thinking)\n\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30931,"question": "Training a neural net; an intuition for weight variation","aSentId": 30936,"answer": "A simple, intuitive example is a 2 layer neural network with one node in each layer trying to learn the identity function.\n\nOutput = Input * w1 * w2\n\nNow all we need is w1 * w2 to equal 1 in order to learn the solution. There are an infinite number of solutions e.g. for (w1,w2) we could have: (1,1) (0.5, 2) (0.25, 4).\n\nObviously it's not clear this intuition holds after adding nonlinearities and more nodes but Yann Le Cunn gave a talk recently about some polynomial analysis of deep networks and argued this intuition does in fact generalize. I only heard about it second hand though..","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30938,"question": "How to find computer vision / image processing PhD topic ideas?","aSentId": 30939,"answer": "Autonomous vehicles using visual spectrum cameras! ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30938,"question": "How to find computer vision / image processing PhD topic ideas?","aSentId": 30941,"answer": "Start working on something that you and your advisor agree on. A PhD is seldom a lone pursuit. Find an advisor or a mentor you can talk to, to find which projects to work on. Try a handful. The first few projects might not work. But the process of doing them will allow you to connect dots across concepts and help you develop a workflow. Read papers at the same time. Keep a strict schedule. Pay attention to paper submission deadlines. They are great things that allow you to get a lot done to manage your PhD research work.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30942,"question": "Start working on something that you and your advisor agree on. A PhD is seldom a lone pursuit. Find an advisor or a mentor you can talk to, to find which projects to work on. Try a handful. The first few projects might not work. But the process of doing them will allow you to connect dots across concepts and help you develop a workflow. Read papers at the same time. Keep a strict schedule. Pay attention to paper submission deadlines. They are great things that allow you to get a lot done to manage your PhD research work.","aSentId": 30943,"answer": "It also helps to work with an open source toolkit that has a great community around it. That developer community will answer a lot of your questions, don't be afraid to get under the hood. Learn to use a good debugger.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30938,"question": "How to find computer vision / image processing PhD topic ideas?","aSentId": 30945,"answer": "I'm going to do you a service and tell you that if you're unable to find a thesis topic, then maybe you shouldn't be getting a PhD (Please read before downvoting: Getting a PhD in CS is far different from getting a PhD in chemistry, bio, etc).  It seems like you didn't do enough research to figure out if getting a PhD even makes sense for you (it absolutely does NOT make sense for most people in computer science).  If you 100% want to become a professor, then I guess it's the route that you have to take; however, you're the one who's supposedly interested in this field, yet you ask reddit for a topic?  That doesn't make any sense, buddy.  \n\nYou either haven't done enough research in the field, in the merits of getting a PhD or you're unable to be independent--all of which are not good.  Some people will tell you that it's okay to not know exactly what you want to do in your PhD.  I think that ONLY makes sense if you 100% know that you want to be a professor (a decision that I would hope is based on a lot of experience in TAing, doing research, talking with professors about what they actually do on a daily basis, etc), especially in CS.  You're giving up a lot of potential salary to pursue a degree that only very few jobs actually care about--in fact, the PhD can even make you a worse applicant at a lot of place (especially startups).  On top of that, the jobs that actually require one to have a PhD are few and far between.  \n\nSo yeah, if I were you, I'd make sure that getting a PhD is what you want.  Doing it for the sake of doing it/delaying getting out of the academia/etc is going to bite you in the ass, and if you're intelligent, you're really going to regret it.  Getting a PhD can be a great decision for some, but with respect to computer science, I'd say that it's not a good decision for most people.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30946,"question": "I'm going to do you a service and tell you that if you're unable to find a thesis topic, then maybe you shouldn't be getting a PhD (Please read before downvoting: Getting a PhD in CS is far different from getting a PhD in chemistry, bio, etc).  It seems like you didn't do enough research to figure out if getting a PhD even makes sense for you (it absolutely does NOT make sense for most people in computer science).  If you 100% want to become a professor, then I guess it's the route that you have to take; however, you're the one who's supposedly interested in this field, yet you ask reddit for a topic?  That doesn't make any sense, buddy.  \n\nYou either haven't done enough research in the field, in the merits of getting a PhD or you're unable to be independent--all of which are not good.  Some people will tell you that it's okay to not know exactly what you want to do in your PhD.  I think that ONLY makes sense if you 100% know that you want to be a professor (a decision that I would hope is based on a lot of experience in TAing, doing research, talking with professors about what they actually do on a daily basis, etc), especially in CS.  You're giving up a lot of potential salary to pursue a degree that only very few jobs actually care about--in fact, the PhD can even make you a worse applicant at a lot of place (especially startups).  On top of that, the jobs that actually require one to have a PhD are few and far between.  \n\nSo yeah, if I were you, I'd make sure that getting a PhD is what you want.  Doing it for the sake of doing it/delaying getting out of the academia/etc is going to bite you in the ass, and if you're intelligent, you're really going to regret it.  Getting a PhD can be a great decision for some, but with respect to computer science, I'd say that it's not a good decision for most people.","aSentId": 30947,"answer": "I'm not in US. I'm in Central Europe, though relocation is easy. There are not many jobs in which you can utilize CV. Using ML to optimize cashflow or improve turbine power output by 0.1 percent is not really my idea of good time.\n\nThe other type of jobs I know are about being code monkey until you are 40 and then maybe get promoted to management, push code monkeys around and get heart-attack at 50 due to missed deadlines.\n\nI'm probably naive, but I would rather do something interesting than shovel data from pile A to pile B 50hrs/week.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30948,"question": "I'm not in US. I'm in Central Europe, though relocation is easy. There are not many jobs in which you can utilize CV. Using ML to optimize cashflow or improve turbine power output by 0.1 percent is not really my idea of good time.\n\nThe other type of jobs I know are about being code monkey until you are 40 and then maybe get promoted to management, push code monkeys around and get heart-attack at 50 due to missed deadlines.\n\nI'm probably naive, but I would rather do something interesting than shovel data from pile A to pile B 50hrs/week.","aSentId": 30949,"answer": "I'm not familiar with the non-US scene; however, you can definitely do non-code monkey jobs without a PhD.  It's pretty absurd to think all of the brilliant engineering without PhDs (this is a simple numbers game, there are far more brilliant engineers without PhDs than with PhDs) are code monkeys.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30951,"question": "Go get a job and quit wasting time in school, unless you're already filthy rich\n\nSchool's awesome, but making $150k+ is even more awesome","aSentId": 30952,"answer": "Where exactly do you work, making $150k+ (and can you get me in)?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30954,"question": "LeCun: \"Text Understanding from Scratch\"","aSentId": 30955,"answer": "The title should be \n\nZhang: \"Text Understanding from Scratch\"\n\n\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30956,"question": "The title should be \n\nZhang: \"Text Understanding from Scratch\"\n\n\n","aSentId": 30957,"answer": "I wish I could up vote this 100times. It's really unfair how the first authors are usually ignored because a \"superstar\" is on the list of authors. I saw a paper a few weeks ago where Yoshua Bengio was on a long list of authors but the paper was also dubbed \"Bengio blah blah\". Really not cool! OP/Moderator should make the change accordingly.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30958,"question": "I wish I could up vote this 100times. It's really unfair how the first authors are usually ignored because a \"superstar\" is on the list of authors. I saw a paper a few weeks ago where Yoshua Bengio was on a long list of authors but the paper was also dubbed \"Bengio blah blah\". Really not cool! OP/Moderator should make the change accordingly.","aSentId": 30959,"answer": "it's a lot easier to remember half a dozen superstars than all of their grad students. \n\nand they benefit by being associated with the big name since hardly anyone will bother reading papers fully.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30956,"question": "The title should be \n\nZhang: \"Text Understanding from Scratch\"\n\n\n","aSentId": 30961,"answer": "Don't worry, he'll get credit. You come for the brand name and stay for the up-and-coming grad student who did all the work.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30956,"question": "The title should be \n\nZhang: \"Text Understanding from Scratch\"\n\n\n","aSentId": 30963,"answer": "You're right, but the paper is worth reading and frankly I knew that people would give it a look given LeCun's history and name recognition. \n\nI don't mean to diminish Zhang's work at all. Despite the issues identified in other comments I do think he has accomplished something meaningful here.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30964,"question": "You're right, but the paper is worth reading and frankly I knew that people would give it a look given LeCun's history and name recognition. \n\nI don't mean to diminish Zhang's work at all. Despite the issues identified in other comments I do think he has accomplished something meaningful here.","aSentId": 30965,"answer": "Quite frankly, I was more attracted to the main title than I was by the name. If not for the comment above, I might never have known or realized something was up.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30966,"question": "Quite frankly, I was more attracted to the main title than I was by the name. If not for the comment above, I might never have known or realized something was up.","aSentId": 30967,"answer": "Yeah. 2 Authors, one a grad student. You know the student deserves a big chunk of the credit either way.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30954,"question": "LeCun: \"Text Understanding from Scratch\"","aSentId": 30969,"answer": "Hmm.. I respect the authors immensely, but there are points in the paper which are not clear for me.\n\nThe baseline models take only single words in account, while ConvNet is allowed to look at the whole text. An obvious question: is the extra quality a result of more information available to the classifier, or is it a result of some ConvNet advantages?\n\nI think it makes sense to compare ConvNet with a classifier trained on character-level ngrams. One can apply a classifier trained on char-level ngrams to ontology classification, sentiment analysis, and text categorization problems; they should work well. It doesn't mean we've got \"text understanding from characterlevel inputs all the way up to abstract text concepts\". \n\nChar-level BoW model and a ConvNet will have access to the same information, and the difference between them would be attributed to ConvNet qualities.\n\nBag-of-words model they use is also very restricted - why limit the vocabulary just to 5000 words? I'm not sure it is how BoW models are commonly used. It could be more fair \u0435o do e.g. PCA on full vectors, or use vectors directly - they are sparse, so high dimension is not necessarily a problem. For sentiment analysis of long reviews handling of more than one word could help - unigram BoW model can't learn negation. \n\nI'm sure authors already though about it, and there is a reason such baselines were chosen. Could please someone explain it? Any ideas are welcome!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30954,"question": "LeCun: \"Text Understanding from Scratch\"","aSentId": 30971,"answer": "&gt;  ConvNets do not require knowledge of syntax or semantic structures \u2013 inference directly to high-level targets is fine. This also invalidates the assumption that\nstructured predictions and language models are necessary for high-level text understanding.\n\nIs this usage of \"text understanding\" common in the machine learning community? \n\nWhile there is no universally agreed-upon definition of what it means to \"understand\" a text, most linguists and NLP researchers would probably agree that it involves something like being able to answer questions like \"Who did what to whom, when, how, and why?\"\n\nThe almost 30-year-old [Norvig paper](http://norvig.com/aaai87.pdf) [pdf] cited in the introduction considers text understanding to even involve being able to make inferences. This is a far cry from the text classification experiments by Zhang &amp; LeCun.\n\nNow, if you define \"high-level text understanding\" to mean \"text classification\", then Zhang &amp; LeCun indeed show that you don't need to consider structure to complete the task, but I'm not aware of anyone who claims that you do.\n\nFurthermore, even with that definition, I don't think the claim that you don't need language models is valid. Exactly like character n-gram language models, ConvNets are trained on character sequences and make their predictions based on character sequences. \n\nPerformance is also similar: In texts from rather distinct domains (the 14 manually-picked DBpedia classes, Amazon polar reviews, news categories) both n-gram models and ConvNets perform well, while accuracy drops for less distinct domains (Yahoo! Answers). So it shouldn't be too far of a stretch to see the ConvNets trained by Zhang &amp; LeCun as sophisticated language models.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30972,"question": "&gt;  ConvNets do not require knowledge of syntax or semantic structures \u2013 inference directly to high-level targets is fine. This also invalidates the assumption that\nstructured predictions and language models are necessary for high-level text understanding.\n\nIs this usage of \"text understanding\" common in the machine learning community? \n\nWhile there is no universally agreed-upon definition of what it means to \"understand\" a text, most linguists and NLP researchers would probably agree that it involves something like being able to answer questions like \"Who did what to whom, when, how, and why?\"\n\nThe almost 30-year-old [Norvig paper](http://norvig.com/aaai87.pdf) [pdf] cited in the introduction considers text understanding to even involve being able to make inferences. This is a far cry from the text classification experiments by Zhang &amp; LeCun.\n\nNow, if you define \"high-level text understanding\" to mean \"text classification\", then Zhang &amp; LeCun indeed show that you don't need to consider structure to complete the task, but I'm not aware of anyone who claims that you do.\n\nFurthermore, even with that definition, I don't think the claim that you don't need language models is valid. Exactly like character n-gram language models, ConvNets are trained on character sequences and make their predictions based on character sequences. \n\nPerformance is also similar: In texts from rather distinct domains (the 14 manually-picked DBpedia classes, Amazon polar reviews, news categories) both n-gram models and ConvNets perform well, while accuracy drops for less distinct domains (Yahoo! Answers). So it shouldn't be too far of a stretch to see the ConvNets trained by Zhang &amp; LeCun as sophisticated language models.","aSentId": 30973,"answer": "Le Cun  and Hinton (ala his AAAI talk)  and others are making (imo catty) swipes at symbolism.  They're reviving PDP from the 80s,  but this time with some better tricks. \n\nThe fact of the matter is that statistical mapping will only get so far.  For instance,  I doubt the winograd schemas will ever be conquered by statistical mapping like DL.  Sure,  it's going to be integral,  so much so that they've shifted multiple fields.  But when you have to reason,  at least superficially,  about those maps,  you're using symbols.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30976,"question": "Good paper, it makes sense that we want to get down to the character level for language understanding since it is much lower-dimensional than word level. Figuring out how to do unsupervised learning with char level convnets seems like an important question since there is so much unlabeled text, and in some cases it is hard to pick a single label for a large piece of text, perhaps convolutional autoencoders would work well here.\n\nThe authors touch on the potential to produce output text in the same way many recent image caption systems have done (convnet to rnn), that feels more like sequence-to-sequence mapping which could be done all with rnns, hopefully we will see some more papers comparing the two approaches. ","aSentId": 30977,"answer": "&gt; ...it makes sense that we want to get down to the character level for language understanding since it is much lower-dimensional than word level.\n\nI'm not sure I see the point. The information is at the word level, not the character level, unless words have internal structure such that words which are similar on the character level are similar in other ways. This is true to a limited extent when you consider prefixes, suffixes, and compound words, but until we see an AI/ML approach that learns these concepts from the data, I'm inclined to think it is better to hard-code this kind of structural relationship into your data analysis strategy.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30978,"question": "&gt; ...it makes sense that we want to get down to the character level for language understanding since it is much lower-dimensional than word level.\n\nI'm not sure I see the point. The information is at the word level, not the character level, unless words have internal structure such that words which are similar on the character level are similar in other ways. This is true to a limited extent when you consider prefixes, suffixes, and compound words, but until we see an AI/ML approach that learns these concepts from the data, I'm inclined to think it is better to hard-code this kind of structural relationship into your data analysis strategy.","aSentId": 30979,"answer": "Convnets should be able to do prefix/suffix identification at the character level, plus it will be tolerant to spelling mistakes which is a nice feature for text. For word embedding or other word-level features there is going to have to be a pre processing step to do some sort of feature extraction, one of the big wins of deep learning is that it should do feature extraction for us so it would be nice to work directly with the lowest-level representation of the information. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30980,"question": "Convnets should be able to do prefix/suffix identification at the character level, plus it will be tolerant to spelling mistakes which is a nice feature for text. For word embedding or other word-level features there is going to have to be a pre processing step to do some sort of feature extraction, one of the big wins of deep learning is that it should do feature extraction for us so it would be nice to work directly with the lowest-level representation of the information. ","aSentId": 30981,"answer": "&gt; one of the big wins of deep learning is that it should do feature extraction for us\n\nI think there is a big misconception hidden in that statement. One of the big wins of DL is that we don't have to do manual FE in many cases. But we only knew so in hindsight. \n\nIf we want to get the best results possible, we will always have to add a manual FE step. Especially, since many well working features devised by domain experts researchers are just not efficiently discovered by a DNN on its own. (Not in the vision domain, but e.g. biological signals.) \n\n(E.g. zero crossing is a more general than XOR and thus will already require two layers, Laplace is an optimisation problem and thus hopeless to achieve with a few layers.)\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30980,"question": "Convnets should be able to do prefix/suffix identification at the character level, plus it will be tolerant to spelling mistakes which is a nice feature for text. For word embedding or other word-level features there is going to have to be a pre processing step to do some sort of feature extraction, one of the big wins of deep learning is that it should do feature extraction for us so it would be nice to work directly with the lowest-level representation of the information. ","aSentId": 30983,"answer": "&gt; plus it will be tolerant to spelling mistakes which is a nice feature for text.\n\nVery interesting point. This type of approach could be especially useful for robustness to \"homophonic\" misspellings like there/their/they're.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30985,"question": "Does it strike anyone else that this work completely ignores the RNN based work in NLP of the last year?","aSentId": 30986,"answer": "I don't think the point is to ignore it RNNs, as much as it is to be a tour de force demonstration of what a pure, non-specialized \"brute force\" deep network can do. We all know theoretically that deep networks are universal function approximators, but there's a long way from theory to knowing exactly what that means in practice. So this result in my mind is really about demonstrating the generality of the deep neural network algorithm.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30987,"question": "I don't think the point is to ignore it RNNs, as much as it is to be a tour de force demonstration of what a pure, non-specialized \"brute force\" deep network can do. We all know theoretically that deep networks are universal function approximators, but there's a long way from theory to knowing exactly what that means in practice. So this result in my mind is really about demonstrating the generality of the deep neural network algorithm.","aSentId": 30988,"answer": "I am not saying that they are ignoring RNNs on purpose or because they are evil. \n\nBut when claiming that deep nets can do \"text understanding\" [1], it is just a shame that Cho's and Ilya's neural language models are just not mentioned with a single cite while neural word embeddings are. Because we already knew that deep nets can do pretty impressive stuff in the NLP domain. It's not them breaking the news. \n\n\n [1] Whatever that is.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30991,"question": "Their bag of words baseline is incredibly simple. Nerfed would be a more accurate description. It ignores all the components that make large linear models often competitive if not superior (almost always the case with smaller datasets) to fancier CNN/RNN models such as potentially millions of features, tf-idf, NB features (for classification problems) and using bi and tri grams. \n\n","aSentId": 30992,"answer": "Are the performance results shown actually competitive with more reasonable methods? I noticed they don't show performance results from previous papers.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30993,"question": "Are the performance results shown actually competitive with more reasonable methods? I noticed they don't show performance results from previous papers.","aSentId": 30994,"answer": "Part of the problem is that deep learning works much better on larger datasets, but on small ones traditional ML methods greatly outperform DL. I'm not very familiar with NLP datasets outside of machine translation(MT datasets got hundreds of millions words BTW). But I suspect this was one of the reasons as to why they introduced new ones. \n\nEDIT, from the paper QUOTE: *\"The unfortunate fact in literature is that there is no openly accessible dataset that is large enough or with labels of sufficient quality for us, although the research on text understanding has been conducted for tens of years.\"*","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30995,"question": "Part of the problem is that deep learning works much better on larger datasets, but on small ones traditional ML methods greatly outperform DL. I'm not very familiar with NLP datasets outside of machine translation(MT datasets got hundreds of millions words BTW). But I suspect this was one of the reasons as to why they introduced new ones. \n\nEDIT, from the paper QUOTE: *\"The unfortunate fact in literature is that there is no openly accessible dataset that is large enough or with labels of sufficient quality for us, although the research on text understanding has been conducted for tens of years.\"*","aSentId": 30996,"answer": "Can someone more familiar with NLP methods and datasets chime in on this? I highly doubt there is a lack of large NLP datasets, especially given how simple it was to collect the datasets for this particular paper. I would really like to see Richard Socher comment about this.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 30999,"question": "Can't wait to get my hands on the pylearn2 YAML model for this!!","aSentId": 31000,"answer": "They did it in torch, so unless someone at NYU wants to port it...we probably won't see a YAML model anytime soon.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31001,"question": "They did it in torch, so unless someone at NYU wants to port it...we probably won't see a YAML model anytime soon.","aSentId": 31002,"answer": "This really would be pretty easy to do in theano, its just 1d convolutions and regular maxpooling. The only thing you have to put together is the 1dconv, Theano doesn't have a built in one but there are several threads around with people that have posted code. I'm definitely gonna give this a try when I get some good text data to use it with. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31003,"question": "This really would be pretty easy to do in theano, its just 1d convolutions and regular maxpooling. The only thing you have to put together is the 1dconv, Theano doesn't have a built in one but there are several threads around with people that have posted code. I'm definitely gonna give this a try when I get some good text data to use it with. ","aSentId": 31004,"answer": "Is there any software limitation stopping people from treating signals of length N as Nx1 images?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31003,"question": "This really would be pretty easy to do in theano, its just 1d convolutions and regular maxpooling. The only thing you have to put together is the 1dconv, Theano doesn't have a built in one but there are several threads around with people that have posted code. I'm definitely gonna give this a try when I get some good text data to use it with. ","aSentId": 31006,"answer": "Post it here or let me know if you do, I want to play around with this model some.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31008,"question": "They could have applied their temporal convnet to word2vec vectors in the same way that their convnet handled character inputs. I bet that works better than the bag of centroids model.\n\nAnyway, are any of their datasets going to be packaged up nicely to allow comparison of results? It's disappointing when a neat algorithm gets introduced but they use proprietary datasets to evaluate it.","aSentId": 31009,"answer": "I am Xiang Zhang. We are in the process of submitting the paper and cleaning up the source code. After that (2 months or so), we will release the code used for training these ConvNets and all the datasets. For these datasets, if direct distribution is not possible, we will make our best effort in releasing code to extract from their original source the same collection of samples.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31010,"question": "I am Xiang Zhang. We are in the process of submitting the paper and cleaning up the source code. After that (2 months or so), we will release the code used for training these ConvNets and all the datasets. For these datasets, if direct distribution is not possible, we will make our best effort in releasing code to extract from their original source the same collection of samples.","aSentId": 31011,"answer": "Thanks! We need more large benchmarks for NLP. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31016,"question": "How can I use machine learning for this simple task?","aSentId": 31017,"answer": "This is an optimization problem","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31018,"question": "This is an optimization problem","aSentId": 31019,"answer": "maybe but wouldn't it be more robust to do it with machine learning, since with ML you could get extra data, to actually build a model, and allow you to more easily explore alternative hypothesis about the model?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31016,"question": "How can I use machine learning for this simple task?","aSentId": 31021,"answer": "10 supplements with 4 ways of taking them... So we have 4 to the 10 possibilities and combos. That means in 2,872 years we will adequately explore the state space. \n\nIf you're looking for if interaction effects (a and b) help more than just a xor b... Or basically anything with actual statistical rigor you'll have a bad time. You have a lot of possible output states too, making the problem even harder with just one person with limited data. \n\nI'd simplify the problem. Make taking the 10 drugs a yes or no, bringing the entire state space down to 2 to the 10. I'd then make the output: felt good or didn't feel good. Then I'd make a decision tree based on the results. You're likely to never cover the sample space adequately but if you only have modest interaction and most effects are additive, this would at least get you towards some real answer. \n\n\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31016,"question": "How can I use machine learning for this simple task?","aSentId": 31023,"answer": "Anova, not ML, this is a statistics problem, you do not have enough data to use ML. You need to learn to design statistically powerful experiments.\n\nOr you can just use examine.com and do some research on the supplements. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31029,"question": "How to not waste-time/procrastinate while ml scripts are running?","aSentId": 31030,"answer": "You could do pushups.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31031,"question": "You could do pushups.","aSentId": 31032,"answer": "If he gets into deep network learning, he can do strongman competitions on the weekends.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31029,"question": "How to not waste-time/procrastinate while ml scripts are running?","aSentId": 31034,"answer": "browse /r/machinelearning  \n\n\n....  \n\nshit.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31029,"question": "How to not waste-time/procrastinate while ml scripts are running?","aSentId": 31036,"answer": "I think most people do what you do. So don't worry too much about it.  I find reading papers the best thing to do while scripts are running. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31039,"question": "Batch up a bunch of analysis that will take a longer time and then go do something else.\n\nHalf the reason i'm in this field was the realization in collage that I could go rock climbing in the time it took for my MCMC's to converge.\n\nWhen you're working on a tricky component write a unit test, benchmark or small test case that runs just that component allowing you to get quicker feedback. This is good practice anyways.","aSentId": 31040,"answer": "I can vouch for this, especially the batching part. Try by any means possible to collect all the \"down time\" into larger blocks. If you are going to run the same simulation multiple times with different parameters for example, automate and script the entire thing.\n\nThe thing is that 2-5 minutes is too short to do anything useful, but 1-3 *hours* is not. You're never gonna find anything useful to fill those 2-3 minute breaks with, no matter how hard you try. Also, research shows that multitasking and constant task switching completely ruins your performance and focus, even among people who \"think\" they are very good at multitasking.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31042,"question": "Answering questions on Stackoverflow is good. It keeps surprising me how little I know even through I'm performing well at my job.\n\nAlso, I just today started to write my own little package (in R). It's lots of small tinkering that doesn't take much deep work and that I can switch out of real fast. It's something I haven't done before and that will look good on my resume once I get anywhere with it.","aSentId": 31043,"answer": "What's your package about? Are you looking for collaborators? ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31049,"question": "Get up and do some stretching.  Stay fit! ","aSentId": 31050,"answer": "This is perhaps the only way I will ever get fit.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31060,"question": "The Man Who Tried to Redeem the World with Logic","aSentId": 31061,"answer": "What a beautiful article. The end made me tear up slightly. Thanks for the link! ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31060,"question": "The Man Who Tried to Redeem the World with Logic","aSentId": 31063,"answer": "This was the such a great read.  Kinda made my morning actually.  Sad ending and a shane Pitts burnt his dissertation.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31060,"question": "The Man Who Tried to Redeem the World with Logic","aSentId": 31065,"answer": "Fantastic article!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31067,"question": "Hm, the frog experiment looks quite interesting. Do we know how human vision work?\n\nI remember some talk about Deep Learning (presumably, Ng's), where it's been stated that if you \"rewire\" eyes and ears, that is connect eyes to the region of the brain responsible for audio processing, then that region would learn to see. Yet, if our eyes do somewhat meaningful feature extraction (put you favorite convnets into the eyes, and a logistic regression into the brain), then brain could be very simple and generic: with good features you don't even need non-linearities.\n\nSo if our sensorial organs include these powerful domain-specific feature extractors, why do we hope that meaningful features could be learned (besides the recent success in CV, NLP, SR)?","aSentId": 31068,"answer": "People can learn to see from electrodes placed on their skin or tongue, or even pixels converted to a soundwave. So there probably is a general algorithm, even if evolution has fine-tuned the visual system over time.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31067,"question": "Hm, the frog experiment looks quite interesting. Do we know how human vision work?\n\nI remember some talk about Deep Learning (presumably, Ng's), where it's been stated that if you \"rewire\" eyes and ears, that is connect eyes to the region of the brain responsible for audio processing, then that region would learn to see. Yet, if our eyes do somewhat meaningful feature extraction (put you favorite convnets into the eyes, and a logistic regression into the brain), then brain could be very simple and generic: with good features you don't even need non-linearities.\n\nSo if our sensorial organs include these powerful domain-specific feature extractors, why do we hope that meaningful features could be learned (besides the recent success in CV, NLP, SR)?","aSentId": 31070,"answer": "&gt; why do we hope that meaningful features could be learned (besides the recent success in CV, NLP, SR)?\n\nUnless you believe in creationism, those \"powerful domain-specific feature extractors\" were learnt at some point. \n\nIt doesn't matter if it happens at the individual or the population (genetic) level, our neural network training algorithms could be seen as analogous to either one.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31072,"question": "Where does the author get this idea?\n\n&gt; After all, it had been Wiener who discovered a precise mathematical definition of information\n\nClaude Shannon is known for doing this. Weiner was of course a famed probabilist, but he is known for proving the existence of the Weiner measure.\n\nEDIT: In case you are as confused as I was, here's a PDF of Shannon's paper. Indeed, Shannon mentions Wiener's previous work a handful of times. http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf","aSentId": 31073,"answer": "If you read Shannon's seminal paper, \"A Mathematical Theory of Communication\", he cites and acknowledges Wiener multiple times.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31074,"question": "If you read Shannon's seminal paper, \"A Mathematical Theory of Communication\", he cites and acknowledges Wiener multiple times.","aSentId": 31075,"answer": "Thanks, I didn't realize that.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31072,"question": "Where does the author get this idea?\n\n&gt; After all, it had been Wiener who discovered a precise mathematical definition of information\n\nClaude Shannon is known for doing this. Weiner was of course a famed probabilist, but he is known for proving the existence of the Weiner measure.\n\nEDIT: In case you are as confused as I was, here's a PDF of Shannon's paper. Indeed, Shannon mentions Wiener's previous work a handful of times. http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf","aSentId": 31077,"answer": "&gt; Claude Shannon is known for doing this. \n\nBah!  Kolmogorov!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31078,"question": "&gt; Claude Shannon is known for doing this. \n\nBah!  Kolmogorov!","aSentId": 31079,"answer": "There's a great piece on Kolmogorv in the nautilus as well: http://nautil.us/issue/4/the-unlikely/the-man-who-invented-modern-probability","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31072,"question": "Where does the author get this idea?\n\n&gt; After all, it had been Wiener who discovered a precise mathematical definition of information\n\nClaude Shannon is known for doing this. Weiner was of course a famed probabilist, but he is known for proving the existence of the Weiner measure.\n\nEDIT: In case you are as confused as I was, here's a PDF of Shannon's paper. Indeed, Shannon mentions Wiener's previous work a handful of times. http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf","aSentId": 31081,"answer": "I don't know the history of information theory extraordinarily well, but I know that Shannon's original formulation/derivation was not considered sufficient rigorous by the mathematical community, and several years had to be spent giving it a firm mathematical grounding. I had assumed that Weiner was one of these mathematicians who helped to make Shannon's work rigorous. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31083,"question": "RE.WORK Deep Learning Summit coming to Boston and London this year, videos soon available from San Francisco edition (featured Andrew Ng, Greg Corrado, Clarifai etc)","aSentId": 31084,"answer": "Holy shit that price. Looks like a great set of speakers though. Let's see if I can trick my boss into getting me to go.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31083,"question": "RE.WORK Deep Learning Summit coming to Boston and London this year, videos soon available from San Francisco edition (featured Andrew Ng, Greg Corrado, Clarifai etc)","aSentId": 31086,"answer": "I really enjoyed the summit.  Planning on going to the London one.  \n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31083,"question": "RE.WORK Deep Learning Summit coming to Boston and London this year, videos soon available from San Francisco edition (featured Andrew Ng, Greg Corrado, Clarifai etc)","aSentId": 31088,"answer": "Will you upload all the videos from the summit or only some?\nHow do you select the videos?\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31090,"question": "Please explain support vector *regression* like i'm five.","aSentId": 31091,"answer": "It's just trying to draw a line through your training points.  So it's just like regular old linear regression except for the following three details: (1) there is an epsilon parameter that means \"If the line fits a point to within epsilon then that's good enough; stop trying to fit it and worry about fitting other points.\" (2) there is a C parameter and the smaller you make it the more you are telling it to find \"non-wiggly lines\".  So if you run SVR and get some crazy wiggly output that's obviously not right you can often make C smaller and it will stop being crazy.  And finally (3) when there are outliers (e.g. bad points that will never fit your line) in your data they will only mess up your result a little bit.  This is because SVR only gets upset about outliers in proportion to how far away they are from the line it wants to fit.  This is contrasted with normal linear regression which gets upset in proportion to the square of the distance from the line.  Regular linear regression worries too much about these bad points.\n\nTL;DR: SVR is trying to draw a line that gets within epsilon of all the points.  Some points are bad and can't be made to get within epsilon and SVR doesn't get too upset about them whereas other regression methods flip out.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31092,"question": "It's just trying to draw a line through your training points.  So it's just like regular old linear regression except for the following three details: (1) there is an epsilon parameter that means \"If the line fits a point to within epsilon then that's good enough; stop trying to fit it and worry about fitting other points.\" (2) there is a C parameter and the smaller you make it the more you are telling it to find \"non-wiggly lines\".  So if you run SVR and get some crazy wiggly output that's obviously not right you can often make C smaller and it will stop being crazy.  And finally (3) when there are outliers (e.g. bad points that will never fit your line) in your data they will only mess up your result a little bit.  This is because SVR only gets upset about outliers in proportion to how far away they are from the line it wants to fit.  This is contrasted with normal linear regression which gets upset in proportion to the square of the distance from the line.  Regular linear regression worries too much about these bad points.\n\nTL;DR: SVR is trying to draw a line that gets within epsilon of all the points.  Some points are bad and can't be made to get within epsilon and SVR doesn't get too upset about them whereas other regression methods flip out.","aSentId": 31093,"answer": "I like your simple explanation. Just curious. How can a linear regression-like thing have \"wiggly\" lines? Is this taking the kernel trick into account?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31094,"question": "I like your simple explanation. Just curious. How can a linear regression-like thing have \"wiggly\" lines? Is this taking the kernel trick into account?","aSentId": 31095,"answer": "Good question.  I have always struggled to follow how a rbf kernel can lead to the shapes that it does. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31096,"question": "Good question.  I have always struggled to follow how a rbf kernel can lead to the shapes that it does. ","aSentId": 31097,"answer": "One resource that helped me understand, somewhat better, RBF kernels was this: [link](https://charlesmartin14.wordpress.com/2012/02/06/kernels_part_1/)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31096,"question": "Good question.  I have always struggled to follow how a rbf kernel can lead to the shapes that it does. ","aSentId": 31099,"answer": "I may be wrong,  but I've often thought about it as svm working in an implicit feature space.  The implicit space can be nonlinear to your actual space.  It does it through the kernels.  An rbf is just a gaussian.  It returns a number that specifies how within a gaussian a pair of points are.  It's best to imagine this in 2d.  Imagine the heat map of a gauusians pdf.  The closer points are to the center,  the better they are. \n\nTldr. Nonlinear through the implicit dimensions of the rbf which is just basically a gaussian function that calculates similarity between points.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31100,"question": "I may be wrong,  but I've often thought about it as svm working in an implicit feature space.  The implicit space can be nonlinear to your actual space.  It does it through the kernels.  An rbf is just a gaussian.  It returns a number that specifies how within a gaussian a pair of points are.  It's best to imagine this in 2d.  Imagine the heat map of a gauusians pdf.  The closer points are to the center,  the better they are. \n\nTldr. Nonlinear through the implicit dimensions of the rbf which is just basically a gaussian function that calculates similarity between points.","aSentId": 31101,"answer": "If the kernel is positive definite symmetric, then there is a Reproducing Kernel Hilbert Space (RKHS) associated with the kernel. The kernel induces an inner product on this space. For SVM, the classification is then linear in the associated RKHS, but nonlinear in the original, this is why you get a \"squiggly\" decision boundary.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31094,"question": "I like your simple explanation. Just curious. How can a linear regression-like thing have \"wiggly\" lines? Is this taking the kernel trick into account?","aSentId": 31103,"answer": "/u/airthimble gave the right answer.  But here is a simple example.  Let's imagine you are trying to map a single number, x, to y.  Maybe you think the best thing to do is to fit a polynomial curve.  So you want to find the parameters of the polynomial y = Ax^4 + Bx^3 + Cx^2 + Dx + E.  You can do this using SVR since the parameters are linear in y.  That is, since you can write it like y = dot_product([A,B,C,D,E],[x^4 ,x^3 ,x^2 ,x ,1]) you can use SVR because it's linear.  \n\nWhen SVR tries to fit the line in 5 dimensions it's really fitting a polynomial that will be wiggly.  Moreover, making C small makes SVR try to find small values of the A,B,C,D,E parameters and when those are smaller the polynomial will be less wiggly because that's how polynomials are.  This same kind of reasoning applies to many other kernels used in kernel methods.  Usually the \"make the parameters small\" corresponds to something like making the derivatives of the function small which makes things less wiggly.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31090,"question": "Please explain support vector *regression* like i'm five.","aSentId": 31105,"answer": "I think the linked comment does a great job explaining like your five. ELI5 explanations provide a way to first grapple with things(like SVM), but you aren't going to really understand the how and why of it without sitting through and actually working through the derivation.\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31107,"question": "Question about text classification algorithm","aSentId": 31108,"answer": "Are you using an ML toolkit?  If so, check if it has multiclass or multi-label support.\n\nIf you're coding your own, you can transform a multi-label problem into a multiclass problem by enumerating all combinations of labels, eg. A but not B vs. both A and B vs. B but not A.  If you already have multiclass implemented, it should be easy to do.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31110,"question": "Can someone explain segmentation using HMMs?","aSentId": 31111,"answer": "Disclaimer: I'm an NLP person who doesn't do audio or image processing, so my descriptions of speech are grossly oversimplified.  If you're interested in NLP applications, David Blei's paper on HMM topic segmentation is a cool read that goes into topology.\n\nHere's a (very simple) speech example.  You have some audio files, split into half-second clips, and you want to identify individual words.  There are a lot of ways you could set this up.\n\n1) Your training data would be sequences of labeled clips.  One labeling scheme could be word or not-word.  Another could be Inside Outside Beginning (IOB) tags, ie. middle of word, not a word, beginning of word.  If you have a small, closed set of expected words (eg. your audio files are people reciting phone numbers), you could use the words as labels.\n\n2) Topology is important and is related to your labeling scheme.  One possibility could be three hidden states, one each for IOB; each hidden state produces a set of observable features and transitions to another hidden state.  Another possibility could be one hidden state for each word you expect to see; again, each hidden state produces some observable sounds (eg. /t/ shouldn't be produced by 'banana'), but here the transitions between hidden states can be constrained by grammar or a language model.\n\n3) Actual segmentation will depend on your labeling.  If you use word versus not-word or IOB, then not-word/O is the boundary, and everything between two not-words/Os is a segment.  If you label with words, the output would be a word for every clip, so boundaries would be between two clips labeled with different words, and sequences of clips labeled with the same word would be segments.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31114,"question": "WikiGalaxy: Explore Wikipedia in 3D","aSentId": 31115,"answer": "That's pretty cool, but what is it doing in /r/MachineLearning? (is it using ML behind the scenes? Can't find anything in the about)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31116,"question": "That's pretty cool, but what is it doing in /r/MachineLearning? (is it using ML behind the scenes? Can't find anything in the about)","aSentId": 31117,"answer": "It's probably using T-SNE or something of sorts to map everything in a 3D environment","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31116,"question": "That's pretty cool, but what is it doing in /r/MachineLearning? (is it using ML behind the scenes? Can't find anything in the about)","aSentId": 31119,"answer": "I'm quite confident those clusters have been identified with some ML techniques.. haven't found any information about the details, though!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31121,"question": "Is it possible to model the demand for parking space based upon parking ticket data?","aSentId": 31122,"answer": "Do you have *some* data about demand itself?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31123,"question": "Do you have *some* data about demand itself?","aSentId": 31124,"answer": "The count of parked cars is equivalent to demand when the number of parked cars is less than the total capacity of the lot.  However, when the lot is fully, you never know how many people wanted to park there who couldn't find a spot.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31125,"question": "The count of parked cars is equivalent to demand when the number of parked cars is less than the total capacity of the lot.  However, when the lot is fully, you never know how many people wanted to park there who couldn't find a spot.  ","aSentId": 31126,"answer": "The problem is i never know when it is full, so i need to estimate the actual demand even if it is full. If this is possible. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31127,"question": "The problem is i never know when it is full, so i need to estimate the actual demand even if it is full. If this is possible. ","aSentId": 31128,"answer": "Do you know when people leave the lot, or only when they enter?  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31129,"question": "Do you know when people leave the lot, or only when they enter?  ","aSentId": 31130,"answer": "Yep i know when they leave and enter ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31131,"question": "Yep i know when they leave and enter ","aSentId": 31132,"answer": "This should tell you how many cars are in the lot.  Combined with the size of the lot, this tells you when it's full.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31121,"question": "Is it possible to model the demand for parking space based upon parking ticket data?","aSentId": 31134,"answer": "Is your goal to forecast demand or to estimate demand for periods where the lot was full?  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31135,"question": "Is your goal to forecast demand or to estimate demand for periods where the lot was full?  ","aSentId": 31136,"answer": "Do you know if there is a method to estimate demand for periods where the lot was full?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31139,"question": "A Neural Turing Machine implementation using Torch","aSentId": 31140,"answer": "Anyone have any examples of tasks that NTMs have been shown to perform well on? Or are they too new?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31141,"question": "Anyone have any examples of tasks that NTMs have been shown to perform well on? Or are they too new?","aSentId": 31142,"answer": "Other than the ones in the paper I haven't heard anything. The assumption is that a NTM should be as good as or better as LSTM, so anywhere a LSTM rnn is used should be a good place to look.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31143,"question": "Other than the ones in the paper I haven't heard anything. The assumption is that a NTM should be as good as or better as LSTM, so anywhere a LSTM rnn is used should be a good place to look.","aSentId": 31144,"answer": "I asked Ilya Sutskever if he considered using NTM for his NIPS Sequence to Sequence Learning paper.  He said that he'd only expect NTM to provide value for learning from really long sequences and that single sentences would be fine with LSTM.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31145,"question": "I asked Ilya Sutskever if he considered using NTM for his NIPS Sequence to Sequence Learning paper.  He said that he'd only expect NTM to provide value for learning from really long sequences and that single sentences would be fine with LSTM.  ","aSentId": 31146,"answer": "The one I really wanna try an NTM on is his generating text with recurrent neural network paper, that's gonna be one of the first things I do assuming I can get an NTM that replicates(or gets close to) the five published results. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31141,"question": "Anyone have any examples of tasks that NTMs have been shown to perform well on? Or are they too new?","aSentId": 31148,"answer": "No but I know a lot of places will hire you if you have machine learning on your resume..","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31139,"question": "A Neural Turing Machine implementation using Torch","aSentId": 31150,"answer": "Google says they're publishing they're work, but it's more like \"Hey, look how awesome stuff we can build. Now good luck trying to replicate this\". \n\nHave any1 compared the above implementation performance with the original paper?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31151,"question": "Google says they're publishing they're work, but it's more like \"Hey, look how awesome stuff we can build. Now good luck trying to replicate this\". \n\nHave any1 compared the above implementation performance with the original paper?","aSentId": 31152,"answer": "Don't think so, to be honest, most of DeepMind's papers are poorly written in terms of details and reproducibility.\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31139,"question": "A Neural Turing Machine implementation using Torch","aSentId": 31154,"answer": "So I have no experience with torch, but from looking at the code the controller is constructed with the following steps:\n\n1. LSTM controller produces output based on Input, read_prev, and the LSTM's own parameters.\n2. Read, then update the memory based on LSTM output. A read and a write weight are produced during this step, each from their own head. \n3. Produce an output based on the LSTM output. \n\nThis is basically the same approach [shawntan](https://github.com/shawntan/neural-turing-machines) used in his NTM, with the important differences being his controller is feed-forward, and only uses a single head to produce a single weight which controlls both reading and writing. \n\nBased on the actual paper I'm inclined to believe that there is a separate read and write weighing at each timestep, but it is less clear if you need a unique head for both. Graves in the paper says that the LSTM controller uses a single head, while this implementation could be said to use two heads, but this might just be semantics. \n\nFor the copy task it is not clear if this implementation exhibits generality beyond what it has been trained on, I've trained several versions of an NTM that converges on copy up to length 20 but fails for longer sequences, unlike the results published in the NTM paper. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31156,"question": "Is this the first open-source NTM?\n\nHow hard was it to go from the paper to actually building it?","aSentId": 31157,"answer": "[This](https://github.com/shawntan/neural-turing-machines) one was posted a few months ago, it can't yet replicate the results that were published in the paper. The big challenge for building a NTM is designing the controller, there are basically no details in the paper, the one I linked uses a feed-forward controller while the torch one uses an LSTM controller. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31158,"question": "[This](https://github.com/shawntan/neural-turing-machines) one was posted a few months ago, it can't yet replicate the results that were published in the paper. The big challenge for building a NTM is designing the controller, there are basically no details in the paper, the one I linked uses a feed-forward controller while the torch one uses an LSTM controller. ","aSentId": 31159,"answer": "seems like it's time for conference/journal paper acceptance to require publishing working code.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31160,"question": "seems like it's time for conference/journal paper acceptance to require publishing working code.","aSentId": 31161,"answer": "That would be nice, but I'm just glad there is a decent culture of publishing in general. NTMs could potentially outperform LSTM (pending more research with them) in most/all scenarios, and since LSTMs have been involved in a bunch of really interesting stuff lately it could be a big advantage to keep NTMs internal to Google. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31162,"question": "That would be nice, but I'm just glad there is a decent culture of publishing in general. NTMs could potentially outperform LSTM (pending more research with them) in most/all scenarios, and since LSTMs have been involved in a bunch of really interesting stuff lately it could be a big advantage to keep NTMs internal to Google. ","aSentId": 31163,"answer": "&gt;  it could be a big advantage to keep NTMs internal to Google\n\nPerhaps. Or google could be trolling facebook/microsoft by holding back a bunch of vital details. \n\nUntil they release code or someone replicates their results it's hard to say.\n\nI think the contrast between opinions on NTM/DeepMind and HTM/Numenta is strange, given that empirical evidence for both is similar.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31164,"question": "&gt;  it could be a big advantage to keep NTMs internal to Google\n\nPerhaps. Or google could be trolling facebook/microsoft by holding back a bunch of vital details. \n\nUntil they release code or someone replicates their results it's hard to say.\n\nI think the contrast between opinions on NTM/DeepMind and HTM/Numenta is strange, given that empirical evidence for both is similar.","aSentId": 31165,"answer": "\"I think the contrast between opinions on NTM/DeepMind and HTM/Numenta is strange, given that empirical evidence for both is similar.\"\n\nIt's pretty different, because NTM came from researchers with a track record of producing good results.  See Alex Graves's results on forecasting, speech recognition, and handwriting synthesis.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31164,"question": "&gt;  it could be a big advantage to keep NTMs internal to Google\n\nPerhaps. Or google could be trolling facebook/microsoft by holding back a bunch of vital details. \n\nUntil they release code or someone replicates their results it's hard to say.\n\nI think the contrast between opinions on NTM/DeepMind and HTM/Numenta is strange, given that empirical evidence for both is similar.","aSentId": 31167,"answer": "I've replicated the copy and repeat copy task results personally, the remaining problem is to make sure it happens every time. The owner of this repository seems to have associative recall done, so I wouldn't be surprised to see a code package pretty soon that will replicate all 5 published results. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31164,"question": "&gt;  it could be a big advantage to keep NTMs internal to Google\n\nPerhaps. Or google could be trolling facebook/microsoft by holding back a bunch of vital details. \n\nUntil they release code or someone replicates their results it's hard to say.\n\nI think the contrast between opinions on NTM/DeepMind and HTM/Numenta is strange, given that empirical evidence for both is similar.","aSentId": 31169,"answer": "Numenta has been around for a while now, and I haven't heard of it revolutionizing anything.  Plus the goal of the NTM is to replicate human working memory.  Working memory is extremely important to higher level cognition.  Research estimates that more than 50% of the variance in human IQ is explained by working memory alone.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31170,"question": "Numenta has been around for a while now, and I haven't heard of it revolutionizing anything.  Plus the goal of the NTM is to replicate human working memory.  Working memory is extremely important to higher level cognition.  Research estimates that more than 50% of the variance in human IQ is explained by working memory alone.","aSentId": 31171,"answer": "NTM hasn't revolutionized anything either, and all the rest is blather just like HTM.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31160,"question": "seems like it's time for conference/journal paper acceptance to require publishing working code.","aSentId": 31173,"answer": "Was the NTM paper published in any journals or conferences? I just remember it being on arxiv...","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31160,"question": "seems like it's time for conference/journal paper acceptance to require publishing working code.","aSentId": 31175,"answer": "yeah that might be a bad idea.  In that case Google would have published nothing.  At least with their paper they have put enough info out there for people to figure it out eventually.\n\nmaybe they are just using it for recruiting, and they are going to hire whoever successfully clones it first.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31176,"question": "yeah that might be a bad idea.  In that case Google would have published nothing.  At least with their paper they have put enough info out there for people to figure it out eventually.\n\nmaybe they are just using it for recruiting, and they are going to hire whoever successfully clones it first.","aSentId": 31177,"answer": "I don't buy that. Who would want to work for google if it meant they could never publish on their work?\n\nBesides, Facebook doesn't seem to have a problem open sourcing at least some of their internal code, and aren't they supposed to be the evil ones?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31181,"question": "Question about SVM solver algorithms, in matlab specifically","aSentId": 31182,"answer": "&gt; Furthermore the minimum found with L1Qp isn't necessarily better than what's found with SMO, but I haven't been able to rigorously test this.\n\nThe SVM objective function is convex. There is one minimum. If the SMO solver is getting you there quicker... well, you know what to do.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31183,"question": "&gt; Furthermore the minimum found with L1Qp isn't necessarily better than what's found with SMO, but I haven't been able to rigorously test this.\n\nThe SVM objective function is convex. There is one minimum. If the SMO solver is getting you there quicker... well, you know what to do.","aSentId": 31184,"answer": "Do you have any idea of why SMO is getting there more quickly? It sounds like in principle L1Qp should.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31185,"question": "Do you have any idea of why SMO is getting there more quickly? It sounds like in principle L1Qp should.  ","aSentId": 31186,"answer": "Not off the top of my head, but I imagine the conditions under which one optimizer beats the other are somewhat problem dependent.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31185,"question": "Do you have any idea of why SMO is getting there more quickly? It sounds like in principle L1Qp should.  ","aSentId": 31188,"answer": "L1Qp is a general quatratic programming solver, i.e. it will work on any qp problem and is more flexible than SMO. SMO is only good at the exact qp problem that SVM does. SVM was fairly slow until it was found SMO could do it quickly. So it is not surprising SMO is faster.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31189,"question": "L1Qp is a general quatratic programming solver, i.e. it will work on any qp problem and is more flexible than SMO. SMO is only good at the exact qp problem that SVM does. SVM was fairly slow until it was found SMO could do it quickly. So it is not surprising SMO is faster.","aSentId": 31190,"answer": "Hey, thanks a lot for this -- it was exactly what I was looking for. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31181,"question": "Question about SVM solver algorithms, in matlab specifically","aSentId": 31192,"answer": "You might want to send this over to /r/MLQuestions","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31181,"question": "Question about SVM solver algorithms, in matlab specifically","aSentId": 31194,"answer": "&gt;So, my question is: what are the benefits of using L1Qp?\n\nNone. L1Qp is using a generalized algorithm for QP problems. SMO is a specialized algorithm for SVMs. \n\n&gt;A second question is: what is matlab doing to the Kernel via the KernelScale argument pair? In the documentation it says it divides the kernel by the kernel scale elementwise and then applies the 'appropriate kernel norm'. It's this last part that I'm a little confused by -- are they normalizing each column/row vector to unit length?\n\nThe appropriate Kernel is the one you selected for use. If you picked RBF, then it will use the RBF kernel *after* re-scaling the dimensions of the problem. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31195,"question": "&gt;So, my question is: what are the benefits of using L1Qp?\n\nNone. L1Qp is using a generalized algorithm for QP problems. SMO is a specialized algorithm for SVMs. \n\n&gt;A second question is: what is matlab doing to the Kernel via the KernelScale argument pair? In the documentation it says it divides the kernel by the kernel scale elementwise and then applies the 'appropriate kernel norm'. It's this last part that I'm a little confused by -- are they normalizing each column/row vector to unit length?\n\nThe appropriate Kernel is the one you selected for use. If you picked RBF, then it will use the RBF kernel *after* re-scaling the dimensions of the problem. ","aSentId": 31196,"answer": "Hey, thanks a lot for this.  I think either I'm misunderstanding your second question or I misstated the second question, though.  I indeed want to use the polynomial kernel, but I'm wondering exactly what is done to the kernel function by the KernelScale argument.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31197,"question": "Hey, thanks a lot for this.  I think either I'm misunderstanding your second question or I misstated the second question, though.  I indeed want to use the polynomial kernel, but I'm wondering exactly what is done to the kernel function by the KernelScale argument.  ","aSentId": 31198,"answer": "Nothing is done to the kernel by that argument, the *features* into the kernel are modified. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31200,"question": "HTML5 Genetic Algorithm Biped Walkers","aSentId": 31201,"answer": "[Champions fall early, traveled not as far as some non champions but still have higher score? Why?](http://s29.postimg.org/h3voybnhz/Untitled.jpg)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31202,"question": "[Champions fall early, traveled not as far as some non champions but still have higher score? Why?](http://s29.postimg.org/h3voybnhz/Untitled.jpg)","aSentId": 31203,"answer": "&gt; In order to promote upright walking, each creature receives points based on how high the head is in relation to its feet, and how much it advances while upright. Bonus points are given for each proper step forward. \n\n\nMy guess is the champion was walking very upright and took \"proper\" steps while the other guys kind of stumbled forward.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31202,"question": "[Champions fall early, traveled not as far as some non champions but still have higher score? Why?](http://s29.postimg.org/h3voybnhz/Untitled.jpg)","aSentId": 31205,"answer": "&gt; Elitist selection.\n\nChampions (genomes with highest fitness/score) from the previous generation are competing with mutated genomes in the current generation, which might perform better.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31200,"question": "HTML5 Genetic Algorithm Biped Walkers","aSentId": 31207,"answer": "I'd watch it all day long.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31200,"question": "HTML5 Genetic Algorithm Biped Walkers","aSentId": 31209,"answer": "Are the champions actually mating and having offspring or are they just being copied to the next gen?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31211,"question": "so basically a QWOP solver\n\nalso, my \"champion\" got 620.87 on the 62nd generation. 100 gens later and still the winner","aSentId": 31212,"answer": "Qovezo Kizoko?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31214,"question": "This is awesome...did you write this? \n\nDoes anyone have information on what the typical peak score is?","aSentId": 31215,"answer": "Nope.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31219,"question": "Very fun project! Kudos.\n\nI took a peek at your code and noticed that the Walker genes encode sinusoid coefficients to set joint motor speeds in Walker.simulationStep(). I was a bit disappointed because that means you don't use any feedback from \"sensors\" like joint position, head-foot-ground angle, travel speed, etc. \n\nDo you plan on adding these feedback mechanisms?\nIt would also be interesting to see average speed included in the fitness function so it would push them to start running.","aSentId": 31220,"answer": "You'll have to ask the creator: http://rednuht.org/ :-)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 31221,"question": "You'll have to ask the creator: http://rednuht.org/ :-)","aSentId": 31222,"answer": "haha, oops. I should have checked that first.","corpus": "reddit"}]