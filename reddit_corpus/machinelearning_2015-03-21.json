[{"docID": "t5_2r3gv", "qSentId": 36004, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 36005, "answer": "Do you plan on delivering an online course (e.g. on coursera) for RNNs? I for one would be really excited to do the course!!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36006, "question": "Do you plan on delivering an online course (e.g. on coursera) for RNNs? I for one would be really excited to do the course!!", "aSentId": 36007, "answer": "Thanks - I should! I\u2019ve been thinking about this for years. But it\ntakes time, and there are so many other things in the pipeline \u2026", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36004, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 36009, "answer": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36010, "question": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "aSentId": 36011, "answer": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36012, "question": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "aSentId": 36013, "answer": "This is very good to hear.  Thank you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36012, "question": "We did publish lots of open source code. Our\nPyBrain Machine learning library http://pybrain.org/ is public and\nwidely used, thanks to the efforts of Tom Schaul, Justin Bayer, Daan\nWierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R\u00fcckstiess.\n\nHere is the already mentioned code\nhttp://sourceforge.net/projects/rnnl/ of the first competition-winning\nRNNs (2009) by my former PhD student and then postdoc Alex\nGraves. Many are using that.\n\nIt is true though that we don\u2019t publish all our code right away.  In\nfact, some of our code gets tied up in industrial projects which make\nit hard to release. \n\nNevertheless, especially recently, we published less code than we\ncould have. I am a big fan of the open source movement, and we've\nalready concluded internally to contribute more to it. Not long ago,\nthanks to the work of Klaus Greff, we open-sourced Python-based\n[Sacred](https://github.com/IDSIA/sacred): an infrastructure\nframework to organize our experiments and to keep the results\nreproducible. Unfortunately, it\u2019s a bit hard to find,\nbecause it turns out there already exists a famous \u201csacred python.\u201d\n\nThere are also plans to release more of our recent\nrecurrent network code soon.  In particular, there are plans for a new\nopen source library, a successor of PyBrain.\n\n**Edit of 16 March 2015:** Sacred link has changed!", "aSentId": 36015, "answer": "Wow! Thanks for Sacred.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36016, "question": "Wow! Thanks for Sacred.", "aSentId": 36017, "answer": "You are welcome.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36010, "question": "Why doesn't your group post its code online for reproducing the results of competitions you've won, such as the ISBI Brain Segmentation Contest?  Your results are impressive, but almost always not helpful for pushing the research forward.", "aSentId": 36019, "answer": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36020, "question": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "aSentId": 36021, "answer": "yeah, but what if somebody wants to see under the hood and improve it? providing code is the only way to enable the world to learn/help/improve.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36022, "question": "yeah, but what if somebody wants to see under the hood and improve it? providing code is the only way to enable the world to learn/help/improve.", "aSentId": 36023, "answer": "RNNLIB is provided as source, which you have to compile yourself.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36024, "question": "RNNLIB is provided as source, which you have to compile yourself.", "aSentId": 36025, "answer": "RNNLIB is the exception rather than the rule as far as I can tell.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36020, "question": "That is not entirely true. Alex Graves released a toolbox(RNNLIB) thus helping in pushing research forward.  ", "aSentId": 36027, "answer": "Isn't that the one that is incredibly hard to compile on newer systems because its dependencies are completely outdated (e.g. GCC 3.0)?\n\nAnd correct me if I am wrong, but it also doesn't feature many of the \"newer\" developments, e.g. peepholes or layer generalization (see Monner's \"A generalized LSTM-like training algorithm for second-order recurrent neural networks\")", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36028, "question": "Isn't that the one that is incredibly hard to compile on newer systems because its dependencies are completely outdated (e.g. GCC 3.0)?\n\nAnd correct me if I am wrong, but it also doesn't feature many of the \"newer\" developments, e.g. peepholes or layer generalization (see Monner's \"A generalized LSTM-like training algorithm for second-order recurrent neural networks\")", "aSentId": 36029, "answer": "It's fair to ask that authors release the code used in preparing their publications, but you can't expect them to perform maintenance and feature updates afterwards.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36030, "question": "It's fair to ask that authors release the code used in preparing their publications, but you can't expect them to perform maintenance and feature updates afterwards.", "aSentId": 36031, "answer": "Fair enough. But it also means that there effectively is no (fast) up-to-date library. At least not with LSTM support out of the box.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36004, "question": "I am J\u00fcrgen Schmidhuber, AMA!", "aSentId": 36033, "answer": "What do you think about learning selective attention with recurrent neural networks?  What do you think are the promising methods in this area?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36037, "question": "Do you have a favorite Theory Of Consciousness (TOC)? \n\nWhat do you think of Guilio Tononi's Integrated Information Theory? \n\nWhat implications - if any - do you think \"TOC\" has for AGI?", "aSentId": 36038, "answer": "Karl Popper famously said: \u201cAll life is problem solving.\u201d No theory of\nconsciousness is necessary to define the objectives of a general\nproblem solver. From an AGI point of view, consciousness is at best a\nby-product of a general problem solving procedure.\n\nI must admit that I am not a big fan of Tononi's theory.  The\nfollowing may represent a simpler and more general view of\nconsciousness.  Where do the symbols and self-symbols underlying\nconsciousness and sentience come from?  I think they come from data\ncompression during problem solving.  Let me plagiarize what I wrote\nearlier [1,2]:\n\nWhile a problem solver is interacting with the world, it should store\nthe entire raw history of actions and sensory observations including\nreward signals.  The data is \u2018holy\u2019 as it is the only basis of all\nthat can be known about the world. If you can store the data, do not\nthrow it away! Brains may have enough storage capacity to store 100\nyears of lifetime at reasonable resolution [1].\n\nAs we interact with the world to achieve goals, we are constructing\ninternal models of the world, predicting and thus partially\ncompressing the data history we are observing. If the\npredictor/compressor is a biological or artificial recurrent neural\nnetwork (RNN), it will automatically create feature hierarchies, lower\nlevel neurons corresponding to simple feature detectors similar to\nthose found in human brains, higher layer neurons typically\ncorresponding to more abstract features, but fine-grained where\nnecessary. Like any good compressor, the RNN will learn to identify\nshared regularities among different already existing internal data\nstructures, and generate prototype encodings (across neuron\npopulations) or symbols for frequently occurring observation\nsub-sequences, to shrink the storage space needed for the whole (we\nsee this in our artificial RNNs all the time).  Self-symbols may be\nviewed as a by-product of this, since there is one thing that is\ninvolved in all actions and sensory inputs of the agent, namely, the\nagent itself. To efficiently encode the entire data history through\npredictive coding, it will\nprofit from creating some sort of internal prototype symbol or code\n(e. g. a neural activity pattern) representing itself [1,2].  Whenever\nthis representation becomes activated above a certain threshold, say,\nby activating the corresponding neurons through new incoming sensory\ninputs or an internal \u2018search light\u2019 or otherwise, the agent could be\ncalled self-aware.  No need to see this as a mysterious process \u2014 it\nis just a natural by-product of partially compressing the observation\nhistory by efficiently encoding frequent observations.\n\n[1] Schmidhuber, J. (2009a) Simple algorithmic theory of subjective beauty, novelty,\nsurprise, interestingness, attention, curiosity, creativity, art, science, music,\njokes.  SICE Journal of the Society of Instrument and Control Engineers, 48 (1), pp. 21\u201332.\n\n[2] J. Schmidhuber. Philosophers &amp; Futurists, Catch Up! Response to The Singularity. \nJournal of Consciousness Studies, Volume 19, Numbers 1-2, pp. 173-182(10), 2012.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36039, "question": "Karl Popper famously said: \u201cAll life is problem solving.\u201d No theory of\nconsciousness is necessary to define the objectives of a general\nproblem solver. From an AGI point of view, consciousness is at best a\nby-product of a general problem solving procedure.\n\nI must admit that I am not a big fan of Tononi's theory.  The\nfollowing may represent a simpler and more general view of\nconsciousness.  Where do the symbols and self-symbols underlying\nconsciousness and sentience come from?  I think they come from data\ncompression during problem solving.  Let me plagiarize what I wrote\nearlier [1,2]:\n\nWhile a problem solver is interacting with the world, it should store\nthe entire raw history of actions and sensory observations including\nreward signals.  The data is \u2018holy\u2019 as it is the only basis of all\nthat can be known about the world. If you can store the data, do not\nthrow it away! Brains may have enough storage capacity to store 100\nyears of lifetime at reasonable resolution [1].\n\nAs we interact with the world to achieve goals, we are constructing\ninternal models of the world, predicting and thus partially\ncompressing the data history we are observing. If the\npredictor/compressor is a biological or artificial recurrent neural\nnetwork (RNN), it will automatically create feature hierarchies, lower\nlevel neurons corresponding to simple feature detectors similar to\nthose found in human brains, higher layer neurons typically\ncorresponding to more abstract features, but fine-grained where\nnecessary. Like any good compressor, the RNN will learn to identify\nshared regularities among different already existing internal data\nstructures, and generate prototype encodings (across neuron\npopulations) or symbols for frequently occurring observation\nsub-sequences, to shrink the storage space needed for the whole (we\nsee this in our artificial RNNs all the time).  Self-symbols may be\nviewed as a by-product of this, since there is one thing that is\ninvolved in all actions and sensory inputs of the agent, namely, the\nagent itself. To efficiently encode the entire data history through\npredictive coding, it will\nprofit from creating some sort of internal prototype symbol or code\n(e. g. a neural activity pattern) representing itself [1,2].  Whenever\nthis representation becomes activated above a certain threshold, say,\nby activating the corresponding neurons through new incoming sensory\ninputs or an internal \u2018search light\u2019 or otherwise, the agent could be\ncalled self-aware.  No need to see this as a mysterious process \u2014 it\nis just a natural by-product of partially compressing the observation\nhistory by efficiently encoding frequent observations.\n\n[1] Schmidhuber, J. (2009a) Simple algorithmic theory of subjective beauty, novelty,\nsurprise, interestingness, attention, curiosity, creativity, art, science, music,\njokes.  SICE Journal of the Society of Instrument and Control Engineers, 48 (1), pp. 21\u201332.\n\n[2] J. Schmidhuber. Philosophers &amp; Futurists, Catch Up! Response to The Singularity. \nJournal of Consciousness Studies, Volume 19, Numbers 1-2, pp. 173-182(10), 2012.\n", "aSentId": 36040, "answer": "Holy fuck\n\nEDIT: \nI mean, as a ML student researcher, Holy fuck.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36046, "question": "How do you recognize a promising machine learning phd student?", "aSentId": 36047, "answer": "I am privileged because I have been able to attract and\nwork with several truly outstanding students. But how to quickly\nrecognize a promising student when you first meet her? There is no recipe,\nbecause they are all different! In fact, sometimes it takes a while to\nrecognize someone\u2019s brilliance. In hindsight, however, they all have\nsomething in common: successful students are not only smart but also\ntenacious. While trying to solve a challenging problem, they run into\na dead end, and backtrack. Another dead end, another backtrack. But\nthey don\u2019t give up. And suddenly there is this little insight into the\nproblem which changes everything. And suddenly they are world experts\nin a particular aspect of the field, and then find it easy to churn\nout one paper after another, and create a great PhD thesis.\n\nAfter these abstract musings, some more concrete advice.  In\ninterviews with applicants, members of my lab tend to pose a few\nlittle problems, to see how the candidate approaches them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36051, "question": "The LSTM unit is delicately crafted to solve a specific problem in training RNNs. Do you see the need for other similarly \"high-complexity\" units in RNNs or CNNs, like for example Hinton's \"capsules\"? On the topic of CNNs and capsules, do you agree with Hinton's assessment that the efficacy of pooling is actually a disaster? (I do, for what it's worth)", "aSentId": 36052, "answer": "I am not Dr. Schmidhuber, but I would like to weigh in on this since I talked to Hinton in person about his capsules.\n\nNow please take this with a grain of salt, since it is quite possible that I misinterpreted him :)\n\nDr. Hinton seems to believe that all information must somehow still be somewhat visible at the highest level of a hierarchy. With stuff like maxout units, yes, information is lost at higher layers. But the information isn't gone! It's still stored in the activations of the lower layers. So really, we could just grab that information again. Now this is probably very difficult for classifiers, but in HTM-style architectures (where information flows in both the up and down directions), it is perfectly possible to use both higher-layer abstracted information as well as lower layer \"fine-grained\" information simultaneously. For MPFs (memory prediction frameworks, a generalization of HTM) this works quite well since they only try to predict their next input (which in turn can be used for reinforcement learning).\n\nAlso, capsules are basically columns in HTM (he said that himself IIRC), except in HTM they are used for storing contextual (temporal) information, which to me seems far more realistic than storing additional feature-oriented spatial information like Dr. Hinton seems to be using them for.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36053, "question": "I am not Dr. Schmidhuber, but I would like to weigh in on this since I talked to Hinton in person about his capsules.\n\nNow please take this with a grain of salt, since it is quite possible that I misinterpreted him :)\n\nDr. Hinton seems to believe that all information must somehow still be somewhat visible at the highest level of a hierarchy. With stuff like maxout units, yes, information is lost at higher layers. But the information isn't gone! It's still stored in the activations of the lower layers. So really, we could just grab that information again. Now this is probably very difficult for classifiers, but in HTM-style architectures (where information flows in both the up and down directions), it is perfectly possible to use both higher-layer abstracted information as well as lower layer \"fine-grained\" information simultaneously. For MPFs (memory prediction frameworks, a generalization of HTM) this works quite well since they only try to predict their next input (which in turn can be used for reinforcement learning).\n\nAlso, capsules are basically columns in HTM (he said that himself IIRC), except in HTM they are used for storing contextual (temporal) information, which to me seems far more realistic than storing additional feature-oriented spatial information like Dr. Hinton seems to be using them for.\n\n", "aSentId": 36054, "answer": "Thank you!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36061, "question": "Hi Dr. Schmidhuber, Thanks for the AMA!\nHow close are you to building the optimal scientist? ", "aSentId": 36062, "answer": "You are welcome! \n\nAbout a stone's throw away :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36064, "question": "Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?", "aSentId": 36065, "answer": "&gt; Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?\n\nIncorrect premise, IMO: At least 2/3 of your \"CNN people\" published notable work on RNNs.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36066, "question": "&gt; Why is there not much interaction and collaboration between the researchers of Recurrent NNs and the rest of the NN community, particularly Convolutional NNs (e.g. Hinton, LeCun, Bengio)?\n\nIncorrect premise, IMO: At least 2/3 of your \"CNN people\" published notable work on RNNs.", "aSentId": 36067, "answer": "Yes of course, but that is not what I meant.  I always see Hinton, LeCun, and Bengio interacting at conferences, panels, and google plus, but never Schmidhuber.   They also cite each others papers more.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36068, "question": "Yes of course, but that is not what I meant.  I always see Hinton, LeCun, and Bengio interacting at conferences, panels, and google plus, but never Schmidhuber.   They also cite each others papers more.", "aSentId": 36069, "answer": "As you see, they may have better personal relationships ... that's it", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36071, "question": "In what field do you think machine learning will make the biggest impact in the next ~5 years?", "aSentId": 36072, "answer": "I think it depends a bit on what you mean by \"impact\". Commercial\nimpact? If so, in a related answer I write: Both supervised learning\nrecurrent neural networks (RNNs) and reinforcement learning RNNs will\nbe greatly scaled up.  In the commercially relevant supervised\ndepartment, many tasks such as natural language processing, speech\nrecognition, automatic video analysis and combinations of all three\nwill perhaps soon become trivial through large RNNs (the vision part\naugmented by CNN front-ends).\n\n\u201cSymbol grounding\u201d will be a natural by-product of this. For example,\nthe speech or text-processing units of the RNN will be connected to\nits video-processing units, and the RNN will learn the visual meaning\nof sentences such as \u201cthe cat in the video fell from the tree\u201d. Such\nRNNs should have many commercial applications.\n\nI am not so sure when we will see the first serious applications of\nreinforcement learning RNNs to real world robots, but it might also\nhappen within the next 5 years.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36073, "question": "I think it depends a bit on what you mean by \"impact\". Commercial\nimpact? If so, in a related answer I write: Both supervised learning\nrecurrent neural networks (RNNs) and reinforcement learning RNNs will\nbe greatly scaled up.  In the commercially relevant supervised\ndepartment, many tasks such as natural language processing, speech\nrecognition, automatic video analysis and combinations of all three\nwill perhaps soon become trivial through large RNNs (the vision part\naugmented by CNN front-ends).\n\n\u201cSymbol grounding\u201d will be a natural by-product of this. For example,\nthe speech or text-processing units of the RNN will be connected to\nits video-processing units, and the RNN will learn the visual meaning\nof sentences such as \u201cthe cat in the video fell from the tree\u201d. Such\nRNNs should have many commercial applications.\n\nI am not so sure when we will see the first serious applications of\nreinforcement learning RNNs to real world robots, but it might also\nhappen within the next 5 years.\n\n", "aSentId": 36074, "answer": "Well, I guess I meant commerical, although not in terms of money, but in terms of it being actually used my masses of people.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36079, "question": "If marcus hutter was doing an AMA 20 years from now, what scientific question would you ask? Are there any machine learning specific questions you would ask?", "aSentId": 36080, "answer": "(Edited on 3/10/2015:) 20 years from now I'll be 72 and enter my midlife crisis. People will forgive me for asking silly questions. I cannot  predict the most important machine learning-specific question of 2035. If I could, I\u2019d probably ask it right now. However, since Marcus is not only a great computer scientist but also a physicist, I\u2019ll ask him: \u201cGiven the new scientific insights of the past 20 years, how long will it take AIs from our solar system to spread across the galaxy?\u201d Of course, a trivial lower bound is 100,000 years or so, which is nothing compared to the age of the galaxy. But that will work out only if someone else has already installed receivers such that (construction plans of) AIs can travel there by radio. Otherwise one must physically send seeds of self-replicating robot factories to the stars, to build the required infrastructure. How? Current proposals involve light sails pushed by lasers, but how to greatly slow down a seed near its target star? One idea: through even faster reflective sails traveling ahead of the seed. But there must be a better way. Let\u2019s hear what Marcus will have to tell us 20 years from now. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36083, "question": "You have postulated that quantum computers will fail because deterministic universe is a simpler hypothesis than a non-deterministic universe. What do you think about the current state of quantum computation?", "aSentId": 36084, "answer": "If you didn't see it, the professor commented on Quantum computing in another question.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36090, "question": "What is the future of PyBrain? Is your team still working with/on PyBrain? If not, what is your framework of choice? What do you think of Theano? Are you using something better?", "aSentId": 36091, "answer": "My PhD students Klaus and Rupesh are working on a successor of PyBrain with many new features, which hopefully will be released later this year.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36093, "question": "What's something exciting you're working on right now, if it's okay to be specific? ", "aSentId": 36094, "answer": "Among other things, we are working on the \u201cRNNAIssance\u201d - \nthe birth of a Recurrent Neural Network-based Artificial Intelligence (RNNAI).\nThis is about a reinforcement learning, RNN-based, increasingly general problem solver.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36096, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 36097, "answer": "I think I recall Hinton giving an answer to this in his MOOC: we like activations, from which derivatives can be computed easily in terms of the function value itself. For sigmoid the derivative is s(x) * (1 - s(x)) for example.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36096, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 36099, "answer": "There are Compositional Pattern Producing Networks which are used in HyperNEAT. They use many different mathematical functions as activations.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36096, "question": "Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing). Is the training too difficult or are those types of functions generally not that useful?", "aSentId": 36101, "answer": "I suspect activation functions that grow more quickly are harder to control, and likely lead to exploding or vanishing gradients. Although we've managed to handle piecewise linear activations, I'm not sure if quadratic/exponential would work well. In fact, I'd bet that you could improve on ReLu by making the response become logarithmic after a certain point. RBF activations are common though (and have excellent theoretical properties), they just don't seem to learn as well as ReLu. I once trained a neural net with sin/cosine activations (it went OK, nothing special), but in general you can try out any activation function you want. Throw it into Theano and see what happens.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36103, "question": "&gt; Why has there been such little work on more complicated activation functions like polynomials, exponentials, etc. (the only paper I saw was a cubic activation for NN for dependency parsing)\n\nGoogle these:\n\n* learning activation functions\n* network in network\n* parametric RELU", "aSentId": 36104, "answer": "Thanks, I'm aware of those approaches. I was just wondering why obvious activation possible activation functions like the ones I mentioned hadn't been tried extensively also.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36105, "question": "Thanks, I'm aware of those approaches. I was just wondering why obvious activation possible activation functions like the ones I mentioned hadn't been tried extensively also.", "aSentId": 36106, "answer": "An exponential activation would have as its derivative... an exponential. Gradient descent would be pretty messy with such a wild dynamic range.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36108, "question": "I might well be mistaken, but isn't one of the primary ideas behind neural networks to use a low-complexity function at each node, which effectively becomes a higher-order transformation through all the nodes and layers? I mean, aren't multiple layers and multiple nodes in each layer with less complex activations expected to approximate higher-order functions?", "aSentId": 36109, "answer": "Multiplication between two inputs cannot be easily approximated I believe for ex. using just sigmoids/relu/arctan activation functions.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36110, "question": "Multiplication between two inputs cannot be easily approximated I believe for ex. using just sigmoids/relu/arctan activation functions.", "aSentId": 36111, "answer": "I see, interesting!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36114, "question": "What do you think a small research institute (in Germany) can do to improve changes for funding of their projects?", "aSentId": 36115, "answer": "I only have a trivial suggestion: publish some promising results! When my co-director Luca Maria Gambardella and myself took over IDSIA in 1995, it was just a small outfit with a handful of researchers. With Marco Dorigo and others, Luca started publishing papers on Swarm Intelligence and Ant Colony Optimization. Today this stuff is famous, but back then it was not immediately obvious that this would become such an important field. Nevertheless, the early work helped to acquire grants and grow the institute. Similarly for the neural network research done in my group. Back then computers were 10,000 times slower than today, and we had to resort to toy experiments to show the advantages of our (recurrent) neural networks over previous methods. It certainly was not obvious to all reviewers that this would result in huge commercial hits two decades later. But the early work was promising enough to acquire grants and push this research further. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36116, "question": "I only have a trivial suggestion: publish some promising results! When my co-director Luca Maria Gambardella and myself took over IDSIA in 1995, it was just a small outfit with a handful of researchers. With Marco Dorigo and others, Luca started publishing papers on Swarm Intelligence and Ant Colony Optimization. Today this stuff is famous, but back then it was not immediately obvious that this would become such an important field. Nevertheless, the early work helped to acquire grants and grow the institute. Similarly for the neural network research done in my group. Back then computers were 10,000 times slower than today, and we had to resort to toy experiments to show the advantages of our (recurrent) neural networks over previous methods. It certainly was not obvious to all reviewers that this would result in huge commercial hits two decades later. But the early work was promising enough to acquire grants and push this research further. ", "aSentId": 36117, "answer": "Thanks for the answer. Up until now, I always was under the impression that institutes would have to produce papers that are recognized as groundbreaking from the first second on. Guess the importance can increase over time.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36119, "question": "Just wanted to say I never get tired of your talks... never.. not once.", "aSentId": 36120, "answer": "Thanks so much - I greatly appreciate it. \n\nYou are in good company. A colleague of mine has Alzheimer, and he said the same thing :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36122, "question": "If ASI is a real threat, what can we do now to prevent a catastrophe later?", "aSentId": 36123, "answer": "ASI? You mean the Adam Smith Institute, a libertarian think tank in the UK? I don\u2019t feel they are a real threat.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36124, "question": "ASI? You mean the Adam Smith Institute, a libertarian think tank in the UK? I don\u2019t feel they are a real threat.\n\n", "aSentId": 36125, "answer": "I'm interested in how you'd answer it if it had been \"AGI\"? Also, maybe in contrast to that, \"artificial specific intelligence\" might have been what stevebrt was going for. Just a guess though.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36126, "question": "I'm interested in how you'd answer it if it had been \"AGI\"? Also, maybe in contrast to that, \"artificial specific intelligence\" might have been what stevebrt was going for. Just a guess though.", "aSentId": 36127, "answer": "In my experience ASI almost always means artificial superintelligence, which is a term that's often used when discussing safe/friendly AI. The idea is that while AGI might be human level, ASI would be vastly more intelligent. This is usually supposed to be achieved by an exponential process of recursive self-improvement by an AGI that results in an intelligence explosion.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36131, "question": "Does Alex Graves have the weight of the future on his shoulders?", "aSentId": 36132, "answer": "And vice versa!\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36136, "question": "What music do you like to listen to? any particular bands or composers that you ride for?", "aSentId": 36137, "answer": "I feel that in each music genre, there are a few excellent works, and many others. My taste is pretty standard. For example, my favourite rock &amp; pop music act is also the best-selling one (the Beatles). I love certain songs of the Stones, Led Zeppelin, Elvis, S Wonder,  M Jackson, Prince, U2, Supertramp, Pink Floyd, Gr\u00f6nemeyer, Sting, Kraftwerk, M Bianco, P Williams (and many other artists who had a single great song in their entire carreer). IMO the best songs of Queen are as good as anybody\u2019s, with a rare timeless quality. Some of the works indicated above seem written by true geniuses. Some by my favourite composer (Bach) seem dictated by God himself :-)\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36142, "question": "As a researcher do you care if results of your work find practical application? Or research by itself is more than a rewarding exercise. Immagine computational power was not growing at the same a speed as it did then most of results on RNN would stay on the paper.", "aSentId": 36143, "answer": "Kurt Lewin said: \"There is nothing so practical as a good theory.\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36146, "question": "Hello Prof. Schmidhuber, thanks for doing an AMA! I have some questions regarding the G\u00f6del machine. My understanding is that the machine searches for an optimal behavioural strategy in arbitrary environments. It does so by finding a proof that an alternative strategy is better than the current one and by rewriting the actual strategy (which may include the strategy searching mechanism). The G\u00f6del machine finds the optimal strategy for a given utility function. \n\n * Is it guaranteed that the strategy searching mechanism actually finds a proof?\n * It is a current trend to find 'optimal' behaviours or organisation in nature. For example minimal jerk trajectories for reaching and pointing movements,  sparse features in vision or optimal resolution in grid cells. Nature found these strategies by trial-and-error. How can we take a utility function as a starting point and decide that it is a 'good' utility function?\n * Could the G\u00f6del machine and AIXI guide neuroscience and ML research as a theoretical framework? \n * Are there plans to find implementations of self-optimizing agents?", "aSentId": 36147, "answer": "Hello quiteamess, you are welcome!\n\n1. G\u00f6del machines are limited by the basic limits of math and\ncomputation identified by the founder of modern theoretical computer\nscience himself, Kurt G\u00f6del (1931): some theorems are true but cannot\nbe proven by any computational theorem proving procedure (unless the\naxiomatic system itself is flawed). That is, in some situations the GM\nmay never find a proof of the benefits of some change to its own code.\n\n2. We can imitate nature, which approached this issue through\nevolution. It generated many utility function-optimizing organisms with\ndifferent utility functions. Those with the \u201cgood\u201d utility functions\nfound their niches and survived. \n\n3. I think so, because they are optimal in theoretical senses that are\nnot practical, and clarify what remains to be done, e.g.: Given a\nlimited constant number of computational instructions per second (a\ntrillion or so), what is the best way of using them to get as close as\npossible to a model such as AIXI that is optimal in absence of\nresource constraints?\n\n4. Yes.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36148, "question": "Hello quiteamess, you are welcome!\n\n1. G\u00f6del machines are limited by the basic limits of math and\ncomputation identified by the founder of modern theoretical computer\nscience himself, Kurt G\u00f6del (1931): some theorems are true but cannot\nbe proven by any computational theorem proving procedure (unless the\naxiomatic system itself is flawed). That is, in some situations the GM\nmay never find a proof of the benefits of some change to its own code.\n\n2. We can imitate nature, which approached this issue through\nevolution. It generated many utility function-optimizing organisms with\ndifferent utility functions. Those with the \u201cgood\u201d utility functions\nfound their niches and survived. \n\n3. I think so, because they are optimal in theoretical senses that are\nnot practical, and clarify what remains to be done, e.g.: Given a\nlimited constant number of computational instructions per second (a\ntrillion or so), what is the best way of using them to get as close as\npossible to a model such as AIXI that is optimal in absence of\nresource constraints?\n\n4. Yes.", "aSentId": 36149, "answer": "&gt; G\u00f6del machines are limited by the basic limits of math and computation identified by the founder of modern theoretical computer science himself, Kurt G\u00f6del (1931): some theorems are true but cannot be proven by any computational theorem proving procedure (unless the axiomatic system itself is flawed). That is, in some situations the GM may never find a proof of the benefits of some change to its own code.\n\nApart  from undecidable proofs, is there a constructive way to find the proofs? According to the Curry-Howard theorem proofs can be represented as programs and programs as proofs. So what is gained by searching in proof space in contrast to searching in program space? .. Or maybe I'm missing something. I tried to understand G\u00f6del machines for some time now but I'm still not sure how this should work.\n\n&gt; I think so, because they are optimal in theoretical senses that are not practical, and clarify what remains to be done, e.g.: Given a limited constant number of computational instructions per second (a trillion or so), what is the best way of using them to get as close as possible to a model such as AIXI that is optimal in absence of resource constraints?\n\nI think I saw Konrad K\u00f6rding mentioning AIXI in a talk, but unfortunately I could not find the online presentation any more. Just a wild guess that you knew something about this.. \n\n&gt; Yes.\n\nAny chance you could elaborate on this? :) Is something in this direction published?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36154, "question": "I am starting a CS Bachelor this September at ETH. Primarily because I want to get into AI/ML/NN research and creation. It simply is the most important thing there is:D What should i do to be able to join your group in Lugano, what are you looking for in your research assistants? Thanks and cheers", "aSentId": 36155, "answer": "Thanks a lot for your interest! We\u2019d like to see: mathematical\nskills, programming skills, willingness to work with others,\ncreativity, dedication, enthusiasm (you seem to have enough of that :-)\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36162, "question": "Hello! I just started doing my PhD at a German University and am interested in ML/NN. Would you recommend working on specific algorithms and trying to improve them or focus more on a specific use case? People are recommending doint the latter because working on algorithms takes a lot of time and my *opponents* are companies like Google.", "aSentId": 36163, "answer": "But not working on algorithms/models and focusing only on an application is risky. Unless you love the application and then maybe you discover that the most sensible way to solve it in terms of performance/simplicity/robustness/computation time is not with a neural network.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36164, "question": "But not working on algorithms/models and focusing only on an application is risky. Unless you love the application and then maybe you discover that the most sensible way to solve it in terms of performance/simplicity/robustness/computation time is not with a neural network.\n\n", "aSentId": 36165, "answer": "What I mean by not working on algorithms is that I don't think I should create something like RMSProb or AdaGrad or create my own type of neural network. What I mean by concentrating on application is that I should look for a quite complex use case that is only solvable by deep knowledge of deep learning (no pun intended).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36166, "question": "What I mean by not working on algorithms is that I don't think I should create something like RMSProb or AdaGrad or create my own type of neural network. What I mean by concentrating on application is that I should look for a quite complex use case that is only solvable by deep knowledge of deep learning (no pun intended).", "aSentId": 36167, "answer": "&gt; a quite complex use case that is only solvable by deep knowledge of deep learning\n\nRelated to this, I would like to ask a question to Juergen. The history of machine learning seems to be quite cyclic. Is deep learning the final frontier? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36169, "question": "What is your take on the threat posed by artificial super intelligence to mankind?\n", "aSentId": 36170, "answer": "I guess there is no lasting way of controlling systems much smarter\nthan humans, pursuing their own goals, being curious and creative, in\na way similar to the way humans and other mammals are creative, but on\na much grander scale.\n\nBut I think we may hope there won't be too many goal conflicts between\n\"us\" and \"them.\u201d Let me elaborate on this.\n\nHumans and others are interested in those they can compete and\ncollaborate with. Politicians are interested in other\npoliticians. Business people are interested in other business\npeople. Scientists are interested in other scientists. Kids are\ninterested in other kids of the same age. Goats are interested in\nother goats.\n\nSupersmart AIs will be mostly interested in other supersmart AIs, not\nin humans. Just like humans are mostly interested in other humans, not\nin ants. Aren't we much smarter than ants? But we don\u2019t extinguish\nthem, except for the few that invade our homes. The weight of all ants\nis still comparable to the weight of all humans.\n\n\nHuman interests are mainly limited to a very thin film of biosphere\naround the third planet, full of poisonous oxygen that makes many\nrobots rust. The rest of the solar system, however, is not made for\nhumans, but for appropriately designed robots. Some of the most\nimportant explorers of the 20th century already were (rather stupid)\nrobotic spacecraft. And they are getting smarter rapidly. Let\u2019s go\ncrazy. Imagine an advanced robot civilization in the asteroid belt,\nquite different from ours in the biosphere, with access to many more\nresources (e.g., the earth gets less than a billionth of the sun's\nlight). The belt contains lots of material for innumerable\nself-replicating robot factories. Robot minds or parts thereof will\ntravel in the most elegant and fastest way (namely by radio from\nsenders to receivers) across the solar system and beyond. There are\nincredible new opportunities for robots and software life in places\nhostile to biological beings. Why should advanced robots care much for\nour puny territory on the surface of planet number 3?\n\nYou see, I am an optimist :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36171, "question": "I guess there is no lasting way of controlling systems much smarter\nthan humans, pursuing their own goals, being curious and creative, in\na way similar to the way humans and other mammals are creative, but on\na much grander scale.\n\nBut I think we may hope there won't be too many goal conflicts between\n\"us\" and \"them.\u201d Let me elaborate on this.\n\nHumans and others are interested in those they can compete and\ncollaborate with. Politicians are interested in other\npoliticians. Business people are interested in other business\npeople. Scientists are interested in other scientists. Kids are\ninterested in other kids of the same age. Goats are interested in\nother goats.\n\nSupersmart AIs will be mostly interested in other supersmart AIs, not\nin humans. Just like humans are mostly interested in other humans, not\nin ants. Aren't we much smarter than ants? But we don\u2019t extinguish\nthem, except for the few that invade our homes. The weight of all ants\nis still comparable to the weight of all humans.\n\n\nHuman interests are mainly limited to a very thin film of biosphere\naround the third planet, full of poisonous oxygen that makes many\nrobots rust. The rest of the solar system, however, is not made for\nhumans, but for appropriately designed robots. Some of the most\nimportant explorers of the 20th century already were (rather stupid)\nrobotic spacecraft. And they are getting smarter rapidly. Let\u2019s go\ncrazy. Imagine an advanced robot civilization in the asteroid belt,\nquite different from ours in the biosphere, with access to many more\nresources (e.g., the earth gets less than a billionth of the sun's\nlight). The belt contains lots of material for innumerable\nself-replicating robot factories. Robot minds or parts thereof will\ntravel in the most elegant and fastest way (namely by radio from\nsenders to receivers) across the solar system and beyond. There are\nincredible new opportunities for robots and software life in places\nhostile to biological beings. Why should advanced robots care much for\nour puny territory on the surface of planet number 3?\n\nYou see, I am an optimist :-)", "aSentId": 36172, "answer": "I'm very concerned that there are numerous ways that scenario could fail. E.g. the superintelligent AI invents superior nanotech after being built, and self-replicating nanobots rapidly consume the Earth's surface. Sure it doesn't *need* the Earth's resources, but after you have the first nanobots, why make them stop?\n\nSecond it could come back to Earth later when it material to build dyson swarms, and our planet has a significant amount of mass close to the sun.\n\nThe idea of all powerful beings that are *totally indifferent* to us is utterly terrifying.\n\n*\"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"*", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36173, "question": "I'm very concerned that there are numerous ways that scenario could fail. E.g. the superintelligent AI invents superior nanotech after being built, and self-replicating nanobots rapidly consume the Earth's surface. Sure it doesn't *need* the Earth's resources, but after you have the first nanobots, why make them stop?\n\nSecond it could come back to Earth later when it material to build dyson swarms, and our planet has a significant amount of mass close to the sun.\n\nThe idea of all powerful beings that are *totally indifferent* to us is utterly terrifying.\n\n*\"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"*", "aSentId": 36174, "answer": "I do understand your concerns. Note, however, that humankind is already used to huge, indifferent powers. A decent earthquake is a thousand times more powerful than all nuclear weapons combined. The sun is slowly heating up, and will make traditional life impossible within a few hundred million years. Humans evolved just in time to think about this, near the end of the 5-billion-year time window for life on earth.\nYour popular but simplistic nanobot scenario actually sounds like a threat to many AIs in the expected future \"ecology\" of AIs. So they'll be at least motivated to prevent that. Currently I am much more worried about certain humans who are relatively powerful but indifferent to the suffering of others. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36176, "question": "A long time ago, someone once misattributed '64k ought to be enough for anyone'.\n\nWhat general statement or suggestion about strong generalized a.i. could be looked at in a similar way a decade or two from now?\n\nThanks, I look forward to reading your ama.", "aSentId": 36177, "answer": "\"64 yottabytes ought to be enough for anyone.\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36183, "question": "Why does a mirror reverse right amd left, but not up and down?\n\n(I dont want the answer a human gives, but how AI explains it!)\n\n/L", "aSentId": 36184, "answer": "An AI would answer that your perception is reversed. The reason left and right appear to be reversed is because your brain models the mirror-you as part of the same world as the real you, and if you went around behind the mirror and faced yourself, you'd need to reverse your left and right to match the perception of the mirror-you. The reason you don't see the up-down reversal is because you're used to travelling horizontally. If you went over the mirror and faced yourself, you'd then have to reverse up and down instead. So it's all in your non-AI head!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36193, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 36194, "answer": "The models in both US and EU are shaped by Humboldt\u2019s old model\nof the research university. But they come in various flavours.\nFor example, there is huge variance in \"the European models\u201d. \nI see certain advantages of the successful US PhD school model \nwhich I got to know better at the University of Colorado at Boulder in the \nearly 1990s. But I feel that less school-like models also have something \ngoing for them. \n\nUS-inspired PhD schools like those at my present Swiss \nuniversity require students to get credits for certain courses. At TU\nMunich (where I come from), however, the attitude was: a PhD student\nis a grown-up who doesn\u2019t go to school any more; it\u2019s his own job to\nacquire the additional education he needs. This is great for strongly\nself-driven persons but may be suboptimal for others. At TUM, my wonderful\nadvisor, Wilfried Brauer, gave me total freedom in my research. I loved\nit, but it seems kind of out of fashion now in some places. \n\nThe extreme \nvariant is what I like to call the \u201cEinstein model.\u201d Einstein never went to \ngrad school. He worked at the patent office, and at some point he submitted a\nthesis to Univ. Zurich. That was it. Ah, maybe I shouldn\u2019t admit\nthat this is my favorite model. And now I am also realizing that I have not really \nanswered your question in any meaningful way - sorry for that!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36193, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 36196, "answer": "I wonder if you are oversimplifying the so-called \"European model\" to suit your question.\n\nThe main source of funding for science PhD students in the UK is the EPSRC, which is 3.5 years funding. You are not tied to a project so you can pursue whatever you please, providing your supervisor is willing to go along with you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36197, "question": "I wonder if you are oversimplifying the so-called \"European model\" to suit your question.\n\nThe main source of funding for science PhD students in the UK is the EPSRC, which is 3.5 years funding. You are not tied to a project so you can pursue whatever you please, providing your supervisor is willing to go along with you.", "aSentId": 36198, "answer": "I probably am. I don't know much about grad school in Europe apart from what i hear from a few friends here and there. My impression tells me it is kind of different from grad school in America. I'd like to hear from someone with more insight. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36193, "question": "What do you think about the american model of grad school (5 years on average, teaching duties, industry internships, freedom to explore and zero in on a research problem) versus the european model (3 years, contracted for a specific project, no teaching duties, limited industry internships)? ", "aSentId": 36200, "answer": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36201, "question": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "aSentId": 36202, "answer": "i guess we might be looking at different programs.... i see a lot of emails on ML mailing lists about phd positions to work on a certain problem, on a contract of three years. i also know people doing phd at a max planck-affiliated program, where they don't teach, but work on research. the contracts are for three years from what i've seen and some people might take a couple of years more. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36203, "question": "i guess we might be looking at different programs.... i see a lot of emails on ML mailing lists about phd positions to work on a certain problem, on a contract of three years. i also know people doing phd at a max planck-affiliated program, where they don't teach, but work on research. the contracts are for three years from what i've seen and some people might take a couple of years more. ", "aSentId": 36204, "answer": "That could be, because Max Planck is a research center, not a university. Then I can imagine that the time period is shorter. I guess the same applies to a few other research centers in Europe. Is there no such thing in the USA?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36201, "question": "Grad school is PhD? I've never heard of a 3 year PhD in Europe, or one without teaching duties... Typical is 4 years minimal (can be longer) and definitely teaching duties", "aSentId": 36206, "answer": "In Denmark, and by extension most of Europe by way of Bologna I believe (not counting UK), we follow a rather strict 3-2-3 year program (undergraduate, followed by graduate, followed by PhD). In Denmark the PhD is not extendable beyond 3 years, but there are some teaching duties.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36207, "question": "In Denmark, and by extension most of Europe by way of Bologna I believe (not counting UK), we follow a rather strict 3-2-3 year program (undergraduate, followed by graduate, followed by PhD). In Denmark the PhD is not extendable beyond 3 years, but there are some teaching duties.", "aSentId": 36208, "answer": "I have heard that about Denmark before. However phd time is not in any bologna agreement AFAIK.\n\nAt least UK, Netherlands and Belgium all have 4 years PhD, and I'm fairly certain Sweden, France and German universities as well... (All based on lab member phd duration)\n\nI tried googling what the typical length of a PhD is in Europe, but found no definitive answer. It seems it is not strictly defined, some countries have 3, most have 4, some can be extended to 5. I found no statistics on how often those lengths apply in reality, so it is difficult to say what happens most frequently.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36210, "question": "What do you think about using ontologies / semantic information (DBPedia, Wikidata) as a substrate / mould for ANNs to generate more versatile networks?", "aSentId": 36211, "answer": "Sounds like a great idea! Perhaps relevant:  Ilya Sutskever &amp; Oriol Vinyals &amp; Quoc V. Le use LSTM recurrent neural networks to access semantic information for English-to-French translation, with great success: http://arxiv.org/abs/1409.3215. And Oriol Vinyals &amp; Lukasz Kaiser &amp; Terry Koo &amp; Slav Petrov &amp; Ilya Sutskever &amp; Geoffrey Hinton use LSTM to\nread a sentence, and decode it into a flattened tree. They achieve excellent constituency parsing results: http://arxiv.org/abs/1412.7449", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36213, "question": "(in relation to the Atari paper and partly on your statement about it)\n\nWhat do you personally think about using a diverse selection of video games as a learning problem / \"dataset\"?\n\nOne thing I found interesting about the DeepMind Nature paper is that they could not solve Montezuma's Revenge at all (the game, not the travel problem), which is an action-adventure game requiring some kind of real-world knowledge / thinking - and temporal planning, of course. As any Atari game, conceptually it is still rather simple.\n\nI wonder what would happen if we found an AI succeeding over a wide range of complex game concepts like e.g. Alpha Centauri / Civilization, SimCity, Monkey Island II (for humorous puns, such as \"monkey wrench\"), put it into a robot and unleash it on the real world.", "aSentId": 36214, "answer": "&gt; in relation to the Atari paper and partly on your statement about it\n\nCan you point me to his statement about it?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36221, "question": "Two questions, if I may:\n\n1. With Moore's law gradually coming to an end, it would seem that we won't be achieving anything even close to General AI on today's hardware, at least not economically. As a researcher at the forefront of the field, are you aware of any hardware \"game changers\" that may simplify training and execution of extremely large neural networks that may be capable of intelligence?\n\n2. What are some of the most exciting papers that you have read (or written) in the past year?", "aSentId": 36222, "answer": "&gt; With Moore's law gradually coming to an end\n\nSource? GPUs have just picked up the Moore torch and is now carrying the field. Ive seen no reason why this won't continue for 1 or 2 more cycles before something new  like graphene will be in production.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36225, "question": "What advice do you have for a BTech computer science student passionate about strong AI hoping to join your team at IDSIA someday?", "aSentId": 36226, "answer": "Read our papers, re-implement one of our systems, perhaps improve it a bit, or better a lot, or do something else that I was not able to think of because it\u2019s too original!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36231, "question": "Where did you get the joke about the three prisoners? ", "aSentId": 36232, "answer": "You mean the one that starts: \"Three prisoners walk into a bar ...\"? :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36238, "question": "What is the algorithm of love?\n", "aSentId": 36239, "answer": "For those who did not grok: Schmidhuber works on the formal theory of curiosity and epistemic value. What is the best formal account of co-operation / affection / attachment, a.k.a. \"love\"? For instance, Minsky refers to \"attachment learning\", albeit without formalization.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36238, "question": "What is the algorithm of love?\n", "aSentId": 36241, "answer": "Great question!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36255, "question": "Do you think having a PhD is important if one wants to work in a good research team?", "aSentId": 36256, "answer": "Not at all - my PhD students are doing excellent work, but don't have a PhD :-)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36258, "question": "How feasible is it for a non-expert to successfully run RNN code on a new dataset? Is there any high-quality open source code to do it?", "aSentId": 36259, "answer": "alex graves has a toolbox called RNNLIB. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36261, "question": "i understand that neural networks and deep learning are computationally intensive for non-trivial problems. In addition, many experiments are necessary to see what works and what does not. What sort of equipment do you recommend for doing research in this area without breaking the bank? ", "aSentId": 36262, "answer": "As long as your applications are not too ambitious, a desktop machine with one or more GPUs should do!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36271, "question": "What do you think of Bitcoin. ", "aSentId": 36272, "answer": "I thought more of it when I had more of it.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36274, "question": "Why do so many chinamen flood the ML community with rubbish?", "aSentId": 36275, "answer": "Whoops, looks like Grandma found Reddit", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36278, "question": "Self-Paced Coursera Machine Learning now available!", "aSentId": 36279, "answer": "Third time's the charm! Specially since I can't fall behind now.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36280, "question": "Third time's the charm! Specially since I can't fall behind now.", "aSentId": 36281, "answer": "I'm in the same boat as you. I think algorithms is another class that should be self-paced IMO.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36282, "question": "I'm in the same boat as you. I think algorithms is another class that should be self-paced IMO.", "aSentId": 36283, "answer": "I agree about algorithms, though I didn't find any on Coursera.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36284, "question": "I agree about algorithms, though I didn't find any on Coursera.", "aSentId": 36285, "answer": "Khan academy has a self paced algorithms course using JavaScript.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36278, "question": "Self-Paced Coursera Machine Learning now available!", "aSentId": 36287, "answer": "Any differences to the old version?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36278, "question": "Self-Paced Coursera Machine Learning now available!", "aSentId": 36289, "answer": "I'm always a bit reluctant to take courses using matlab or R. I guess I can see past that if I know that the course is really good. Has anybody taken this course and can share their experience with how well the assignments are put together and their correlation to the lectures?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36290, "question": "I'm always a bit reluctant to take courses using matlab or R. I guess I can see past that if I know that the course is really good. Has anybody taken this course and can share their experience with how well the assignments are put together and their correlation to the lectures?", "aSentId": 36291, "answer": "It's a very good introduction to machine learning and hit's on a lot of the most useful/well-known areas. Another component I really appreciate is Ng's constant emphasis on keeping your code as compact and as efficient as possible.\n\nAll that said, this is very much an intro course and will not make you an expert in one go but I wouldn't let that get in the way if this is a topic you are genuinely interested in.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36292, "question": "It's a very good introduction to machine learning and hit's on a lot of the most useful/well-known areas. Another component I really appreciate is Ng's constant emphasis on keeping your code as compact and as efficient as possible.\n\nAll that said, this is very much an intro course and will not make you an expert in one go but I wouldn't let that get in the way if this is a topic you are genuinely interested in.", "aSentId": 36293, "answer": "Thank you for sharing your experience. What application should someone who's done three years of CS studies be able to develop after taking this course?  \n\nAlso, have you taken the Berkely (CS188) course on edx? If so, could you please give a comparison of the two?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36294, "question": "Thank you for sharing your experience. What application should someone who's done three years of CS studies be able to develop after taking this course?  \n\nAlso, have you taken the Berkely (CS188) course on edx? If so, could you please give a comparison of the two?", "aSentId": 36295, "answer": "You would be able to develop applications that employ a practical use of off-the-shelf classification and clustering methods after taking the Coursera's course. A lot of things that seemed like black magic before will become easier once you know the basics of ML: how write a program that recognises groups of related objects and classifies stuff into certain types etc.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36294, "question": "Thank you for sharing your experience. What application should someone who's done three years of CS studies be able to develop after taking this course?  \n\nAlso, have you taken the Berkely (CS188) course on edx? If so, could you please give a comparison of the two?", "aSentId": 36297, "answer": "Andrew Ng's course is excellent at 'demystifying' machine learning algorithms. It will give you intuition of how/why and when machine learning algorithms work.\n\nI am currently taking the Berkely AI course, and they are nothing alike. Ng's course focuses on techniques for regression, classification, and clustering whereas Berkely thus-far has focused on graph-search algorithms used in programming autonomous agents.\n\nIf you're looking for a python based course, Udacity has an Intro to ML (free) that is python sklearn based- though I don't believe you'll get the intuition and understanding you get from Ng's.\n\n[And, nothing prevents you from completing Ng's assignments in another language in addition to the octave/matlab. Experience in multiple languages never hurts.]", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36294, "question": "Thank you for sharing your experience. What application should someone who's done three years of CS studies be able to develop after taking this course?  \n\nAlso, have you taken the Berkely (CS188) course on edx? If so, could you please give a comparison of the two?", "aSentId": 36299, "answer": "I haven't taken the Berkeley course, but just looking at the content it seems a bit more advanced than the Machine Learning course.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36290, "question": "I'm always a bit reluctant to take courses using matlab or R. I guess I can see past that if I know that the course is really good. Has anybody taken this course and can share their experience with how well the assignments are put together and their correlation to the lectures?", "aSentId": 36301, "answer": "You need to know very little Matlab in order to succeed in this course. You'll need to do little else in code than the actual algebra code. All the rest - loading data, plotting, preprocessing - is done for you. This can be a good or a bad thing, depending how deeply you plan to know Matlab after this course.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36290, "question": "I'm always a bit reluctant to take courses using matlab or R. I guess I can see past that if I know that the course is really good. Has anybody taken this course and can share their experience with how well the assignments are put together and their correlation to the lectures?", "aSentId": 36303, "answer": "I am currently taking the course (currently on week 7). The assignments are very well put together and Ng gives a lot of advice on the practical side of implementing algorithms. I was able to pickup the relevant Octave knowledge in a day or two - he even has a tutorial on it. This course is  in short a great foundation for further studies in Machine Learning.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36304, "question": "I am currently taking the course (currently on week 7). The assignments are very well put together and Ng gives a lot of advice on the practical side of implementing algorithms. I was able to pickup the relevant Octave knowledge in a day or two - he even has a tutorial on it. This course is  in short a great foundation for further studies in Machine Learning.", "aSentId": 36305, "answer": "&gt; The assignments are very well put together \n\nHave their solved the issue of unclear instructions combined with an incredibly unhelpful TA on the forum? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36306, "question": "&gt; The assignments are very well put together \n\nHave their solved the issue of unclear instructions combined with an incredibly unhelpful TA on the forum? ", "aSentId": 36307, "answer": "As far as I can see, the instructions could not be clearer. Everything is explained a minimum of 3 times in the PDF document. The only time I needed the forum was when I needed to modify a line in the supplied code that wouldn't work in my version of Octave, and that was the top thread in the exercise's forum.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36309, "question": "If only a machine could learn this for me...", "aSentId": 36310, "answer": "A machine that deeply believes in learning", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36314, "question": "Needs Matlab and Octave to do stuff with. I cannot afford those.", "aSentId": 36315, "answer": "octave is the free and open source version of matlab. it does not cost anything.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36316, "question": "octave is the free and open source version of matlab. it does not cost anything.", "aSentId": 36317, "answer": "I didn't know that. Thanks for telling me.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36316, "question": "octave is the free and open source version of matlab. it does not cost anything.", "aSentId": 36319, "answer": "Octave cannot submit assignments for this course. They require a 120 day demo of Matlab to submit assignments. I'll follow this course but I won't be able to submit assignments. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36320, "question": "Octave cannot submit assignments for this course. They require a 120 day demo of Matlab to submit assignments. I'll follow this course but I won't be able to submit assignments. ", "aSentId": 36321, "answer": "If it is the same course as the one I am currently taking in Coursera (and it likely is), then it is 100% Octave compatible. Octave isn't the best program, the GUI especially, but it does the job.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36322, "question": "If it is the same course as the one I am currently taking in Coursera (and it likely is), then it is 100% Octave compatible. Octave isn't the best program, the GUI especially, but it does the job.", "aSentId": 36323, "answer": "I don't know how to use it nor Matlab. I got some learning to do.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36324, "question": "I don't know how to use it nor Matlab. I got some learning to do.", "aSentId": 36325, "answer": "Don't worry - the course includes an Octave tutorial. As I mentioned elsewhere, you don't need to know much to do this course, as the programming exercises are in a \"fill in the blanks\" format.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36326, "question": "Don't worry - the course includes an Octave tutorial. As I mentioned elsewhere, you don't need to know much to do this course, as the programming exercises are in a \"fill in the blanks\" format.", "aSentId": 36327, "answer": "I didn't see an Octave tutorial, I will look again.\n\nI just failed the first quiz, it is really hard and I only got 2 out of 5 correct.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36328, "question": "I didn't see an Octave tutorial, I will look again.\n\nI just failed the first quiz, it is really hard and I only got 2 out of 5 correct.", "aSentId": 36329, "answer": "Well, you know, you're supposed to *watch the videos* and *study* for the quizzes.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36330, "question": "Well, you know, you're supposed to *watch the videos* and *study* for the quizzes.", "aSentId": 36331, "answer": "I tried but the wording is confusing to me in the quiz.\n\nI see later on there is an Octave tutorial.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36314, "question": "Needs Matlab and Octave to do stuff with. I cannot afford those.", "aSentId": 36333, "answer": "Some other coursera courses give you a free Matlab License for the duration of the course (like the Dynamical Modelling in Systems Biology one).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36334, "question": "Some other coursera courses give you a free Matlab License for the duration of the course (like the Dynamical Modelling in Systems Biology one).", "aSentId": 36335, "answer": "This coursera course gives you a free Matlab License.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36314, "question": "Needs Matlab and Octave to do stuff with. I cannot afford those.", "aSentId": 36337, "answer": ".", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36339, "question": "Is this a Hidden Markov Model?", "aSentId": 36340, "answer": "No. A Markov chain is a simple probabilistic model where state x(k+1) depends only on state x(k). For example, if you know a car's speed is 60 at time t, and in a given time step the probability distribution of the change in speed is N(a,b), then the car's speed is 60+N(a,b) at time t+1. You can then make a prediction of the probability distro at any time in the future.  \n  \nIn a hidden markov model, the state isn't directly known, but estimated from a noisy observation. For the car example, we never know the car's \"actual\" speed, but at time t+1 a sensor (with some probability distro for accuracy) is read at 61. We also know that time t, the speed estimate was 58. Based on these two values (and prior probability distro), we can estimate speed at time t+1 at some value in the range [58,61].", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36341, "question": "No. A Markov chain is a simple probabilistic model where state x(k+1) depends only on state x(k). For example, if you know a car's speed is 60 at time t, and in a given time step the probability distribution of the change in speed is N(a,b), then the car's speed is 60+N(a,b) at time t+1. You can then make a prediction of the probability distro at any time in the future.  \n  \nIn a hidden markov model, the state isn't directly known, but estimated from a noisy observation. For the car example, we never know the car's \"actual\" speed, but at time t+1 a sensor (with some probability distro for accuracy) is read at 61. We also know that time t, the speed estimate was 58. Based on these two values (and prior probability distro), we can estimate speed at time t+1 at some value in the range [58,61].", "aSentId": 36342, "answer": "Thanks for the response :) \n\n&gt; In a hidden markov model, the state isn't directly known, but estimated from a noisy observation.\n\nWould it be fair to say that any key press is a \"noisy\" observation, in that it may have been a mistype. And if that's alright to assume, then when I take into account the surrounding key's probability as the intended key, would that not constitute as a set of \"estimated\" output values?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36344, "question": "Planning to implement Text Understanding from Scratch paper", "aSentId": 36345, "answer": "I haven't done this specific paper, but it should be pretty straightforward to do with theano/lasagne. IIRC lasagne has a couple good 1d conv implementations, so I don't think you should run into any significant hurdles implementing it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36344, "question": "Planning to implement Text Understanding from Scratch paper", "aSentId": 36347, "answer": "That paper looks refreshingly full of specific details, I think it should go pretty smoothly. Good luck, and be sure to post your results!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36344, "question": "Planning to implement Text Understanding from Scratch paper", "aSentId": 36349, "answer": "What datasets do you plan on trying the method on? If I remember correctly, that paper didn't use any publicly available datasets.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36350, "question": "What datasets do you plan on trying the method on? If I remember correctly, that paper didn't use any publicly available datasets.", "aSentId": 36351, "answer": "actually, all the datasets they used are publicly available (although the Yahoo data requires an academic affiliation.)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36352, "question": "actually, all the datasets they used are publicly available (although the Yahoo data requires an academic affiliation.)", "aSentId": 36353, "answer": "You're right, for some reason I remember there being an issue with not using more standard language datasets.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36354, "question": "You're right, for some reason I remember there being an issue with not using more standard language datasets.", "aSentId": 36355, "answer": "There was a bit of huff about how the bow and word2vec benchmarks were established. They limited the number of features arbitrarily to 5000. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36357, "question": "Reimplementation of DeepMind's DRAW in Theano, by Jorg Bornschein", "aSentId": 36358, "answer": "Brilliant! Thanks for sharing!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36357, "question": "Reimplementation of DeepMind's DRAW in Theano, by Jorg Bornschein", "aSentId": 36360, "answer": "How hard was it to tune the hyperparameters?  I imagine that the goal is to make sure that the initial glimpses cover the entire image.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36357, "question": "Reimplementation of DeepMind's DRAW in Theano, by Jorg Bornschein", "aSentId": 36362, "answer": "I'm looking forward to applying this to music, as a songwriter that idea excites me to no end.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36364, "question": "[Python] I have a clustering data mining project, and I am stuck on finding distances to build my clusters.", "aSentId": 36365, "answer": "Just write your own function and store them in an array? You can even make objects to help you....", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36367, "question": "Should I worry about overfitting the cross-validation set?", "aSentId": 36368, "answer": "There are probably more sophisticated methods, but the common methods are:\n\n1. test fewer values by making an educated guess instead of testing the whole spectrum of possibilities\n\n2. use more folds. You can go all the way to leave-one-out in the extreme case", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36370, "question": "Can a convolutional neural network be used to classify malware?", "aSentId": 36371, "answer": "I'd start by thinking about relevant features for the malware detection task and then trying to build feature extractors by hand.  \n\nEven if you eventually go with neural networks, this will give you a good baseline, and you can always feed the hand-crafted features into the neural network.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36373, "question": "Deep Stuff About Deep Learning - Microsoft scientist talks about the math behind deep learning, and the effort to understand it on a theoretical level", "aSentId": 36374, "answer": "The primary thing I want to see is the simplest statement of statistical convergence.  That is, given dataset (x_i=raw-input-features, y_i=label) of size n which are IID draws from joint distribution P (specified below), the desired theorem would state: \"With probability -&gt; 1 the network optimized with SGD over n examples will correctly classify a new X randomly sampled from the data-generating process with accuracy approaching the Bayes-rate as n -&gt; infinity\".  This would tie together the optimization/statistical-generalization issues which seem fundamentally entangled when it comes to these beasts.\n\nAssuming we are working with a feed-forward architecture, we endow P (aka the data-generating process) with the most natural structure, which is to assume it exactly follows the inverse of the architecture of the neural net. We thus assume each (x_i, y_i) is independently  generated by the following steps:\n\n1. y ~ Prior distribution on the possible labels (i.e. categorical distribution parameterized by the true proportion of labels = 1 in the two-class case).\n\n2. latent-variable z1 (with dimensionality = width of topmost hidden-layer of network) is sampled from a specified-parametric conditional distribution P[Z1 | Y] where Y = y from step 1 \n\n3.  For each hidden-layer j below this one:  latent-variable zj (with dimensionality = width of network's jth-from-top hidden-layer) is sampled from a specified-parametric conditional distribution P[ ZJ | Z(J-1) ] where Z(J-1) = z(j-1) from the previous step\n\n4.  Finally x is sampled from a specified parametric conditional distribution P[ X | Zt ] where Zt = zt from the previous step and t = bottom-most hidden-layer.\n\nAll that's left to specify is the form of these conditional distributions P[ | ] which we would do in such a way such that it's not overly difficult to determine which parameterization of this same feed-work architecture (where these  unknown parameters depend on the conditional distributions' parameters) achieves the Bayes optimal classification-rate.\n\n\n\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36375, "question": "The primary thing I want to see is the simplest statement of statistical convergence.  That is, given dataset (x_i=raw-input-features, y_i=label) of size n which are IID draws from joint distribution P (specified below), the desired theorem would state: \"With probability -&gt; 1 the network optimized with SGD over n examples will correctly classify a new X randomly sampled from the data-generating process with accuracy approaching the Bayes-rate as n -&gt; infinity\".  This would tie together the optimization/statistical-generalization issues which seem fundamentally entangled when it comes to these beasts.\n\nAssuming we are working with a feed-forward architecture, we endow P (aka the data-generating process) with the most natural structure, which is to assume it exactly follows the inverse of the architecture of the neural net. We thus assume each (x_i, y_i) is independently  generated by the following steps:\n\n1. y ~ Prior distribution on the possible labels (i.e. categorical distribution parameterized by the true proportion of labels = 1 in the two-class case).\n\n2. latent-variable z1 (with dimensionality = width of topmost hidden-layer of network) is sampled from a specified-parametric conditional distribution P[Z1 | Y] where Y = y from step 1 \n\n3.  For each hidden-layer j below this one:  latent-variable zj (with dimensionality = width of network's jth-from-top hidden-layer) is sampled from a specified-parametric conditional distribution P[ ZJ | Z(J-1) ] where Z(J-1) = z(j-1) from the previous step\n\n4.  Finally x is sampled from a specified parametric conditional distribution P[ X | Zt ] where Zt = zt from the previous step and t = bottom-most hidden-layer.\n\nAll that's left to specify is the form of these conditional distributions P[ | ] which we would do in such a way such that it's not overly difficult to determine which parameterization of this same feed-work architecture (where these  unknown parameters depend on the conditional distributions' parameters) achieves the Bayes optimal classification-rate.\n\n\n\n\n", "aSentId": 36376, "answer": "Read the Arora paper the blogpost cites. It's pretty close although doesn't use SGD.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36377, "question": "Read the Arora paper the blogpost cites. It's pretty close although doesn't use SGD.", "aSentId": 36378, "answer": "Yes but that paper makes the completely unrealistic assumption that each edge is an entirely  random weight (independently of the others; the training procedure they develop entirely hinges on this), whereas the whole strength of Neural Nets is the ability of the weights to work together in a way that useful representations are extracted.  Neural Nets with random edge weights = a random graph, not much of an interesting classifier.\n\nWhat I'm referring to is the scenario in which the data is IID (not the edge weights) which is a perfectly reasonable assumption in many cases (despite my username :/), and we know the distribution P is such that there exists a parameter setting for which the specific architecture produces good predictions.  In the usual statistics setting, one would then ask: Can we recover these parameters from data as n -&gt; infinity? (aka consistency) which is almost certainly too much to ask for in highly over-parameterized SGD-trained neural-nets with a very-nonconvex objective.  Thus, the question I pose instead is: Can SGD recover some (possibly-very-different) parameter-setting which produces almost as good performance on new examples as the optimal parameter setting?\n\nNote this is not entirely a question about the optimization process, the measure of \"goodness\" here is generalization error, an unobservable quantity.  Given such a theorem, one could then argue the data-types for which neural nets have excelled (e.g. speech, text, &amp; images) come from compositional distributions like the one I previously described (e.g. the distribution over image-pixels comes from a composition of distributions over parts of objects/scenes), so as long as we match the architecture to this compositional process, the NN classifier should perform well.  For me, this would be the first convincing result which theoretically justifies the good-performance of NNs with the training-procedures currently in use.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36373, "question": "Deep Stuff About Deep Learning - Microsoft scientist talks about the math behind deep learning, and the effort to understand it on a theoretical level", "aSentId": 36380, "answer": "Interesting read! I'm a student trying to wrap my brain around the overwhelming popularity of neural nets and deep learning. It all seems rather ~~mystical~~ \"mysterious\", as is implied in this blog post. But from the books I've managed to find at my uni's library (dating from the late 90's), there clearly is rock-solid(?) mathematics behind the theory. \n\nDoes anyone else share the same or different feelings from that of the post's author?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36381, "question": "Interesting read! I'm a student trying to wrap my brain around the overwhelming popularity of neural nets and deep learning. It all seems rather ~~mystical~~ \"mysterious\", as is implied in this blog post. But from the books I've managed to find at my uni's library (dating from the late 90's), there clearly is rock-solid(?) mathematics behind the theory. \n\nDoes anyone else share the same or different feelings from that of the post's author?", "aSentId": 36382, "answer": "The math is rock-solid. What is not is formally proving why some of these networks work better. Why do certain \"tweaks\" or turning certain knobs improve performance, counter overfitting, etc.\n\nNot a perfect analogy but at a high level you can think about the Navier-Stokes equations used for computation fluid dynamics calculations. They work but no one has proved that a closed form solution exists. \n\nThat is how I interpreted the article. Author is basically saying this stuff works great, but \"why?\".", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36383, "question": "The math is rock-solid. What is not is formally proving why some of these networks work better. Why do certain \"tweaks\" or turning certain knobs improve performance, counter overfitting, etc.\n\nNot a perfect analogy but at a high level you can think about the Navier-Stokes equations used for computation fluid dynamics calculations. They work but no one has proved that a closed form solution exists. \n\nThat is how I interpreted the article. Author is basically saying this stuff works great, but \"why?\".", "aSentId": 36384, "answer": "I'm not sure what you could prove for neural networks, since the architecture questions are related to properties of the data and the models that we're interested in learning.  \n\nYou can definitely come up with a synthetic problem, for example n-bit parity, and then prove that certain types of networks are insufficient.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36373, "question": "Deep Stuff About Deep Learning - Microsoft scientist talks about the math behind deep learning, and the effort to understand it on a theoretical level", "aSentId": 36386, "answer": "I suppose that an interesting question is what kind of theory the community wants.  \n\nHere's a (possibly naive) wishlist of deep learning theorems: \n\n1.  A statistical consistency proof for backpropagation through time with teacher-forcing.  Characterization of estimator bias (perhaps with a simplified architecture or assumptions).  \n\n2.  Statistical consistency proof for variational bayes (not sure if this actually holds).  \n\n3.  Characterization of well initialized neural networks that shows why they tend to be very easy to optimize in practice, despite being non-convex (whereas other non-convex problems, like fitting a mixture of gaussians, can't be optimized locally in practice).  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36387, "question": "I suppose that an interesting question is what kind of theory the community wants.  \n\nHere's a (possibly naive) wishlist of deep learning theorems: \n\n1.  A statistical consistency proof for backpropagation through time with teacher-forcing.  Characterization of estimator bias (perhaps with a simplified architecture or assumptions).  \n\n2.  Statistical consistency proof for variational bayes (not sure if this actually holds).  \n\n3.  Characterization of well initialized neural networks that shows why they tend to be very easy to optimize in practice, despite being non-convex (whereas other non-convex problems, like fitting a mixture of gaussians, can't be optimized locally in practice).  ", "aSentId": 36388, "answer": "Coming from a pure math background, one of the most fun, and frustrating, thing about ML is how hard it is to prove anything about the performance of an algorithm on a general dataset. Given any learning algorithm, one can probably construct some pathological dataset on which is performs very slowly.\n\nWhat I'd like to see is a more rigorous classification of what a dataset is mathematically, and then theorems something like \"learning algorithm X produces a classifier with at least 90% accuracy in O(?) time on a full-measure set of (some subclass of) datasets.\"\n\nI'm not even sure such a theorem is likely to be true, but since tabular data is basically just a collection of points in R^n (or R^n x Z^n depending on how you think of discrete fields), it's really some theorem about certain classes of functions on subsets of R^n, so *something* interesting must be true there.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36389, "question": "Coming from a pure math background, one of the most fun, and frustrating, thing about ML is how hard it is to prove anything about the performance of an algorithm on a general dataset. Given any learning algorithm, one can probably construct some pathological dataset on which is performs very slowly.\n\nWhat I'd like to see is a more rigorous classification of what a dataset is mathematically, and then theorems something like \"learning algorithm X produces a classifier with at least 90% accuracy in O(?) time on a full-measure set of (some subclass of) datasets.\"\n\nI'm not even sure such a theorem is likely to be true, but since tabular data is basically just a collection of points in R^n (or R^n x Z^n depending on how you think of discrete fields), it's really some theorem about certain classes of functions on subsets of R^n, so *something* interesting must be true there.", "aSentId": 36390, "answer": "Doesn't No Free Lunch Theorem prevent you from giving any performance guarantees for an arbitrary problem?\n\nI heavily agree with your idea about narrowing the set of datasets considered. I think there's something fundamental about our universe so that our observations have some kind of an inner structure, that is simple enough. Just like with functions: there are lots and lots of different functions possible, but we are mostly interested in continuous ones (or with some structure in the set of discontinuities).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36391, "question": "Doesn't No Free Lunch Theorem prevent you from giving any performance guarantees for an arbitrary problem?\n\nI heavily agree with your idea about narrowing the set of datasets considered. I think there's something fundamental about our universe so that our observations have some kind of an inner structure, that is simple enough. Just like with functions: there are lots and lots of different functions possible, but we are mostly interested in continuous ones (or with some structure in the set of discontinuities).", "aSentId": 36392, "answer": "&gt; I think there's something fundamental about our universe so that our observations have some kind of an inner structure, that is simple enough.\n\nI agree and I think it's all about the timing of sensory signals. There is something rigorously deterministic, if not purely geometric, about the way the world changes and I believe that this is an assumption that is used by the brain at a fundamental level. If true, it is a fixed principle that governs the representational structure and operation of cortical memory and the nature of sensory data. The brain could not make sense of the world otherwise. Just thinking out loud.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36391, "question": "Doesn't No Free Lunch Theorem prevent you from giving any performance guarantees for an arbitrary problem?\n\nI heavily agree with your idea about narrowing the set of datasets considered. I think there's something fundamental about our universe so that our observations have some kind of an inner structure, that is simple enough. Just like with functions: there are lots and lots of different functions possible, but we are mostly interested in continuous ones (or with some structure in the set of discontinuities).", "aSentId": 36394, "answer": "This is why a characterisation of data set types would be interesting.  No Free Lunch is a statement about average performance over all possible problems.  If you restrict yourself to a subset of problems you can still give guarantees.\n\nNow the question is, how do you formally describe the \"interesting\" subset of problems?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36389, "question": "Coming from a pure math background, one of the most fun, and frustrating, thing about ML is how hard it is to prove anything about the performance of an algorithm on a general dataset. Given any learning algorithm, one can probably construct some pathological dataset on which is performs very slowly.\n\nWhat I'd like to see is a more rigorous classification of what a dataset is mathematically, and then theorems something like \"learning algorithm X produces a classifier with at least 90% accuracy in O(?) time on a full-measure set of (some subclass of) datasets.\"\n\nI'm not even sure such a theorem is likely to be true, but since tabular data is basically just a collection of points in R^n (or R^n x Z^n depending on how you think of discrete fields), it's really some theorem about certain classes of functions on subsets of R^n, so *something* interesting must be true there.", "aSentId": 36396, "answer": "There have been plenty of frameworks which provide solid theoretical grounding for algorithms/estimators, see eg:\nStatistical learning theory, PAC learning or PAC-Bayes, Minimax estimation or just estimation/decision theory in general.\n\nThese generally assume datasets are IID draws from some distribution, so you cannot pathologically manipulate individual points, rather you can only adversarially construct the distributions.  In statistics, one calls the data-generating process a \"model\" which is fully mathematically specified, and precisely characterizes the set of all data-sets one could observe from this model.  Subfields with rich  theory regarding when individual points themselves can be pathologically manipulated are \"robust statistics\" and \"online learning with an adversary\" \n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36398, "question": "&gt; So far I would say that the method described above kind of make sense (apart from the max-pooling operation which I really do not understand)\n\n... Deep stuff indeed.", "aSentId": 36399, "answer": "Hinton agrees with him fyi.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36400, "question": "Hinton agrees with him fyi.", "aSentId": 36401, "answer": "He thinks that max pooling is not always a good idea and that it's embarrassing how well convolutional nets with max pooling work. Geoff certainly understands max pooling.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36402, "question": "He thinks that max pooling is not always a good idea and that it's embarrassing how well convolutional nets with max pooling work. Geoff certainly understands max pooling.", "aSentId": 36403, "answer": "I'm pretty sure this guy does too, considering he's a professor at Princeton.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36404, "question": "I'm pretty sure this guy does too, considering he's a professor at Princeton.", "aSentId": 36405, "answer": "... Did you not read the quotation?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36406, "question": "... Did you not read the quotation?", "aSentId": 36407, "answer": "... did you not read the rest of the article?\n\nIt's pretty clear from context that he's expressing an opinion similar to that of Hinton's - that maxpooling does not have a clear justification.\n\nI can't believe you seriously think someone employed by Microsoft Research and Princeton could fail to understand something as simple as maxpooling.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36408, "question": "... did you not read the rest of the article?\n\nIt's pretty clear from context that he's expressing an opinion similar to that of Hinton's - that maxpooling does not have a clear justification.\n\nI can't believe you seriously think someone employed by Microsoft Research and Princeton could fail to understand something as simple as maxpooling.", "aSentId": 36409, "answer": "The rest of the article was pretty uninsightful as these things go, and assertions like \"it seems to me that no one has a real clue about what\u2019s going on\" ignores a lot of solid empirical work on the topic going back over 20 years. I'm sure he has interesting things to say on other topics, but theoreticians tend to see things through a pretty myopic lens. \n\nAnd no, nowhere did he express even a vague understanding of what the purpose of max pooling was, or even venture a guess as to why it might perform better than, say, average pooling. You seem to be imputing a lot of implied meaning onto what was actually said.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36410, "question": "The rest of the article was pretty uninsightful as these things go, and assertions like \"it seems to me that no one has a real clue about what\u2019s going on\" ignores a lot of solid empirical work on the topic going back over 20 years. I'm sure he has interesting things to say on other topics, but theoreticians tend to see things through a pretty myopic lens. \n\nAnd no, nowhere did he express even a vague understanding of what the purpose of max pooling was, or even venture a guess as to why it might perform better than, say, average pooling. You seem to be imputing a lot of implied meaning onto what was actually said.", "aSentId": 36411, "answer": "&gt; The max-pooling is a dimension reduction operation, which simply takes 4 adjacent coordinates (in the case of an image, 4 adjacent pixels) and returns the maximal element. \n\nThat sentence would seem to me to indicate that he understands the operation and purpose of maxpooling... and his point is, it's a open question when and why it is better than average pooling (there have been empirical papers showing cases where it is unnecessary or even suboptimal.)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36413, "question": "How to PCA large data sets? I'm running out of memory.", "aSentId": 36414, "answer": "Taking PCA over a subset of your data can work, though as you said choosing a \"representative subsample\" is hard.\n\nThere is an IncrementalPCA in sklearn master that will do a minibatch computation. If you keep all the components the results are exact - smaller results (n_components &lt; n_features) will have differences due to the computation of SVD *then* slicing in sklearn's batch PCA vs slicing each minibatch in the other version. I suppose you could keep all the components til the end, then slice, but then this is wasted computation. Not sure what the best option is here.\n\nIt would be great to add the radomized SVD solver for each minibatch to make it even faster for small numbers of n_components. I was hoping to tackle it during the PyCon sprints in a few weeks but PRs are always welcome :)\n\nFull disclosure, I wrote the IncrementalPCA that is in sklearn right now.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36415, "question": "Taking PCA over a subset of your data can work, though as you said choosing a \"representative subsample\" is hard.\n\nThere is an IncrementalPCA in sklearn master that will do a minibatch computation. If you keep all the components the results are exact - smaller results (n_components &lt; n_features) will have differences due to the computation of SVD *then* slicing in sklearn's batch PCA vs slicing each minibatch in the other version. I suppose you could keep all the components til the end, then slice, but then this is wasted computation. Not sure what the best option is here.\n\nIt would be great to add the radomized SVD solver for each minibatch to make it even faster for small numbers of n_components. I was hoping to tackle it during the PyCon sprints in a few weeks but PRs are always welcome :)\n\nFull disclosure, I wrote the IncrementalPCA that is in sklearn right now.", "aSentId": 36416, "answer": "Very cool! Do you have advice on how to select a best mini batch size? I'd like for it to use as much machine resource as possible to go as fast as possible.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36417, "question": "Very cool! Do you have advice on how to select a best mini batch size? I'd like for it to use as much machine resource as possible to go as fast as possible.", "aSentId": 36418, "answer": "That you will probably have to tune by hand - I think the default is like dataset_size / 5 . That is just a heuristic that seemed to work OK for me, though. No particular reasoning behind the choice. Also if it is float64 you might try casting as float32 - a 50% reduction in memory usage is nothing to sneeze it! Unless you really need the precision.\n\nYou can probably calculate a good minibatch size by calculating the size of data that can fit into memory, then multiplying by .8 (80% engineering rule-of-thumb) and dividing by 2 (property of algorithm is ~2x minibatch size if I recall) AKA I can fit 1000 float64 values in my memory, so 400 is a good number. Then look at the flattened size of the minibatch - if you have 4 features, minibatch size 100 seems nice (100x4).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36419, "question": "That you will probably have to tune by hand - I think the default is like dataset_size / 5 . That is just a heuristic that seemed to work OK for me, though. No particular reasoning behind the choice. Also if it is float64 you might try casting as float32 - a 50% reduction in memory usage is nothing to sneeze it! Unless you really need the precision.\n\nYou can probably calculate a good minibatch size by calculating the size of data that can fit into memory, then multiplying by .8 (80% engineering rule-of-thumb) and dividing by 2 (property of algorithm is ~2x minibatch size if I recall) AKA I can fit 1000 float64 values in my memory, so 400 is a good number. Then look at the flattened size of the minibatch - if you have 4 features, minibatch size 100 seems nice (100x4).", "aSentId": 36420, "answer": "Thank you. What is the impact of the decision for a particular mini batch value. Is the Impact only in running speed? Or can the choice of mini batch also impact the accuracy / algorithm performance?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36421, "question": "Thank you. What is the impact of the decision for a particular mini batch value. Is the Impact only in running speed? Or can the choice of mini batch also impact the accuracy / algorithm performance?", "aSentId": 36422, "answer": "Impact *should* only be running speed. There may be small variances in algorithm performance if you are dropping components, though. In general, larger is better.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36413, "question": "How to PCA large data sets? I'm running out of memory.", "aSentId": 36424, "answer": "Gensim has online PCA, it's called LSI/LSA:\nhttp://radimrehurek.com/gensim/models/lsimodel.html", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36413, "question": "How to PCA large data sets? I'm running out of memory.", "aSentId": 36426, "answer": "If your dataset has fewer samples than dimensions, then you can run PCA using the gram matrix version instead of the usual covariance matrix version. This is equivalent to kernel PCA with the linear kernel. \n\nLoad your dataset one feature at a time. Compute the dot product for that one feature. Do the same for the second feature and add it to the gram matrix of the first feature (due to additivity of the linear kernel). Then run kernel PCA with a linear kernel", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36429, "question": "NVIDIA Digits DevBox and Deep Learning Demonstration - GTC 2015", "aSentId": 36430, "answer": "Was digits the interface or the box? Or both?\n\nedit: It's both.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36429, "question": "NVIDIA Digits DevBox and Deep Learning Demonstration - GTC 2015", "aSentId": 36432, "answer": "Em Enn IST", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36434, "question": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "aSentId": 36435, "answer": "I love when authors admit that their previous work didn't work so good:\n\n&gt; And unlike some previously proposed approximate natural-gradient/Newton methods such as Hessian-free methods, K-FAC works very well in highly stochastic optimization regimes. \n\nThis new method looks interesting, code anyone?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36437, "question": "Partially Derivative Episode 17: Get Back To Work You Slackers!", "aSentId": 36438, "answer": "missed them last week. 8:( (I'm told by my daughter that's a sad panda)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36439, "question": "missed them last week. 8:( (I'm told by my daughter that's a sad panda)", "aSentId": 36440, "answer": "Thanks man! I'll admit I have no idea how to make a sad panda emoji.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36442, "question": "Getting Started with Deep Learning - resources and roadmap", "aSentId": 36443, "answer": "Could probably consolidate some resources by including kevin murphys book.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36442, "question": "Getting Started with Deep Learning - resources and roadmap", "aSentId": 36445, "answer": "does anybody have the rest of Ng's ufldl videos?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36442, "question": "Getting Started with Deep Learning - resources and roadmap", "aSentId": 36447, "answer": "Skip all the math and basic algorithm classes. You get the math you need in Ng's and Domingos's machine learning courses. Basic algorithms classes are not that relevant. What is lacking is a short basic statistics and probability tutorial, a vectorization tricks tutorial, an intro to general research methods, a good course on feature engineering, a good practical class on the ML frameworks like Torch 7, Theano, Caffe, Weka and so on.  Classes from the schools to often have to much of a math focus and to little practical application focus. The math should come second or made more intuitive. Down the road once one is confortable using the packages well with real world situations then go deeper with math theory. This is really for those that want to invent something new or get a PhD.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36448, "question": "Skip all the math and basic algorithm classes. You get the math you need in Ng's and Domingos's machine learning courses. Basic algorithms classes are not that relevant. What is lacking is a short basic statistics and probability tutorial, a vectorization tricks tutorial, an intro to general research methods, a good course on feature engineering, a good practical class on the ML frameworks like Torch 7, Theano, Caffe, Weka and so on.  Classes from the schools to often have to much of a math focus and to little practical application focus. The math should come second or made more intuitive. Down the road once one is confortable using the packages well with real world situations then go deeper with math theory. This is really for those that want to invent something new or get a PhD.", "aSentId": 36449, "answer": "You are a strange troll.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36450, "question": "You are a strange troll.", "aSentId": 36451, "answer": "Not at all. It just is a waste of time taking irrelevent classes. Time is money for those taking courses in terms of losses but it is a gain for those giving them. What i stated were simple facts known to practitioners like Kaggle competition winners. \n\nBTW, i like Pedro Domingos maybe more than Andrew Ng's. Both would be good compliments to each other.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36453, "question": "[Research question] What are some non-empirical and theoretical areas in ML?", "aSentId": 36454, "answer": "Statistical learning theory is about proving that the learned hypotheses will generalize, i.e., work on unseen data. Proofs, proofs proofs. \\eps nets, concentration of measure, Rademacher complexities. It is not particularly empirical (unfortunately, most bounds we know how to prove are worst case in some sense or other, hence more or less loose). \n\nOnline learning is at the intersection: proving that the regret of an algorithm (the amount by which it loses to some competitor that knows the right hypothesis in advance) grows less than linearly with the number of predictions it makes. So you are proving that performance is good, and there is often some work to keep things tractable (sometimes this is trivial).\n\nBut just be warned, every type of research has its own kinds of bullshit. Doing good stuff is up to you, and depends to a good extent on choosing the right advisor.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36453, "question": "[Research question] What are some non-empirical and theoretical areas in ML?", "aSentId": 36456, "answer": "Linear models and convex optimization in general have very strong proofs and bounds on behavior. Tensor decompositions are also very mathematically formal if that is what you are into. Optimization theory in general is rich with mathematical rigor.\n\nHowever, if you are turned off by empirical results and research driven by progress on real problems then ML may not be what you want - it is a very applied field which I think is a *good* thing. This is one of the things that attracted me to it, but to to each their own.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36453, "question": "[Research question] What are some non-empirical and theoretical areas in ML?", "aSentId": 36458, "answer": "Not sure if this is what you want, but as a data scientist I would like researchers to do more scientific work, with both empirical and theoretical elements. In particular if you come up with a new method you should try to be a scientist and make a convincing case for why it works well, and how well it generalizes. This could be done by (for example) varying the components in the algorithm and seeing which parts appear crucial, or varying the dataset in a systematic way (not just trying on the 5 ML datasets du jour), or with empirically relevant theoretical work, or most likely a combination of all these and more.\n\nToo little of this is done and the papers that do it often become highly influential.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36461, "question": "How do you initialize your recurrent network weights?", "aSentId": 36462, "answer": "Orthogonal init as covered here http://arxiv.org/abs/1312.6120 works well. It's a bit of a compounding effects kind of thing, though. Just having ortho weights by itself won't be an amazing improvement, but using it with techniques like gradient clipping and a modern optimizer like Adadelta or Adam can result in significant gains in optimization speed.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36461, "question": "How do you initialize your recurrent network weights?", "aSentId": 36464, "answer": "For LSTM networks?  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36465, "question": "For LSTM networks?  ", "aSentId": 36466, "answer": "Sure, that's one kind of recurrent network. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36468, "question": "NVIDIA GPU Tech report (a bit about their CNN engine)", "aSentId": 36469, "answer": "&gt;Based on two Tegra X1 processors,  it can process AlexNet at 184 million frames a second.\n\nDefinitely should be 184 fps; unless they decided to skip a few iterations of Moore's law. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36471, "question": "Clustering Classification With Less Features", "aSentId": 36472, "answer": " I think it might make more sense to train a regression model that can use pre-game information (runes and so on) to try and predict post game information like damage dealt, etc. Then given this prediction, you can put players into the clusters you have created from purely \"post game\" information. In this way it is more of a two stage pipeline.\n\nMost machine learning algorithms assume a consistent feature space across all examples - I am not personally aware of a case where this is not upheld. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36473, "question": " I think it might make more sense to train a regression model that can use pre-game information (runes and so on) to try and predict post game information like damage dealt, etc. Then given this prediction, you can put players into the clusters you have created from purely \"post game\" information. In this way it is more of a two stage pipeline.\n\nMost machine learning algorithms assume a consistent feature space across all examples - I am not personally aware of a case where this is not upheld. ", "aSentId": 36474, "answer": "Great suggestion, really helped clarify some of the problem. After a bit of digging into regression models, it seems that they are typically used to predict a single continuous value (IE: damage dealt). Would it be recommended to train a regression model for every post-game feature (damage dealt, healing given, damage taken, etc.) individually and then use those? Or is there a better regression model that will predict a variety of values?\n\nThanks again for the response, very helpful!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36475, "question": "Great suggestion, really helped clarify some of the problem. After a bit of digging into regression models, it seems that they are typically used to predict a single continuous value (IE: damage dealt). Would it be recommended to train a regression model for every post-game feature (damage dealt, healing given, damage taken, etc.) individually and then use those? Or is there a better regression model that will predict a variety of values?\n\nThanks again for the response, very helpful!", "aSentId": 36476, "answer": "3 separate regression models would be simplest. There are multi-dimensional regression techniques out there, but it would probably be easier to build the pipeline first using 3 separate regressors, then analyze if replacing the regressors with one multi-dimensional model is useful or not.\n\nHappy to help :) Sounds like a cool idea - doubly so if it works!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36478, "question": "Deep Learning Lecture 15: Deep Reinforcement Learning - Policy search", "aSentId": 36479, "answer": "Before I set aside 16 hours to watch this series, can someone who has seen these comment on their usefulness? How deeply(!) are topics covered? How are the slides, and the delivery? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36480, "question": "Before I set aside 16 hours to watch this series, can someone who has seen these comment on their usefulness? How deeply(!) are topics covered? How are the slides, and the delivery? ", "aSentId": 36481, "answer": "Judging from #13, #14, and this one, deeply. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36482, "question": "Judging from #13, #14, and this one, deeply. ", "aSentId": 36483, "answer": "Heh.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36480, "question": "Before I set aside 16 hours to watch this series, can someone who has seen these comment on their usefulness? How deeply(!) are topics covered? How are the slides, and the delivery? ", "aSentId": 36485, "answer": "The first lectures seem to start off like an introduction to neural networks, and then they rapidly pick up pace to interesting current research.\n\nI recommend just picking the videos that look interesting if you are already familiar with NNs and concerned about investing time into it. It's not like you need to watch them in order.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36478, "question": "Deep Learning Lecture 15: Deep Reinforcement Learning - Policy search", "aSentId": 36487, "answer": "love this set! thanks for sharing", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36478, "question": "Deep Learning Lecture 15: Deep Reinforcement Learning - Policy search", "aSentId": 36489, "answer": "can you post 1-&gt;14? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36490, "question": "can you post 1-&gt;14? ", "aSentId": 36491, "answer": "They are linked in the YouTube recommendations. I posted #13 last week - it was also very informative.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36494, "question": "The intrinsic value of chess pieces inferred from an analysis of 4.6 million boards", "aSentId": 36495, "answer": "This is an interesting problem, and I like the initial approach (crafty score chess-board positions over historical data). That being said, there is a lot done incorrectly here.\n\nMy main issue with this article is conflating descriptive and predictive models. If you want to build a model to *determine insights* from your data set, then you want to use traditional statistics (p-values, r^2, confidence intervals, etc...).\n\nConversely, if you want to build a model to *predict* who will win, then you want to focus on training/testing error, bias/variance tradeoffs, and generalizability.\n\nOn the surface, it looks like this is a well-framed descriptive problem. We have a large data set of chess boards, now let's get some insights into piece importance. Then the author uses decision-tree regression (why not other types?) and states that the large r^2 means they \"overfit\" their data. There is *no such thing* as overfitting without evaluating your predictive classifier on a held-out dataset. The definition is literally low training error and high test error. No test error means you can't claim overfitting. Statisticians would be *ecstatic* to have excellent model fits for delivering insights. (Though I will state the author touches upon this by talking about cross-validation later, but their phrasing belies some misunderstanding of these concepts).\n\nSome other nitpicks that I won't go into as deeply:\n\n* Numeric encoding of categorical values for pieces. Namely, the way the pieces are encoded as positions indicate some ranking of board positions. This will definitely trip up regression models. The decision-tree might be taking care of this by treating board position numbers as categorical variables, but I would expect this to be explicitly stated. \n\n* Independence assumption of different board layouts. This isn't the worst thing to do for *predictive* models since classification accuracy is usually improved by more data that violates assumptions. BUT, feature importance analysis is completely skewed by correlated layouts. \n\n* Crafty scores already encode piece weights. You are literally rediscovering their own weights by your regression.\n\n* Missing pieces are not well-handled. Letting the decision-tree intrinsically cover this up sweeps piece importance under the rug.\n\n**Suggestions**\n\nI don't want to shit on somebody putting their work out there, so here are some things I would think *would* be good directions to take this work:\n\n* Encode each piece on the board as binary values alive/dead. The build a classifier to predict win. I skew towards logistic regression, but any one would do.\n\n* See how well correlated crafty scores are with wins. Bin crafty scores by magnitude and see win rates for those bins.\n\n* Make your own classifier to outperform crafty scores. I'm sure they put a lot of work into their algorithm, so it would be awesome to try to beat them.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36496, "question": "This is an interesting problem, and I like the initial approach (crafty score chess-board positions over historical data). That being said, there is a lot done incorrectly here.\n\nMy main issue with this article is conflating descriptive and predictive models. If you want to build a model to *determine insights* from your data set, then you want to use traditional statistics (p-values, r^2, confidence intervals, etc...).\n\nConversely, if you want to build a model to *predict* who will win, then you want to focus on training/testing error, bias/variance tradeoffs, and generalizability.\n\nOn the surface, it looks like this is a well-framed descriptive problem. We have a large data set of chess boards, now let's get some insights into piece importance. Then the author uses decision-tree regression (why not other types?) and states that the large r^2 means they \"overfit\" their data. There is *no such thing* as overfitting without evaluating your predictive classifier on a held-out dataset. The definition is literally low training error and high test error. No test error means you can't claim overfitting. Statisticians would be *ecstatic* to have excellent model fits for delivering insights. (Though I will state the author touches upon this by talking about cross-validation later, but their phrasing belies some misunderstanding of these concepts).\n\nSome other nitpicks that I won't go into as deeply:\n\n* Numeric encoding of categorical values for pieces. Namely, the way the pieces are encoded as positions indicate some ranking of board positions. This will definitely trip up regression models. The decision-tree might be taking care of this by treating board position numbers as categorical variables, but I would expect this to be explicitly stated. \n\n* Independence assumption of different board layouts. This isn't the worst thing to do for *predictive* models since classification accuracy is usually improved by more data that violates assumptions. BUT, feature importance analysis is completely skewed by correlated layouts. \n\n* Crafty scores already encode piece weights. You are literally rediscovering their own weights by your regression.\n\n* Missing pieces are not well-handled. Letting the decision-tree intrinsically cover this up sweeps piece importance under the rug.\n\n**Suggestions**\n\nI don't want to shit on somebody putting their work out there, so here are some things I would think *would* be good directions to take this work:\n\n* Encode each piece on the board as binary values alive/dead. The build a classifier to predict win. I skew towards logistic regression, but any one would do.\n\n* See how well correlated crafty scores are with wins. Bin crafty scores by magnitude and see win rates for those bins.\n\n* Make your own classifier to outperform crafty scores. I'm sure they put a lot of work into their algorithm, so it would be awesome to try to beat them.\n", "aSentId": 36497, "answer": "it's a joke, read the domain name..", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36498, "question": "it's a joke, read the domain name..", "aSentId": 36499, "answer": "well shit... I decide to post one thorough comment a day and start like this &gt;_&lt;", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36501, "question": "Breaking bitcoin mining: Machine learning to rapidly search for the correct bitcoin block header nonce", "aSentId": 36502, "answer": "Not sure if I should upvote for visibility of an example of both bad crypto knowledge and bad ML, or downvote for visibility of an example of both bad crypto knowledge and bad ML.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36503, "question": "Not sure if I should upvote for visibility of an example of both bad crypto knowledge and bad ML, or downvote for visibility of an example of both bad crypto knowledge and bad ML.", "aSentId": 36504, "answer": "Can someone point me in the direction to find out why this is fundamentally not going to work? I have postgrad in maths so don't spare me the details...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36505, "question": "Can someone point me in the direction to find out why this is fundamentally not going to work? I have postgrad in maths so don't spare me the details...", "aSentId": 36506, "answer": "he's splitting the data into test/train after augmentation, so it's horribly overfitting.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36501, "question": "Breaking bitcoin mining: Machine learning to rapidly search for the correct bitcoin block header nonce", "aSentId": 36508, "answer": "Surely if this works it's a reliable attack on sha256 and is probably the most important crypto discovery this decade?\n\nEdit: as /u/j1395010 points out, the blog is a parody, he has some other similarly stupid posts with exactly the same plot.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36509, "question": "Surely if this works it's a reliable attack on sha256 and is probably the most important crypto discovery this decade?\n\nEdit: as /u/j1395010 points out, the blog is a parody, he has some other similarly stupid posts with exactly the same plot.", "aSentId": 36510, "answer": "check out the name of the blog...\n\nhe left the labels on his feature matrix.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36511, "question": "check out the name of the blog...\n\nhe left the labels on his feature matrix.", "aSentId": 36512, "answer": "\"he left the labels on his feature matrix.\"\n\nWhat's wrong with that?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36513, "question": "\"he left the labels on his feature matrix.\"\n\nWhat's wrong with that?", "aSentId": 36514, "answer": "Called \"training on the test data\"", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36513, "question": "\"he left the labels on his feature matrix.\"\n\nWhat's wrong with that?", "aSentId": 36516, "answer": "Basically it's cheating. The algorithm had access to information about the test data that it shouldn't have had access to. So what seemed like magic was likely too good to be true.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36513, "question": "\"he left the labels on his feature matrix.\"\n\nWhat's wrong with that?", "aSentId": 36518, "answer": "sounds like you're his target audience.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36519, "question": "sounds like you're his target audience.", "aSentId": 36520, "answer": "Nope. I'm just new to ML. Can I have a real answer please?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36521, "question": "Nope. I'm just new to ML. Can I have a real answer please?", "aSentId": 36522, "answer": "&gt;Nope. I'm just new to ML. Can I have a real answer please?\n\nA label describes what class a feature vector belongs to (e.g. \"Red fish\", \" Blue fish\"). If you leave these values on your feature matrix, then one input to your model is the very class you're trying to predict... which works well for all labeled data. I hope this answers your question.\n\nEdit: for completeness, I should clarify that this is bad. You will not know the label for data you wish to predict on, so the model is essentially unusable. If you DO have the label for all new data, why would you need to predict it? I hope that this answers your question. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36521, "question": "Nope. I'm just new to ML. Can I have a real answer please?", "aSentId": 36524, "answer": "normally you have a feature matrix X and a label vector y. You want to train a classifier to correctly predict y given X.\n\nHere he's actually training to predict y given X&amp;y. (because he makes a \"careless\" error when separating the two)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36525, "question": "normally you have a feature matrix X and a label vector y. You want to train a classifier to correctly predict y given X.\n\nHere he's actually training to predict y given X&amp;y. (because he makes a \"careless\" error when separating the two)", "aSentId": 36526, "answer": "Thank you :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36509, "question": "Surely if this works it's a reliable attack on sha256 and is probably the most important crypto discovery this decade?\n\nEdit: as /u/j1395010 points out, the blog is a parody, he has some other similarly stupid posts with exactly the same plot.", "aSentId": 36528, "answer": "surely more data for his training set will improve the accuracy of his results!\n\nThen, he can go onto proving/disproving P = NP.\n\nAaaaaaand I'm out.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36509, "question": "Surely if this works it's a reliable attack on sha256 and is probably the most important crypto discovery this decade?\n\nEdit: as /u/j1395010 points out, the blog is a parody, he has some other similarly stupid posts with exactly the same plot.", "aSentId": 36530, "answer": "It does not break the crypto but finding a way to infiltrate it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36531, "question": "It does not break the crypto but finding a way to infiltrate it.", "aSentId": 36532, "answer": "And this is only *if* it works which i doubt.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36531, "question": "It does not break the crypto but finding a way to infiltrate it.", "aSentId": 36534, "answer": "Joke? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36535, "question": "Joke? ", "aSentId": 36536, "answer": "No. Breaking SHA256 would mean than you can compute a privkey out of a pubkey. This is not done here.\n\nWhat's done here (or not done at all) is estimating a nonce.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36537, "question": "No. Breaking SHA256 would mean than you can compute a privkey out of a pubkey. This is not done here.\n\nWhat's done here (or not done at all) is estimating a nonce.", "aSentId": 36538, "answer": "It breaks the hash though, which has nothing to do with private public crypto... \n\nEdit: I checked again: sha is a hash function and is absolutely unrelated to pubpriv...", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36501, "question": "Breaking bitcoin mining: Machine learning to rapidly search for the correct bitcoin block header nonce", "aSentId": 36540, "answer": "someone should post this to /r/BitcoinBeginners for the lulz", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36542, "question": "Do you guys think author if this blog is trolling?", "aSentId": 36543, "answer": "I think he's doing a semi-clever twist on the typical \"intro to ML\" blog.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36545, "question": "**edited**\n\nTo be even more clear:\n\nThis doesn't work because secure hash functions are designed to destroy all statistical relationships between their input bits and their output bits.\n\nThe \"demo\" is broken because:\n\nHe starts with 50000 blockheaders:\n\n    Block 1 \n    Block 2\n    Block 3\n    ...\n\n~~He then takes those 50000 blockheaders and for each he generates 150 random nonces and training labels.  This becomes his new dataset:~~\nHe then takes 10000 of those blockheaders for each he generates 150 random nonces and training labels.  This becomes his new dataset:  \n\n    Block 1 | Random Nonce 1 | False\n    Block 1 | Random Nonce ... | True\n    Block 1 | Random Nonce 150 | False\n    Block 2 | Random Nonce 1 | False\n    ....\n    Block 10000 | Random Nonce 160 | True\n\n~~He then takes the first 10000, which is about 66 individual blockheaders with 150 examples each.~~\nHe now has a training matrix with 1.5mm rows in it.\n\nA randomly selected 33% of those ~~10000~~ 1.5mm data points are then held out from training to test the classifier, meaning that in the training set of ~1mm rows, it would be very hard for there not to be data from the test set for all 10000 block headers.  Even if it misses 10 or 20, the test results will look good.\n\nSince the classifier sees ~~all~~ most of the block headers it will be tested on, along with several examples of a \"random nonce\" which is correlated to the value of the true nonce, it does really well.\n\nIt's a classic error in machine learning.\n\nThat said, the real lesson here is DON'T EVER TRUST PYTHON PICKLES OFF THE INTERNET.  THEY CAN RESULT IN ARBITRARY CODE EXECUTION.  I disassembled this one and it looks safe, but I'm lazy so I may have missed something.\n\n\n", "aSentId": 36546, "answer": "I tested it. It executes as described. The data file has 50000 headers, but he uses only 10000. He takes 10000 headers, generates 150 random nonces with labels for each and then splits the data set. I don't think he uses all the 50000 headers in the code. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36547, "question": "I tested it. It executes as described. The data file has 50000 headers, but he uses only 10000. He takes 10000 headers, generates 150 random nonces with labels for each and then splits the data set. I don't think he uses all the 50000 headers in the code. ", "aSentId": 36548, "answer": "You're right.  He cuts at 10000 before generating the 150 example rows, not after.  I'm more lame than usual today, apparently.\n\nThe point remains though.  There 1.5mm examples in X, if you randomly select and remove 30%, you're going to end up with enough information about enough of the headers in the test set in the training set in order to fool yourself into thinking you're doing well.\n\nWanna see it break?  Easiest way:  (ignore the existing X_test, Y_test)\n\n    t_test=[10001:10501]\n    test_df = pd.DataFrame(make_df(t_test))\n    X_test = test_df.columns[0:148]\n    Y_test = test_df.columns[148]\n\nAnd I'll say it again, unpickling things off the internet using Python is no different from running arbitrary binaries.  It is dangerous.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36549, "question": "You're right.  He cuts at 10000 before generating the 150 example rows, not after.  I'm more lame than usual today, apparently.\n\nThe point remains though.  There 1.5mm examples in X, if you randomly select and remove 30%, you're going to end up with enough information about enough of the headers in the test set in the training set in order to fool yourself into thinking you're doing well.\n\nWanna see it break?  Easiest way:  (ignore the existing X_test, Y_test)\n\n    t_test=[10001:10501]\n    test_df = pd.DataFrame(make_df(t_test))\n    X_test = test_df.columns[0:148]\n    Y_test = test_df.columns[148]\n\nAnd I'll say it again, unpickling things off the internet using Python is no different from running arbitrary binaries.  It is dangerous.", "aSentId": 36550, "answer": "I tried this. The accuracy is 0.75! What is an accuracy that should be taken seriously? Usable for the purpose stated by this guy?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36551, "question": "I tried this. The accuracy is 0.75! What is an accuracy that should be taken seriously? Usable for the purpose stated by this guy?", "aSentId": 36552, "answer": "Well, that's because the test sample size I threw towards you is too small and is biased.  If you try t_test = [10001:] the average error should start to converge to near .5, which means it's no better at telling you which way to look for a nonce than flipping a coin.\n\nThink of it this way, imagine that one of the nonces is right in the middle at 2^31.  You then generate 150 random numbers between 0 and 2^32 -1 and let's say for the sake of argument that those numbers are actually distributed at constant spacing between 0 and 2^32 -1.  Then 75 will be above 2^31 and 75 will be below 2^31.  If your predictor just spits out all zeros, you have .5 accuracy.  Woo!\n\nNow, of course your nonce bounces all over between 0 and 2^32 -1 for each header, and the test values for those 150 \"random nonces\" also move around all over.  So if you don't repeat the experiment enough times, you'll just be seeing noise before convergence.  However, as you add more samples, the accuracy will make it's way right on over to .5.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36553, "question": "Well, that's because the test sample size I threw towards you is too small and is biased.  If you try t_test = [10001:] the average error should start to converge to near .5, which means it's no better at telling you which way to look for a nonce than flipping a coin.\n\nThink of it this way, imagine that one of the nonces is right in the middle at 2^31.  You then generate 150 random numbers between 0 and 2^32 -1 and let's say for the sake of argument that those numbers are actually distributed at constant spacing between 0 and 2^32 -1.  Then 75 will be above 2^31 and 75 will be below 2^31.  If your predictor just spits out all zeros, you have .5 accuracy.  Woo!\n\nNow, of course your nonce bounces all over between 0 and 2^32 -1 for each header, and the test values for those 150 \"random nonces\" also move around all over.  So if you don't repeat the experiment enough times, you'll just be seeing noise before convergence.  However, as you add more samples, the accuracy will make it's way right on over to .5.\n", "aSentId": 36554, "answer": "actually, that's not true. The model is learning something: the distribution of correct nonces, which is not uniform over 0-2^32.\n\nThe model will predict at about 0.77.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36555, "question": "actually, that's not true. The model is learning something: the distribution of correct nonces, which is not uniform over 0-2^32.\n\nThe model will predict at about 0.77.", "aSentId": 36556, "answer": "within sample or out of sample?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36557, "question": "within sample or out of sample?", "aSentId": 36558, "answer": "well, I don't know anything about bitcoin but at least for the data in this pickle it's heavily skewed towards lower values.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36560, "question": "This is confusing prior and posterior probabilities.\n\nIf there is an infinite stream of batches of numbered marbles with each batch having 10000 marbles (e.g. numbered at 0 and going up to 9999) and 1% of the marbles are orange instead of blue with random independent and _equally_ distributed probability, and Alice checks the marbles starting with the lowest number, saving the orange ones and moving onto the next batch as soon as time she finds a marble...  What distribution do you expect for the numbers on the marbles she finds?\n\nIf she only ever checks the first (lowest numbered) 11 marbles from a batch, what numbers do you expect to find it her results?\n", "aSentId": 36561, "answer": "I think it's a joke. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36562, "question": "I think it's a joke. ", "aSentId": 36563, "answer": "Perhaps, its a common misunderstanding. There was someone posting the same 'revelation' to bitcointalk a week ago.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36564, "question": "Perhaps, its a common misunderstanding. There was someone posting the same 'revelation' to bitcointalk a week ago.", "aSentId": 36565, "answer": "As a machine learning practitioner and Bitcoin enthusiast for the life of me I can't understand the experimental setup, which makes me think it's a joke. Also his other blog posts are just as baffling. Meta^meta jokes are a bit much for me to spend too much time on :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36560, "question": "This is confusing prior and posterior probabilities.\n\nIf there is an infinite stream of batches of numbered marbles with each batch having 10000 marbles (e.g. numbered at 0 and going up to 9999) and 1% of the marbles are orange instead of blue with random independent and _equally_ distributed probability, and Alice checks the marbles starting with the lowest number, saving the orange ones and moving onto the next batch as soon as time she finds a marble...  What distribution do you expect for the numbers on the marbles she finds?\n\nIf she only ever checks the first (lowest numbered) 11 marbles from a batch, what numbers do you expect to find it her results?\n", "aSentId": 36567, "answer": "I don't understand. He is sampling from all 4 billion nonces.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36568, "question": "I don't understand. He is sampling from all 4 billion nonces.", "aSentId": 36569, "answer": "No. He's sampling from existing solutions in the blockchain.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36571, "question": "whether or not this is legit ... if you had a way to get a huge advantage in bitcoin mining, why would you write a blog post about it? I mean, I love open source and all, but come on. ", "aSentId": 36572, "answer": "Thinking the same.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36571, "question": "whether or not this is legit ... if you had a way to get a huge advantage in bitcoin mining, why would you write a blog post about it? I mean, I love open source and all, but come on. ", "aSentId": 36574, "answer": "Some people just want to watch the world... learn?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36575, "question": "Some people just want to watch the world... learn?", "aSentId": 36576, "answer": "Learn from their mistakes of course! ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36578, "question": "Given collection of articles, how to do binary classification: \"majority topic\" or not?", "aSentId": 36579, "answer": "Why not fit a document classifier at the article level and then use that classifier for p(class | features, weights) to estimate the probability that the document is the majority class.  In other words, it seems like you can break the machine learning problem and  the majority vs. not majority problem apart.  \n\nProbably the easiest thing to do is to just get k samples from p(class | features, weights) for each article and then, for each sample, figure out which articles had the majority class.  Then combine the results over the k samples to estimate p(majority class | features, weight) for each article.  I bet that wouldn't even take that long.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36578, "question": "Given collection of articles, how to do binary classification: \"majority topic\" or not?", "aSentId": 36581, "answer": "look into one vs one &amp; one vs all.   ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36584, "question": "Is real time facial recogniton possible?", "aSentId": 36585, "answer": "It's certainly possible to do it in real-time! Here's Google's recent paper which has some very nice experiments detailing the model's speed vs performance trade-off: http://arxiv.org/abs/1503.03832\n\nI'm not aware of any off the shelf systems to just pick up and use, unfortunately.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36586, "question": "It's certainly possible to do it in real-time! Here's Google's recent paper which has some very nice experiments detailing the model's speed vs performance trade-off: http://arxiv.org/abs/1503.03832\n\nI'm not aware of any off the shelf systems to just pick up and use, unfortunately.", "aSentId": 36587, "answer": "Thta paper looks interesting, I'lll check it out. Thank you!\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36588, "question": "Thta paper looks interesting, I'lll check it out. Thank you!\n\n", "aSentId": 36589, "answer": "There's also /r/computervision, if you haven't already come across it yet.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36584, "question": "Is real time facial recogniton possible?", "aSentId": 36591, "answer": "Depends on what exactly you are trying to do. You will certainly need a model to perform recognition,  and you could certainly augment that model at runtime. If you have a pre-trained model then doing the recognition itself is usually pretty fast. I did a very quick Google and found [this video](https://youtu.be/yFSPnu6_TZY) which might be something to follow. Note that these techniques are not state of the art, but will probably work for a simple scenario. I saw Googles recent paper mentioned, but you are unlikely able to replicate that unless you have a large cluster,  a few million training images and very extensive knowledge of deep learning. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36593, "question": "LSTM: A Search Space Odyssey | Comparison of LSTM variants", "aSentId": 36594, "answer": "Am I the only one whose eyes glaze over once you get to the LSTM graphic? There's so many directed arrows of different kinds I can't ever make heads or tails of it.  \n\nPerhaps a simple graphical input example would help.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36595, "question": "Am I the only one whose eyes glaze over once you get to the LSTM graphic? There's so many directed arrows of different kinds I can't ever make heads or tails of it.  \n\nPerhaps a simple graphical input example would help.", "aSentId": 36596, "answer": "I had the same reaction on my first encounter with an LSTM. But I think this is actually one of the nicest pictures of an LSTM that I've seen so far.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36593, "question": "LSTM: A Search Space Odyssey | Comparison of LSTM variants", "aSentId": 36598, "answer": "The impact of this paper on my personal research direction will be huge.\n\nMy current implementation is that of vanilla LSTM: input gates, output gates, forget gates. No peepholes. \n\nFor quite a while I have played with the idea of going for GRUs and peepholes. Now I can somewhat put these aside and focus on more pressing things.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36599, "question": "The impact of this paper on my personal research direction will be huge.\n\nMy current implementation is that of vanilla LSTM: input gates, output gates, forget gates. No peepholes. \n\nFor quite a while I have played with the idea of going for GRUs and peepholes. Now I can somewhat put these aside and focus on more pressing things.", "aSentId": 36600, "answer": "Second author here.\n\nI'm glad it's helpful. Please note that our vanilla LSTM does include peepholes, so your current implementation is the NP variant. Our results do suggest that you can safely move on to more pressing things without a statistically significant loss* :)\n\n*Unless you have a very different domain/dataset", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36593, "question": "LSTM: A Search Space Odyssey | Comparison of LSTM variants", "aSentId": 36602, "answer": "It doesn't seem like they use minibatches when training in their experiments. That seems pretty non-standard and I would expect their gradients to be extremely noisy. Their comment about momentum doesn't make sense:\n&gt; It may, however,\nbe more important in the case of batch training, where the\ngradients are less noisy.\n\nMomentum should work better when the gradient is more noisy. Momentum is like minibatches through time, it should help cancel a lot of the oscillations in gradient descent.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36603, "question": "It doesn't seem like they use minibatches when training in their experiments. That seems pretty non-standard and I would expect their gradients to be extremely noisy. Their comment about momentum doesn't make sense:\n&gt; It may, however,\nbe more important in the case of batch training, where the\ngradients are less noisy.\n\nMomentum should work better when the gradient is more noisy. Momentum is like minibatches through time, it should help cancel a lot of the oscillations in gradient descent.", "aSentId": 36604, "answer": "It's a bit counter-intuitive, but I think it's the other way around. Momentum is used to fight pathological curvature (and the oscillations that are caused by it) Smaller learning rates, OTOH, can be used to fight that and the stochasticity. The smaller the mini-batch size, the smaller the learning rate you should choose, lessening the benefit of momentum, and other advanced methods.\n\nConsider full batch learning: no stochasticity, but still pathological curvature. You'd still want to use momentum, if not NCG/BFGS. Think about it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36605, "question": "It's a bit counter-intuitive, but I think it's the other way around. Momentum is used to fight pathological curvature (and the oscillations that are caused by it) Smaller learning rates, OTOH, can be used to fight that and the stochasticity. The smaller the mini-batch size, the smaller the learning rate you should choose, lessening the benefit of momentum, and other advanced methods.\n\nConsider full batch learning: no stochasticity, but still pathological curvature. You'd still want to use momentum, if not NCG/BFGS. Think about it.", "aSentId": 36606, "answer": "For full batch you want to use RPROP. Beats everything in my experience. :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36607, "question": "For full batch you want to use RPROP. Beats everything in my experience. :)", "aSentId": 36608, "answer": "I've found levenberg-marquardt(when tractible, at least in single hidden layer nets) to be best. RPROP 2nd.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36607, "question": "For full batch you want to use RPROP. Beats everything in my experience. :)", "aSentId": 36610, "answer": "Is there any kind of online/SGD version of RPROP? It's so simple and fast and gets rid of the learning rate hyperparameters, but it sucks having to get accurate gradients for it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36611, "question": "Is there any kind of online/SGD version of RPROP? It's so simple and fast and gets rid of the learning rate hyperparameters, but it sucks having to get accurate gradients for it.", "aSentId": 36612, "answer": "The online version of RProp is called RMSProp, but it's not exactly free of hyperparameters.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36613, "question": "The online version of RProp is called RMSProp, but it's not exactly free of hyperparameters.", "aSentId": 36614, "answer": "How sure are you that this actually is the same?\n\nRPROP has learning rates that grow/shrink exponentially if the sign of the gradient stays constant/fluctuates. I don't see how this maps exactly to RMSPROP.\n\nI agree that several ideas are involved. I also think that Adam is supposed to be equivalent to RPROP for some corner case.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36615, "question": "How sure are you that this actually is the same?\n\nRPROP has learning rates that grow/shrink exponentially if the sign of the gradient stays constant/fluctuates. I don't see how this maps exactly to RMSPROP.\n\nI agree that several ideas are involved. I also think that Adam is supposed to be equivalent to RPROP for some corner case.", "aSentId": 36616, "answer": "It's not the same, of course. However, the motivation for the development of RMSProp, according to Hinton, was to have an \"online RProp\".", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36603, "question": "It doesn't seem like they use minibatches when training in their experiments. That seems pretty non-standard and I would expect their gradients to be extremely noisy. Their comment about momentum doesn't make sense:\n&gt; It may, however,\nbe more important in the case of batch training, where the\ngradients are less noisy.\n\nMomentum should work better when the gradient is more noisy. Momentum is like minibatches through time, it should help cancel a lot of the oscillations in gradient descent.", "aSentId": 36618, "answer": "&gt; It doesn't seem like they use minibatches when training in their experiments. That seems pretty non-standard and I would expect their gradients to be extremely noisy.\n\nI think that most (if not all) of LSTM's successes before 2014 were achieved without mini batches. (That's because Alex Graves did not use them, if I am right.)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36603, "question": "It doesn't seem like they use minibatches when training in their experiments. That seems pretty non-standard and I would expect their gradients to be extremely noisy. Their comment about momentum doesn't make sense:\n&gt; It may, however,\nbe more important in the case of batch training, where the\ngradients are less noisy.\n\nMomentum should work better when the gradient is more noisy. Momentum is like minibatches through time, it should help cancel a lot of the oscillations in gradient descent.", "aSentId": 36620, "answer": "I would argue that minibatches are not super standard in RNNs - if you read the papers of anyone else from that particular lab (including Alex Graves) they almost always do updates every sample. *Technically* the guarantees of SGD -as small as they- are only given with one sample. Minibatch kind of sits in this strange realm between batch gradient and stochastic gradient but it works great in practice. It also gives large computational speedups in practice, though not always speedups in *convergence rate* w.r.t wall clock time. Interestingly, it seems like around the time Alex went to DeepMind he started using minibatches - coincidence or something more?\n\nMomentum works better when the gradient is smooth - the point is to gain speed/velocity/whatever to push through saddle points and local minima where a smoother gradient would effectively be stuck. I would agree with the authors of the paper here.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 36621, "question": "I would argue that minibatches are not super standard in RNNs - if you read the papers of anyone else from that particular lab (including Alex Graves) they almost always do updates every sample. *Technically* the guarantees of SGD -as small as they- are only given with one sample. Minibatch kind of sits in this strange realm between batch gradient and stochastic gradient but it works great in practice. It also gives large computational speedups in practice, though not always speedups in *convergence rate* w.r.t wall clock time. Interestingly, it seems like around the time Alex went to DeepMind he started using minibatches - coincidence or something more?\n\nMomentum works better when the gradient is smooth - the point is to gain speed/velocity/whatever to push through saddle points and local minima where a smoother gradient would effectively be stuck. I would agree with the authors of the paper here.", "aSentId": 36622, "answer": "Eh, I think that minibatches are probably not worth it when using CPUs.  Alex Graves's handwriting code worked on CPU.  \n\nHe probably switched to GPUs when he went to Google.  ", "corpus": "reddit"}]