[{"docID": "t5_2r3gv", "qSentId": 37805, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 37806, "answer": "You again!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37805, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 37808, "answer": "All I can say is _wow_! This shall be interesting.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37805, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 37810, "answer": "You are a god.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37815, "question": "Automated Image Captioning - Andrej Karpathy", "aSentId": 37816, "answer": "Are these from his stanford Convnet lectures? If so, do you know where I can find the rest of them? Love it, thanks for the upload. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37817, "question": "Are these from his stanford Convnet lectures? If so, do you know where I can find the rest of them? Love it, thanks for the upload. ", "aSentId": 37818, "answer": "I recall him saying on a tweet somewhere they had trouble recording videos for the Stanford class unfortunately.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37820, "question": "AskML: Where do you discuss new arXiv papers?", "aSentId": 37821, "answer": "Here, but don't get discouraged if not many people show interest. Some of your audience is really busy, others can't/won't put in the effort to review the paper and others don't get it or are just enthusiasts.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37823, "question": "Navigating the Machine Learning job market", "aSentId": 37824, "answer": "In my experience, most companies with some \"data\" want a \"data scientist\" that specializes in ML, they just dont know why.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37825, "question": "In my experience, most companies with some \"data\" want a \"data scientist\" that specializes in ML, they just dont know why.", "aSentId": 37826, "answer": "a long time ago, i was headhunted by Taco Bell for one such position. They wanted to model their data in visual basic and excel. \n\nI've never seen anything like that before that or since then. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37827, "question": "a long time ago, i was headhunted by Taco Bell for one such position. They wanted to model their data in visual basic and excel. \n\nI've never seen anything like that before that or since then. ", "aSentId": 37828, "answer": "Excel is used by a lot of finance people to do financial models, and if you want to script Excel, you're probably doing it in VBA. (That said there's a firm providing the ability to script excel with python - which is just brilliant, not that I'd encourage the use of excel.)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37829, "question": "Excel is used by a lot of finance people to do financial models, and if you want to script Excel, you're probably doing it in VBA. (That said there's a firm providing the ability to script excel with python - which is just brilliant, not that I'd encourage the use of excel.)", "aSentId": 37830, "answer": "i get that. kind of surprised i have never seen that kind of stuff in this line of work at all except that one time.... and since that kind of a position requires skills very different from someone with my profile, i was surprised they actually tried headhunting me. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37832, "question": "Do all credit card accounts eventually die from fraud?", "aSentId": 37833, "answer": "It'll be an exponential model, and the chance will be fixed across lifetime, which sucks.\n\nMaybe there's a small effect whereby crims don't bother with accounts with small balances, and the chance for new accounts is very low. Maybe.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37836, "question": "Learning Stochastic Recurrent Networks (ICLR 2015)", "aSentId": 37837, "answer": "The author is a pretty active redditor on /r/ML, and this paper will probably get accepted, so I figured he deserves to have his 1 day of fame here :-)\n\n**My question:**\n\nThis is more about the parent work, and I know almost nothing about Variational Bayes: Is there any reason to use SGVB AE over the usual denoising AE in *e.g.* semi-supervised learning? In other words, why should I care about the whole variational business? (Sorry if this is silly)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37838, "question": "The author is a pretty active redditor on /r/ML, and this paper will probably get accepted, so I figured he deserves to have his 1 day of fame here :-)\n\n**My question:**\n\nThis is more about the parent work, and I know almost nothing about Variational Bayes: Is there any reason to use SGVB AE over the usual denoising AE in *e.g.* semi-supervised learning? In other words, why should I care about the whole variational business? (Sorry if this is silly)", "aSentId": 37839, "answer": "There are several advantages:\n\n - you have a proper objective function, i.e. derived from first principles and not a heuristic,\n - estimation of the loglikelihood is extremly straightforward and well functioning (important e.g. for outlier detection or generative modelling)\n - the latent units have a probabilistic interpretation (opens up all kinds of inference),\n - no need to tune regularisation constants or noise levels,\n - efficient sampling.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37840, "question": "There are several advantages:\n\n - you have a proper objective function, i.e. derived from first principles and not a heuristic,\n - estimation of the loglikelihood is extremly straightforward and well functioning (important e.g. for outlier detection or generative modelling)\n - the latent units have a probabilistic interpretation (opens up all kinds of inference),\n - no need to tune regularisation constants or noise levels,\n - efficient sampling.\n", "aSentId": 37841, "answer": "&gt; you have a proper objective function, i.e. derived from first principles and not a heuristic,\n\nI thought they linked denoising and the free energy (the former is the gradient of the latter, if I remember correctly) Is this not principled enough?\n\n&gt; no need to tune regularisation constants or noise levels,\n\nVariational Bayes doesn't overfit?\n\n&gt; efficient sampling\n\n[Deep Generative Stochastic Networks Trainable by Backprop](http://arxiv.org/abs/1306.1091) seems to talk about a trade-off between being able to model multi-modality and the ease of sampling. Do the SGVB approaches let you have your cake and eat it too (model complex distributions, and yet sample from them easily)?\n\n&gt; There are several advantages\n\nAre there disadvantages (compared to denoising / GSN)? \n\n\nThanks!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37842, "question": "&gt; you have a proper objective function, i.e. derived from first principles and not a heuristic,\n\nI thought they linked denoising and the free energy (the former is the gradient of the latter, if I remember correctly) Is this not principled enough?\n\n&gt; no need to tune regularisation constants or noise levels,\n\nVariational Bayes doesn't overfit?\n\n&gt; efficient sampling\n\n[Deep Generative Stochastic Networks Trainable by Backprop](http://arxiv.org/abs/1306.1091) seems to talk about a trade-off between being able to model multi-modality and the ease of sampling. Do the SGVB approaches let you have your cake and eat it too (model complex distributions, and yet sample from them easily)?\n\n&gt; There are several advantages\n\nAre there disadvantages (compared to denoising / GSN)? \n\n\nThanks!", "aSentId": 37843, "answer": "IIRC the correspondence to a probabilistic model for DAE was only shown in a few cases, but I might be incorrect. Anyway, several features such as getting the likelihood and certain kinds of inference (missing value imputation, denoising, sampling) seem more involved than in the case of the VAE.\n\nThe question whether VI can overfit in general is over my head. The whole coding argument seems to be a good inductive bias overall. After all, it seems to be a very good regulariser with no hyper parameters.\n\nThe main disadvantage of learning with SGVB is probably that the optimisation is difficult. My hypothesis is that the distribution of the gradients gets the nastier the more complex the model is. In the end, is usually not stopped because it converges, but because it starts to oscillate wildly around the value it should converge to. In the beginning, learning is as smooth as for normal neural nets.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37844, "question": "IIRC the correspondence to a probabilistic model for DAE was only shown in a few cases, but I might be incorrect. Anyway, several features such as getting the likelihood and certain kinds of inference (missing value imputation, denoising, sampling) seem more involved than in the case of the VAE.\n\nThe question whether VI can overfit in general is over my head. The whole coding argument seems to be a good inductive bias overall. After all, it seems to be a very good regulariser with no hyper parameters.\n\nThe main disadvantage of learning with SGVB is probably that the optimisation is difficult. My hypothesis is that the distribution of the gradients gets the nastier the more complex the model is. In the end, is usually not stopped because it converges, but because it starts to oscillate wildly around the value it should converge to. In the beginning, learning is as smooth as for normal neural nets.\n\n", "aSentId": 37845, "answer": "&gt; IIRC the correspondence to a probabilistic model for DAE was only shown in a few cases, but I might be incorrect.\n\nNo, you're correct. And my impression is that the cases where it does hold are not the ones that work in practice.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37844, "question": "IIRC the correspondence to a probabilistic model for DAE was only shown in a few cases, but I might be incorrect. Anyway, several features such as getting the likelihood and certain kinds of inference (missing value imputation, denoising, sampling) seem more involved than in the case of the VAE.\n\nThe question whether VI can overfit in general is over my head. The whole coding argument seems to be a good inductive bias overall. After all, it seems to be a very good regulariser with no hyper parameters.\n\nThe main disadvantage of learning with SGVB is probably that the optimisation is difficult. My hypothesis is that the distribution of the gradients gets the nastier the more complex the model is. In the end, is usually not stopped because it converges, but because it starts to oscillate wildly around the value it should converge to. In the beginning, learning is as smooth as for normal neural nets.\n\n", "aSentId": 37847, "answer": "Do you think this difficulty is also compounded by the sampling aspect? The additional cost terms that VAE adds don't seem particularly ugly, but I can see the sampling aspect of it causing a mess of convergence issues for gradient descent.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37836, "question": "Learning Stochastic Recurrent Networks (ICLR 2015)", "aSentId": 37849, "answer": "Anyone seen code for this? I am going through both DRAW and STORN at the moment and having code for at least this one would be nice :) . Really cool paper - I am surprised it snuck by me til now!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37851, "question": "Heisenberg 0.5 - An AI powered engine to evaluate your startup idea.", "aSentId": 37852, "answer": "Aren't you gonna steal my idea?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37853, "question": "Aren't you gonna steal my idea?", "aSentId": 37854, "answer": "That's a great idea. It's mine now.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37853, "question": "Aren't you gonna steal my idea?", "aSentId": 37856, "answer": "If you have an idea, chances are other people might have had that idea before you already. What matters is the execution of said idea.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37857, "question": "If you have an idea, chances are other people might have had that idea before you already. What matters is the execution of said idea.", "aSentId": 37858, "answer": "That's true.  But that said there's no need to broadcast your startup idea to be analyzed somehow by an AI engine based on tweets, blog posts, and lists of existing startups.  \n\nYou need to sit down and do *actual market research*, formulate a business plan, design a mockup/prototype, etc.  Then when you are ready and looking for venture capital, co-founders, or users, you share your idea as necessary after having that lead-in period.  But you don't share your clever nascent idea to anonymous strangers at the start of the planning phase, and let someone else try executing the idea better initially.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37851, "question": "Heisenberg 0.5 - An AI powered engine to evaluate your startup idea.", "aSentId": 37860, "answer": "So I sent an idea in right around the moment this was posted and haven't received anything in the 10 hours or so I've waited. I'm warning you guys now, I've seen stuff like this before and It's mostly just to farm ideas. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37861, "question": "So I sent an idea in right around the moment this was posted and haven't received anything in the 10 hours or so I've waited. I'm warning you guys now, I've seen stuff like this before and It's mostly just to farm ideas. ", "aSentId": 37862, "answer": "a@b.c\n\nmy idea was d\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37851, "question": "Heisenberg 0.5 - An AI powered engine to evaluate your startup idea.", "aSentId": 37864, "answer": "I'm skeptical. I don't think you have any tools that can actually ascertain the things you claim.\n\nHow are you generating the \"probabilistic need of target audience for the product\" for instance? My guess is that you have pretty much no data to back this up. Maybe you try to match for tweets containing similar words. But what evidence do you have that this even correlates at all with the need of a target audience? I'm guessing zero.\n\nWhile I'd not be worried about you \"stealing ideas\" because ideas are super cheap, I doubt this adds any value to the startup brainstorming process.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37866, "question": "I submitted my idea about a blog on Artificial Intelligence like 3h ago, and no feedback email yet...", "aSentId": 37867, "answer": "The system is very loaded and few things have broken. We are working on it. The traffic is just massive.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37870, "question": "I'll be honest in that the feedback received was likely accurate, but IMHO too brief. I understand the complexity of what you're doing, though, so it's not that off-putting. I'm not sure this has true real-world use, but it's a great academic exercise. Cheers!", "aSentId": 37871, "answer": "Thank you for the feedback.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37872, "question": "Thank you for the feedback.", "aSentId": 37873, "answer": "Not at all. I plan on submitting a few more things out of curiosity, see what it says. Nice work!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37876, "question": "Let us know if you have some queries regarding it. Follow us on twitter @theziraffes", "aSentId": 37877, "answer": "Can you tell us briefly how it works?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37878, "question": "Can you tell us briefly how it works?", "aSentId": 37879, "answer": "Yes. People are talking about ideas all across the web and there is a certain pattern that we have figured out in the talks. We are constantly collecting such information by scraping and trying to organised everything and bring a relation among them. So, when you talk about an idea, we look for content related to it and figure out what views people have about it and send you a result based on it. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37880, "question": "Yes. People are talking about ideas all across the web and there is a certain pattern that we have figured out in the talks. We are constantly collecting such information by scraping and trying to organised everything and bring a relation among them. So, when you talk about an idea, we look for content related to it and figure out what views people have about it and send you a result based on it. ", "aSentId": 37881, "answer": "So the more novel the idea, the less related samples there would be.. surely the reliability of the result would decrease with novelty?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37882, "question": "So the more novel the idea, the less related samples there would be.. surely the reliability of the result would decrease with novelty?", "aSentId": 37883, "answer": "Right. Currently no ML algorithm can draw on statistical methods and data to evaluate new ideas this complex. It requires reasoning and expressivity above what ML can currently do. But this approach can theoretically find similarities between your idea and previous ideas that have failed or succeeded and let you make your own decision, which could be quite useful. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37884, "question": "Right. Currently no ML algorithm can draw on statistical methods and data to evaluate new ideas this complex. It requires reasoning and expressivity above what ML can currently do. But this approach can theoretically find similarities between your idea and previous ideas that have failed or succeeded and let you make your own decision, which could be quite useful. ", "aSentId": 37885, "answer": "Try it. We have developed one of the best AI currently available. Although the results are brief and like you said the it presents similarity with ideas. We don't give a measure of success though, that is truly complex and impossible currently.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37887, "question": "What are some ML techniques that are being applied to data from sensor networks, IoT?", "aSentId": 37888, "answer": "Compressive Sensing is a really interesting field.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37890, "question": "What interests Reddit? A network analysis of 84M comments by 200K users", "aSentId": 37891, "answer": "I'm certainly inexperienced and maybe lack creativity, but what if any conclusions can we draw from this? It's attractive and amusing but it also seems like a very messy bundle of words I already associate with each other.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37890, "question": "What interests Reddit? A network analysis of 84M comments by 200K users", "aSentId": 37893, "answer": "1} that's a very small dictionary.  When you're looking at highly heterogeneous content, the rare words define the content. There's really no need to pare it down so much. \n\n2) The goal seems to be to construct \"topics\", but he never explicitly says this.  There are many approaches to do this that he doesn't seem aware of - his network approach though is frankly strange.  How does this approach compare to LDA?  How about using community detection methods on the whole word co-mention network? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37890, "question": "What interests Reddit? A network analysis of 84M comments by 200K users", "aSentId": 37895, "answer": "How long did it take you to collect 84M comments from 200,000 redditors? Do you need some sort of distributed processing to do that? Do you care to share your script?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37896, "question": "How long did it take you to collect 84M comments from 200,000 redditors? Do you need some sort of distributed processing to do that? Do you care to share your script?", "aSentId": 37897, "answer": "Google PRAW. I've mined several million comments using it, once, and it took me a day per million comments, IIRC.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37905, "question": "Facebook AI Director Yann LeCun on His Quest to Unleash Deep Learning and Make Machines Smarter", "aSentId": 37906, "answer": "Lol\n\n&gt; \"The current excitement about AI stems, in great part, from groundbreaking advances involving what are known as 'convolutional neural networks.' This machine learning technique promises dramatic improvements in things like computer vision, speech recognition, and natural language processing. You probably have heard of it by its more layperson-friendly name: 'Deep Learning.'\"\n\nApparently Deep Learning = Convolutional Neural Networks\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37907, "question": "Lol\n\n&gt; \"The current excitement about AI stems, in great part, from groundbreaking advances involving what are known as 'convolutional neural networks.' This machine learning technique promises dramatic improvements in things like computer vision, speech recognition, and natural language processing. You probably have heard of it by its more layperson-friendly name: 'Deep Learning.'\"\n\nApparently Deep Learning = Convolutional Neural Networks\n\n", "aSentId": 37908, "answer": "How on earth did you come to that conclusion. Yann certainly wasn't saying this.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37910, "question": "Sentiment Analysis wants to make its debut in consumer mobile apps", "aSentId": 37911, "answer": "Any tips wrt the sentiment analysis technique that runs on the mobile? (I wonder if more readers of \"/r/machinelearning\" are interested in this)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37913, "question": "Dato Open Sources Core of GraphLab ML Library", "aSentId": 37914, "answer": "AGPL, wow. If I am correct, you have to open your code even if you provide it only through a service.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37916, "question": "Text Analysis using Recurrent Neural Networks", "aSentId": 37917, "answer": "Great video lesson. I can see how RNN can be very powerful. But something bothers me. I understand why they work so well with text sequences because the data is always inputted one sequence at a time. But what if you wanted an RNN to learn musical data with multiple overlapping sequences (of notes/sounds) running concurrently? How does that work?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37918, "question": "Great video lesson. I can see how RNN can be very powerful. But something bothers me. I understand why they work so well with text sequences because the data is always inputted one sequence at a time. But what if you wanted an RNN to learn musical data with multiple overlapping sequences (of notes/sounds) running concurrently? How does that work?", "aSentId": 37919, "answer": "There are several options. One is to treat each note as a feature, so at each timestep you have a multi-hot feature vector on the output (or an embedding of this). Another is to have sequence alignment, and basically learn to map a chordal representation to the text \"C\" (or CCCCCCCCCCCCCCCCCC) as the case may be. In general, most real problems including text have this issue of alignment interwoven with the learning problem.\n\nFor an introductory example of RNNs for music (MIDI in this case) check out http://deeplearning.net/tutorial/rnnrbm.html", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37920, "question": "There are several options. One is to treat each note as a feature, so at each timestep you have a multi-hot feature vector on the output (or an embedding of this). Another is to have sequence alignment, and basically learn to map a chordal representation to the text \"C\" (or CCCCCCCCCCCCCCCCCC) as the case may be. In general, most real problems including text have this issue of alignment interwoven with the learning problem.\n\nFor an introductory example of RNNs for music (MIDI in this case) check out http://deeplearning.net/tutorial/rnnrbm.html", "aSentId": 37921, "answer": "Thanks for the reply. I'm still having trouble with it but, hopefully, after going over the tutorial, it will become clearer.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37926, "question": "Introduction to Deep Learning with Python", "aSentId": 37927, "answer": "Thanks!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37926, "question": "Introduction to Deep Learning with Python", "aSentId": 37929, "answer": "awesome! Thanks", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37931, "question": "Intro to Neural Networks, taught by creator of Metacademy", "aSentId": 37932, "answer": "Its not possible to access course videos", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37934, "question": "Sentiment Analysis Over My Personal Email Corpus", "aSentId": 37935, "answer": "How did you define happiness for your classifier? I recently did emotion detection for a 24hout project but I found classifying such emotions extremely difficult. In the end used data from my own surveys that I thought best fit what I thought would represent certain types of emotion or feeling, but my survey pool was small. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37937, "question": "How to calculate Kullback-Leibner divergence when both distribution P and Q contain zero-probable elements?", "aSentId": 37938, "answer": "Strictly speaking, KL-divergence is only really defined when supp(P) is a subset of supp(Q) (ie: for all x such that P(x) is non-zero, Q(x) is also non-zero), which is where you're problem is coming from, and why it's not really addressed in the literature. Everywhere that people use KL-divergence it is assumed that this property holds.\nTo combat the problem, you could just place a prior distribution over the letter probabilities (which I suppose using Laplace smoothing is a special case of) and make an MAP estimate of the letter frequencies as opposed to your current Maximum Likelihood strategy. As to the choice of prior, a uniform one would be the obvious option, but perhaps you could get a better one by looking at letter frequencies in English or something? (Whether that's reasonable or not will depend on the text that you're working with of course)\nOn the slightly odd fact that P is allowed to have zeros and Q is not, I'm not aware of a particularly intuitive argument for why this should be the case (other than just to look at the definition and see that you're dividing by zero), but you actually have to make some (admittedly fairly reasonable) assumptions about how certain quantities behave to allow P to have zeros. For example, if you have x such that P(x) = 0 and Q(x) \\neq 0, then you need to compute 0 * log(0). log(0) isn't really defined, so we have to define 0 * log(0) = 0. A similar thing is the case when P(x) = Q(x) = 0 as well because 0 / 0 isn't really defined either. So if you didn't make these assumptions then KL-divergence wouldn't work for un-supported regions of P either. My point is that you should perhaps also be surprised by the fact that P is allowed to contain zeros, so Q not being allowed to have zeros is less of a jump.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37937, "question": "How to calculate Kullback-Leibner divergence when both distribution P and Q contain zero-probable elements?", "aSentId": 37940, "answer": "Look at the probability mass in the symmetric difference of the supports. As long as this is negligible compared to the mass in the intersection (computed either from the P or the Q) you may actually neglect it, computing the divergence by summing only where you can. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37937, "question": "How to calculate Kullback-Leibner divergence when both distribution P and Q contain zero-probable elements?", "aSentId": 37942, "answer": "According to Information Theory, the KL-divergence is the amount of extra computer bits (information) you would need to encode some data using the wrong distribution Q instead of the true distribution P. Nothing bad happens if a letter in Q is missing from P---it just means you are wasting bits, which will be reflected by your score for the KL-divergence. But if a letter in P does not appear in Q then all the bits in the universe won't be enough to encode the data---you are trying to encode apples using oranges.\n\nEdit&gt; It seems to me that you are using the wrong measure. Perhaps what you want is mutual information, or even conditional entropy? Look 'em up.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37945, "question": "Machine Learning Applied to Multiple Choice Questions", "aSentId": 37946, "answer": "Hi. This is what Oren Etzioni's Allen Institute for Artificial Intelligence  is working on. Last I heard they were working on 3rd and 8th grade science tests.\n\n\nhttp://allenai.org/", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37945, "question": "Machine Learning Applied to Multiple Choice Questions", "aSentId": 37948, "answer": "Just taking a stab here. I'm by no means an expert. But I'd start by looking for common themes (\"all of the above\",  longer answers, position in the order presented, maybe domain specific stuff like number of operators if you're looking at physics) and rank each answer based on those qualities, normalized to be between 1 and -1 let's say (so \"all of the above\" would be close to 1 for that category). For each category feed the question into a NN with that number of input vectors. Number of hidden layers and size would have to be experimented with. I'd end with an output layer of 1, with the highest scoring option being the most likely answer. Train it on test data. \n\nThe hard part would be ingesting the question itself. Some phrases like \"which of the following...\" might make an \"all of the above\" more likely, while \"which best describes...\" would give that answer a low score. Parsing the question would be the tough part. I'd first try various categories of questions as additional input nodes. Maybe have someone completely unfamiliar look at a sample question and guess the correct answer based only on language used, explaining their thinking along the way to know how to categorize questions. It's not much but that's where id start. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 37950, "question": "[Question] What does joint sample state x1 from the multivariate distribution p(x) mean?", "aSentId": 37951, "answer": "Well, suppose X is a vector in a 2-dimensional space and p(X) is a multivariate normal distribution.  \n\nThen the sample state x1 is a point in 2-dimensional space that was sampled from p(X).  You are right that p(X) is no longer specified by a table, though you could discretize the space and approximately represent p(X) with a table.  Then the sample x1 would indeed be an element from the table.  \n\nIt might be worth looking at some examples of 2D multivariate normal distributions to get a better intuition.  ", "corpus": "reddit"}]