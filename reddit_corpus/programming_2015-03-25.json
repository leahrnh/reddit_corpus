[{"docID": "t5_2fwo", "qSentId": 57255, "question": "x86 is a high-level language", "aSentId": 57256, "answer": "I think \"x86 is a virtual machine\" might be more accurate. It's still a machine language, just the machine is abstracted on the cpu.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57257, "question": "I think \"x86 is a virtual machine\" might be more accurate. It's still a machine language, just the machine is abstracted on the cpu.", "aSentId": 57258, "answer": "Totally.  What a weird high-level language though!  How would you design an instruction set architecture nowadays if you got to start from scratch?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57259, "question": "Totally.  What a weird high-level language though!  How would you design an instruction set architecture nowadays if you got to start from scratch?", "aSentId": 57260, "answer": "ARM is actually pretty close to an answer to your question. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57261, "question": "ARM is actually pretty close to an answer to your question. ", "aSentId": 57262, "answer": "No, I'd want register windows.  The original design from the Berkeley RISC 1 wastes registers, but AMD fixed that in their Am29000 chips by letting programs only shift by as many registers as they actually need.\n\nUnfortunately, AMD couldn't afford to support that architecture, because they needed all the engineers to work on x86.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57263, "question": "No, I'd want register windows.  The original design from the Berkeley RISC 1 wastes registers, but AMD fixed that in their Am29000 chips by letting programs only shift by as many registers as they actually need.\n\nUnfortunately, AMD couldn't afford to support that architecture, because they needed all the engineers to work on x86.", "aSentId": 57264, "answer": "Are register windows necessarily part of the instruction set language? It seems that they are an abstraction much like x86's register renaming.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57265, "question": "Are register windows necessarily part of the instruction set language? It seems that they are an abstraction much like x86's register renaming.", "aSentId": 57266, "answer": "You know, they used to be, but maybe not anymore.  Maybe these days the CPU can watch for a pusha/popa pair and implement it as a window shift.\n\nI'm not sure there's any substitute, though, for SPARCs output registers that become input registers for the called subroutine.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57267, "question": "You know, they used to be, but maybe not anymore.  Maybe these days the CPU can watch for a pusha/popa pair and implement it as a window shift.\n\nI'm not sure there's any substitute, though, for SPARCs output registers that become input registers for the called subroutine.", "aSentId": 57268, "answer": "Unfortunately a pusha/popa pair is still required to modify the memory.\n\nYou would have to change the memory model, make the stack abstract or define it in such a way that poped values off the stack are undefined.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57269, "question": "Unfortunately a pusha/popa pair is still required to modify the memory.\n\nYou would have to change the memory model, make the stack abstract or define it in such a way that poped values off the stack are undefined.", "aSentId": 57270, "answer": "I started down this line of logic 8 years ago trust me things started getting really weird the second I went down the road of micro threads with branching and loops handle via mirco-thread changes ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57271, "question": "I started down this line of logic 8 years ago trust me things started getting really weird the second I went down the road of micro threads with branching and loops handle via mirco-thread changes ", "aSentId": 57272, "answer": "Mill", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57263, "question": "No, I'd want register windows.  The original design from the Berkeley RISC 1 wastes registers, but AMD fixed that in their Am29000 chips by letting programs only shift by as many registers as they actually need.\n\nUnfortunately, AMD couldn't afford to support that architecture, because they needed all the engineers to work on x86.", "aSentId": 57274, "answer": "Why would you want register windows? Aren't most call chains deep enough that it doesn't actually help much, and don't you get most of the benefit with register renaming anyways?\n\nI'm not a CPU architect, though. I could be very wrong.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57275, "question": "Why would you want register windows? Aren't most call chains deep enough that it doesn't actually help much, and don't you get most of the benefit with register renaming anyways?\n\nI'm not a CPU architect, though. I could be very wrong.", "aSentId": 57276, "answer": "The register window says these registers are where I'm getting my input data, these are for internal use, and these are getting sent to the subroutines I call.  A single instruction shifts the window and updates the instruction pointer at the same time, so you have real function call semantics, vs. a wild west.\n\nIf you just have reads and writes of registers, pushes, pops and jumps, I'm sure that modern CPUs are good at figuring out what you meant, but it's just going to be heuristics, like optimizing JavaScript.\n\nFor the call chain depth, if you're concerned with running out of registers, I think the CPU saves the shallower calls off to RAM.  You're going to have a lot more activity in the deeper calls, so I wouldn't expect that to be too expensive.\n\nBut I'm not a CPU architect, either.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57277, "question": "The register window says these registers are where I'm getting my input data, these are for internal use, and these are getting sent to the subroutines I call.  A single instruction shifts the window and updates the instruction pointer at the same time, so you have real function call semantics, vs. a wild west.\n\nIf you just have reads and writes of registers, pushes, pops and jumps, I'm sure that modern CPUs are good at figuring out what you meant, but it's just going to be heuristics, like optimizing JavaScript.\n\nFor the call chain depth, if you're concerned with running out of registers, I think the CPU saves the shallower calls off to RAM.  You're going to have a lot more activity in the deeper calls, so I wouldn't expect that to be too expensive.\n\nBut I'm not a CPU architect, either.", "aSentId": 57278, "answer": "Once you exhaust the windows, every call will have to spill one window's registers and will be slower. So you'll have to store 16 registers (8 %iN and 8 %lN) even for a stupid function that just does\n\n    static int f(int n)\n    {\n         return g(n) + 1;\n    }", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57279, "question": "Once you exhaust the windows, every call will have to spill one window's registers and will be slower. So you'll have to store 16 registers (8 %iN and 8 %lN) even for a stupid function that just does\n\n    static int f(int n)\n    {\n         return g(n) + 1;\n    }", "aSentId": 57280, "answer": "Only in *very* naive implementation. A smarter implementation would asynchronously spill the register window into the cache hierarchy without stalling.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57277, "question": "The register window says these registers are where I'm getting my input data, these are for internal use, and these are getting sent to the subroutines I call.  A single instruction shifts the window and updates the instruction pointer at the same time, so you have real function call semantics, vs. a wild west.\n\nIf you just have reads and writes of registers, pushes, pops and jumps, I'm sure that modern CPUs are good at figuring out what you meant, but it's just going to be heuristics, like optimizing JavaScript.\n\nFor the call chain depth, if you're concerned with running out of registers, I think the CPU saves the shallower calls off to RAM.  You're going to have a lot more activity in the deeper calls, so I wouldn't expect that to be too expensive.\n\nBut I'm not a CPU architect, either.", "aSentId": 57282, "answer": "The mill has a hardware spiller which can evict older spilled values to ram.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57263, "question": "No, I'd want register windows.  The original design from the Berkeley RISC 1 wastes registers, but AMD fixed that in their Am29000 chips by letting programs only shift by as many registers as they actually need.\n\nUnfortunately, AMD couldn't afford to support that architecture, because they needed all the engineers to work on x86.", "aSentId": 57284, "answer": "So I've been programming in high level languages for my entire adult life and don't know what a register is. Can you explain? Is it just a memory address?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57261, "question": "ARM is actually pretty close to an answer to your question. ", "aSentId": 57286, "answer": "ARM executes out of order too though.  so many of the weird external behaviours of x86 are present in ARM", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57287, "question": "ARM executes out of order too though.  so many of the weird external behaviours of x86 are present in ARM", "aSentId": 57288, "answer": "Good. That's *very* good. You don't want things *not* executing OoO.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57289, "question": "Good. That's *very* good. You don't want things *not* executing OoO.", "aSentId": 57290, "answer": "As long as it's semantically equivalent whats the problem?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57291, "question": "As long as it's semantically equivalent whats the problem?", "aSentId": 57292, "answer": "But it's not semantically equivalent. Out-of-order execution isn't actually \"out of order\". It only looks that way to people that don't understand the semantics.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57293, "question": "But it's not semantically equivalent. Out-of-order execution isn't actually \"out of order\". It only looks that way to people that don't understand the semantics.", "aSentId": 57294, "answer": "oh sorry I misread what you wrote. That's exactly what I meant.\nDouble negative confused me :(", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57293, "question": "But it's not semantically equivalent. Out-of-order execution isn't actually \"out of order\". It only looks that way to people that don't understand the semantics.", "aSentId": 57296, "answer": "As I understand it, out-of-order execution *really is* out of order.  Really.  For reals.  It's just that the CPU reassembles the output such that the outcome is the same *as if* the instructions had all executed in order. \n\nFrom what I understand (I'm no CPU engineer), there is some seriously deep mojo going on to make this happen.  Sometimes instructions don't depend on one another, so it doesn't matter which runs first, but it's commonplace for the results of one instruction to feed into another.   \n\nOne of the major areas is after a branch; the CPU will see a branch, will predict which way the branch will go, and will start fetching and executing instructions based on its prediction, filling up its pipeline.  Later, it may find out that it guessed wrong, and has to flush the entire chain of executed instructions and their results.  This is called a 'pipeline stall', and the deeper a CPU is pipelined, the more of a performance hit happens.  (any stall is bad, because then the CPU has to basically sit around, drumming its metaphorical fingers, waiting for the RAM fetch from the correct branch to start showing up... this is super, super slow by CPU standards.)  \n\nBut, when it predicts correctly, a whole bunch of instructions get fetched, decoded, and executed even before the results of the branch are known.  That's part of why modern CPUs are so fast, compared to the 386, which just executed the instruction chain in order. \n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57297, "question": "As I understand it, out-of-order execution *really is* out of order.  Really.  For reals.  It's just that the CPU reassembles the output such that the outcome is the same *as if* the instructions had all executed in order. \n\nFrom what I understand (I'm no CPU engineer), there is some seriously deep mojo going on to make this happen.  Sometimes instructions don't depend on one another, so it doesn't matter which runs first, but it's commonplace for the results of one instruction to feed into another.   \n\nOne of the major areas is after a branch; the CPU will see a branch, will predict which way the branch will go, and will start fetching and executing instructions based on its prediction, filling up its pipeline.  Later, it may find out that it guessed wrong, and has to flush the entire chain of executed instructions and their results.  This is called a 'pipeline stall', and the deeper a CPU is pipelined, the more of a performance hit happens.  (any stall is bad, because then the CPU has to basically sit around, drumming its metaphorical fingers, waiting for the RAM fetch from the correct branch to start showing up... this is super, super slow by CPU standards.)  \n\nBut, when it predicts correctly, a whole bunch of instructions get fetched, decoded, and executed even before the results of the branch are known.  That's part of why modern CPUs are so fast, compared to the 386, which just executed the instruction chain in order. \n", "aSentId": 57298, "answer": "When I say 'isn't actually \"out of order\"', I mean that while it is out of order compared to the order the instructions are written in the disassembler's output, it is a mistake to assume that that order is the 'correct' order. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57299, "question": "When I say 'isn't actually \"out of order\"', I mean that while it is out of order compared to the order the instructions are written in the disassembler's output, it is a mistake to assume that that order is the 'correct' order. ", "aSentId": 57300, "answer": "&gt; it is a mistake to assume that that order is the 'correct' order. \n\nWell, isn't that kind of a tautology, though?  If I understand you correctly, you're saying that a CPU can't be out of order, because the order that it executes instructions is, by definition, the correct one. \n\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57301, "question": "&gt; it is a mistake to assume that that order is the 'correct' order. \n\nWell, isn't that kind of a tautology, though?  If I understand you correctly, you're saying that a CPU can't be out of order, because the order that it executes instructions is, by definition, the correct one. \n\n", "aSentId": 57302, "answer": "Suppose you have the following c code (with roughly 1 c line = 1 asm instruction)\n\n    bool isEqualToZero = (x == 0);\n    if (isEqualToZero)\n    {\n        x = y;\n        x += z;\n    }\n\nA normal process would do each line in order, waiting for the previous one to complete. An out-of-order processor could do something like this:\n\n    isEqualToZero = (x == 0);\n    _tmp1 = y;\n    _tmp1 += z;\n    if (isEqualToZero)\n    {\n        x = _tmp1;\n    }\n\nSupposing compares and additions use different parts of execution, it would be able to perform the assign and add before even waiting for the compare to finish (as long as it finished by the if check). This is where the performance gains of modern processors come from.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57301, "question": "&gt; it is a mistake to assume that that order is the 'correct' order. \n\nWell, isn't that kind of a tautology, though?  If I understand you correctly, you're saying that a CPU can't be out of order, because the order that it executes instructions is, by definition, the correct one. \n\n", "aSentId": 57304, "answer": "I think what he means is that some instructions are intrinsically parallel, because they do not depend on each other's outputs. So instead of writing A,B,C,D,E, you can write:\n\nA\n\nB,C\n\nD,E\n\nAnd instructions on the same line are parallel. It's more like some instructions are unordered.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57297, "question": "As I understand it, out-of-order execution *really is* out of order.  Really.  For reals.  It's just that the CPU reassembles the output such that the outcome is the same *as if* the instructions had all executed in order. \n\nFrom what I understand (I'm no CPU engineer), there is some seriously deep mojo going on to make this happen.  Sometimes instructions don't depend on one another, so it doesn't matter which runs first, but it's commonplace for the results of one instruction to feed into another.   \n\nOne of the major areas is after a branch; the CPU will see a branch, will predict which way the branch will go, and will start fetching and executing instructions based on its prediction, filling up its pipeline.  Later, it may find out that it guessed wrong, and has to flush the entire chain of executed instructions and their results.  This is called a 'pipeline stall', and the deeper a CPU is pipelined, the more of a performance hit happens.  (any stall is bad, because then the CPU has to basically sit around, drumming its metaphorical fingers, waiting for the RAM fetch from the correct branch to start showing up... this is super, super slow by CPU standards.)  \n\nBut, when it predicts correctly, a whole bunch of instructions get fetched, decoded, and executed even before the results of the branch are known.  That's part of why modern CPUs are so fast, compared to the 386, which just executed the instruction chain in order. \n", "aSentId": 57306, "answer": "What you're describing is speculative execution. That's a bit newer than OoO.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57291, "question": "As long as it's semantically equivalent whats the problem?", "aSentId": 57308, "answer": "As metionned in the article, it's messing up some instructions timing.\n\nThe deal here is that you don't want the CPU to be sitting idly while waiting for something like a memory or peripheral read. So the processor will continue executing instructions while it waits for the data to come in.\n\nHere's where we introduce the speculative execution component in Intel CPUs. What happens is that while the CPU would normally appear idle, it keeps going on executing instructions. When the peripheral read or write is complete, it will \"jump\" to real execution is. If it reaches branch instructions during this time, it usually will execute both and just drop the one that isn't used once it catches up.\n\nThat might sound a bit confusing, I know it isn't 100% clear for me. In short, in order not to waste CPU cycles waiting for slower reads and writes, it will continue executing code transparently, and continue where it was once the read/write is done. To the programmer it looks completely orderly and sequential, but CPU-wise it is out of order.\n\nThat's the reason why CPU are so fast today, but also the reason why timing is off for the greater part of the x86 instruction set.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57309, "question": "As metionned in the article, it's messing up some instructions timing.\n\nThe deal here is that you don't want the CPU to be sitting idly while waiting for something like a memory or peripheral read. So the processor will continue executing instructions while it waits for the data to come in.\n\nHere's where we introduce the speculative execution component in Intel CPUs. What happens is that while the CPU would normally appear idle, it keeps going on executing instructions. When the peripheral read or write is complete, it will \"jump\" to real execution is. If it reaches branch instructions during this time, it usually will execute both and just drop the one that isn't used once it catches up.\n\nThat might sound a bit confusing, I know it isn't 100% clear for me. In short, in order not to waste CPU cycles waiting for slower reads and writes, it will continue executing code transparently, and continue where it was once the read/write is done. To the programmer it looks completely orderly and sequential, but CPU-wise it is out of order.\n\nThat's the reason why CPU are so fast today, but also the reason why timing is off for the greater part of the x86 instruction set.", "aSentId": 57310, "answer": "yeah I know about CPU architecture I just misread his double negative :P\n\nIt's to do with instruction pipelining, feed forward paths, branch delay slots etc. I'm writing a compiler at the moment so these things are kind of important to know (although it's not for x86).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57289, "question": "Good. That's *very* good. You don't want things *not* executing OoO.", "aSentId": 57312, "answer": "Well, that may depend on the length of the pipeline and how much variation in the average number of clocks to resolve and op.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57289, "question": "Good. That's *very* good. You don't want things *not* executing OoO.", "aSentId": 57314, "answer": "No, thank you, I do not want OoO in the GPU cores. I'd rather have more cores per square mm, at a lower clock rate.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57315, "question": "No, thank you, I do not want OoO in the GPU cores. I'd rather have more cores per square mm, at a lower clock rate.", "aSentId": 57316, "answer": "Multicore performance is only good for some algorithms, where single-threaded performance is good all the time, for every workload, if for no other reason than the chip can go back to sleep sooner.\n\nIf we had the choice between 1 hypothetical processor at 10GHz, and 5 at 2GHz, the 1 would be much better for almost everyone, at least in the consumer space. \n\nSome algorithms scale very well, so that even 6 at 2 would be better than 1 at 10, but those are few and far between.  Some end up in a gray area, where some number of 2GHz cores will eventually exceed the throughput of 1 at 10GHz.  But for many workloads, it wouldn't matter if you could throw an infinite number of 2GHz cores at the problem, it still wouldn't be as fast as 1 at 10.  \n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57317, "question": "Multicore performance is only good for some algorithms, where single-threaded performance is good all the time, for every workload, if for no other reason than the chip can go back to sleep sooner.\n\nIf we had the choice between 1 hypothetical processor at 10GHz, and 5 at 2GHz, the 1 would be much better for almost everyone, at least in the consumer space. \n\nSome algorithms scale very well, so that even 6 at 2 would be better than 1 at 10, but those are few and far between.  Some end up in a gray area, where some number of 2GHz cores will eventually exceed the throughput of 1 at 10GHz.  But for many workloads, it wouldn't matter if you could throw an infinite number of 2GHz cores at the problem, it still wouldn't be as fast as 1 at 10.  \n", "aSentId": 57318, "answer": "Ok. Try to get a decent performance per watt from a beefy OoO. Not a hypothetical one, but any of the real things.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57317, "question": "Multicore performance is only good for some algorithms, where single-threaded performance is good all the time, for every workload, if for no other reason than the chip can go back to sleep sooner.\n\nIf we had the choice between 1 hypothetical processor at 10GHz, and 5 at 2GHz, the 1 would be much better for almost everyone, at least in the consumer space. \n\nSome algorithms scale very well, so that even 6 at 2 would be better than 1 at 10, but those are few and far between.  Some end up in a gray area, where some number of 2GHz cores will eventually exceed the throughput of 1 at 10GHz.  But for many workloads, it wouldn't matter if you could throw an infinite number of 2GHz cores at the problem, it still wouldn't be as fast as 1 at 10.  \n", "aSentId": 57320, "answer": "Not to mention the voltage needed to get a CPU to run at 10GHz smoothly is significantly higher than 2 cores at 5GHz. Intel kinda learned that lesson the hard way.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57321, "question": "Not to mention the voltage needed to get a CPU to run at 10GHz smoothly is significantly higher than 2 cores at 5GHz. Intel kinda learned that lesson the hard way.", "aSentId": 57322, "answer": "Well, basically, if Intel *could* give us single-core processors at 10GHz, they absolutely would, because that's what we really need. \n\nBut that's not what they can make, so we get multicore at about 4GHz, tops.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57315, "question": "No, thank you, I do not want OoO in the GPU cores. I'd rather have more cores per square mm, at a lower clock rate.", "aSentId": 57324, "answer": "We're talking about CPUs, dude.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57325, "question": "We're talking about CPUs, dude.", "aSentId": 57326, "answer": "There are many cases when I'd prefer, say, Cortex-A7 (which is multi-issue, but not OoO, thank you very much) to something much more power-hungry, like an OoO Cortex-A15. Same thing as with GPUs - area and/or power. CPUs are not any different, you have to choose the right balance.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57327, "question": "There are many cases when I'd prefer, say, Cortex-A7 (which is multi-issue, but not OoO, thank you very much) to something much more power-hungry, like an OoO Cortex-A15. Same thing as with GPUs - area and/or power. CPUs are not any different, you have to choose the right balance.", "aSentId": 57328, "answer": "Dude, just stop.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57329, "question": "Dude, just stop.", "aSentId": 57330, "answer": "Raspberry Pi 2 is 4xA7. Still below 2W. Good luck getting there with anything OoO.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57331, "question": "Raspberry Pi 2 is 4xA7. Still below 2W. Good luck getting there with anything OoO.", "aSentId": 57332, "answer": "We're not talking about shitty mobile CPUs, dude.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57333, "question": "We're not talking about shitty mobile CPUs, dude.", "aSentId": 57334, "answer": "You're definitely not in a position to decide what deserves to be called a CPU.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57335, "question": "You're definitely not in a position to decide what deserves to be called a CPU.", "aSentId": 57336, "answer": "No dude, you're just being silly. \n\nDude.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57261, "question": "ARM is actually pretty close to an answer to your question. ", "aSentId": 57338, "answer": "&gt;ARM is actually pretty close to an answer to your question.\n\nWhy do you say that? It is just as suitable as x86 for building low latency CPUs that pretend to execute one instruction at a time in their written order. It too and suffers from many of the same pitfalls as x86 because they aren't that different where it actually matters. Examples: \n\n* ARM is a variable length instruction set. It supports 2, 4, and 8B code. Length decoding is hard. x86 goes a bit crazier, 1B-32B. However, they both need to do length decoding and as a result it is not as simple as building multiple decoders to get good decode bandwidth out of either. At least x86 has better code size. \n\n* ARM doesn't actually have enough architectural registers to forgo renaming. 32 64b registers is twice x86, both are not the 100+ actually needed for decent performance. Regardless, rather have my CPU resolve this than devote instruction bits to register addressing. \n\n* ARM has a few incredibly complicated instructions that must be decoded into many simple operations... like x86. Sure it doesn't go crazy with it, but its only natural to propose the same solutions. Its not like supporting weird instructions adds much complexity, but STR and STM are certainly not RISC. They are only adding more as ARM gains popularity in real workstations. \n\nAssuming we are talking about ARMv7 or ARMv8 as ARM is not a single backwards compatible ISA. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57342, "question": "Narrow width RISC? That can't even be as fast as x86. ARM/MIPS/Power etc are all pretty terrible for fast execution given the trade-offs in modern hardware.", "aSentId": 57343, "answer": "As fast compared with what? Watt/cycles?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57344, "question": "As fast compared with what? Watt/cycles?", "aSentId": 57345, "answer": "Maximum possible throughput. x86 and it's decendent's more complicated instructions act as a compression format somewhat mitigating the biggest bottleneck on modern processors ie the one in memory bandwidth. None of the RISC architectures do this well at all.\n\nThey don't require the massive decoding infrastructure that x86 does, but die space isn't exactly in short supply.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57342, "question": "Narrow width RISC? That can't even be as fast as x86. ARM/MIPS/Power etc are all pretty terrible for fast execution given the trade-offs in modern hardware.", "aSentId": 57347, "answer": "AArch64 fixed most of that issues, it does not limit execution capabilities (i.e., no predication, no delay slots, etc.).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57259, "question": "Totally.  What a weird high-level language though!  How would you design an instruction set architecture nowadays if you got to start from scratch?", "aSentId": 57349, "answer": "[Like this](http://millcomputing.com/docs/).\n\nEDIT: Yesyes you can write timing side-channel safe code with that, it's got an explicit pipeline and instructions have to be scheduled by the assembler. Needs drilling further down to the hardware than a usual compiler would, but it's a piece of cake, compared to architectures that are too smart for their own good.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57350, "question": "[Like this](http://millcomputing.com/docs/).\n\nEDIT: Yesyes you can write timing side-channel safe code with that, it's got an explicit pipeline and instructions have to be scheduled by the assembler. Needs drilling further down to the hardware than a usual compiler would, but it's a piece of cake, compared to architectures that are too smart for their own good.", "aSentId": 57351, "answer": "Ivan Godard is the only person with powerful enough wizard hair to pull something like this off.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57352, "question": "Ivan Godard is the only person with powerful enough wizard hair to pull something like this off.", "aSentId": 57353, "answer": "Can confirm: [wow](http://electronics360.globalspec.com/images/assets/843/3843/99eb26c2-69c0-4fe8-9411-d2a477e48450.jpg).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57350, "question": "[Like this](http://millcomputing.com/docs/).\n\nEDIT: Yesyes you can write timing side-channel safe code with that, it's got an explicit pipeline and instructions have to be scheduled by the assembler. Needs drilling further down to the hardware than a usual compiler would, but it's a piece of cake, compared to architectures that are too smart for their own good.", "aSentId": 57355, "answer": "That looks really cool--hope it comes to fruition.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57356, "question": "That looks really cool--hope it comes to fruition.", "aSentId": 57357, "answer": "So many great ideas. I wonder how fast it could execute x86 code (by VM or native VM)? If fast enough, that could aid its adoption massively.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57350, "question": "[Like this](http://millcomputing.com/docs/).\n\nEDIT: Yesyes you can write timing side-channel safe code with that, it's got an explicit pipeline and instructions have to be scheduled by the assembler. Needs drilling further down to the hardware than a usual compiler would, but it's a piece of cake, compared to architectures that are too smart for their own good.", "aSentId": 57359, "answer": "The Mill is a terrible, terrible model for an abstract machine. The very design of it is based on exposing as much of the actual hardware as possible.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57360, "question": "The Mill is a terrible, terrible model for an abstract machine. The very design of it is based on exposing as much of the actual hardware as possible.", "aSentId": 57361, "answer": "So?  Programmers don't see it and compilers can manage the complexity.  Seems like the best criterion is not simplicity but scalability--that is, how well it will work when we have ten or a hundred times as many gates.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57362, "question": "So?  Programmers don't see it and compilers can manage the complexity.  Seems like the best criterion is not simplicity but scalability--that is, how well it will work when we have ten or a hundred times as many gates.\n", "aSentId": 57363, "answer": "You'd have to recompile if you upgraded the processor. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57364, "question": "You'd have to recompile if you upgraded the processor. ", "aSentId": 57365, "answer": "Re-assemble, not recompile. There's a processor-independent assembly format, what's left is instruction scheduling and spilling from the belt. \n\nThat functionality is AFAIU going to come with the OS or even BIOS, and not really much different from having a dynamic loader first have a look at your code. At least the information of how to do that should come with the CPU, it ought to know its belt size, configuration of functional units etc.\n\nWhether the assembler itself is a ROM routine or not is another question, and might be dependent on feature set. Say, the ROM routine not being able to translate instructions it doesn't have hardware for into emulation routines. But I can't imagine they'd be having CPU-dependent bootloader code: On CPU startup, read some bytes from somewhere, put them through the ROM routine, then execute the result. A bootloader doesn't need fancy instructions so that should work out fine.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57360, "question": "The Mill is a terrible, terrible model for an abstract machine. The very design of it is based on exposing as much of the actual hardware as possible.", "aSentId": 57367, "answer": "So was the 8086. The fact that there is now a virtual machine implemented in hardware is because the trade-offs involved in modern chips are very different from 25 years ago.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57368, "question": "So was the 8086. The fact that there is now a virtual machine implemented in hardware is because the trade-offs involved in modern chips are very different from 25 years ago.", "aSentId": 57369, "answer": "No. At least part of why x86 is so successful is that it's basic programming model is surprisingly amenable to being an abstract machine model. It allows widely differing implementations that provide the same programming interface. It has plenty of parts that are not very suitable for this, but those parts are implemented very slowly so most people just kind of pretend they are not there.\n\nThe other old CPU arch in wide use that's quite good as an abstract model is ARM.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57370, "question": "No. At least part of why x86 is so successful is that it's basic programming model is surprisingly amenable to being an abstract machine model. It allows widely differing implementations that provide the same programming interface. It has plenty of parts that are not very suitable for this, but those parts are implemented very slowly so most people just kind of pretend they are not there.\n\nThe other old CPU arch in wide use that's quite good as an abstract model is ARM.", "aSentId": 57371, "answer": "They're both quite close to a von neuman machine, but they expose massive amounts of \"implementation detail\" the arbitrary (small) number of addressable registers available for example. Besides the mill may expose a lot of details, but it's the compilers job to worry about optimising for each machine the code is run on. You can just compile to a stable intermediate form then run a final optimising pass during package installation, same way IBM have been doing for 50 years.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57370, "question": "No. At least part of why x86 is so successful is that it's basic programming model is surprisingly amenable to being an abstract machine model. It allows widely differing implementations that provide the same programming interface. It has plenty of parts that are not very suitable for this, but those parts are implemented very slowly so most people just kind of pretend they are not there.\n\nThe other old CPU arch in wide use that's quite good as an abstract model is ARM.", "aSentId": 57373, "answer": "PPC too.  Basically anything that doesn't have delay slots.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57360, "question": "The Mill is a terrible, terrible model for an abstract machine. The very design of it is based on exposing as much of the actual hardware as possible.", "aSentId": 57375, "answer": "Recipe for pouring cement on it. Hey, let's expose ALL of our functions and methods to the outside! That way, if we change the design of ANYTHING, we can break EVERYTHING!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57377, "question": "Well this is remarkable for its overall architecture, not necessarily its instruction design.  As the designers themselves put it, it has so many details in the assembly language, that nobody would ever want to program it by hand this way.", "aSentId": 57378, "answer": "Only the early very simple stuff and CISC was ever supposed to be hand-written. RISC may be manageable, but it still is designed for compilers, not humans.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57380, "question": "Is there an FPGA source for this? Could one even be programmed as this?", "aSentId": 57381, "answer": "From what they release, they're currently working on FPGA implementations, as stepping stone to raw silicon. All what he's talking about is results from software simulation.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57259, "question": "Totally.  What a weird high-level language though!  How would you design an instruction set architecture nowadays if you got to start from scratch?", "aSentId": 57384, "answer": "TBH, I feel like Intel's IA64 architecture never really got a fair shake.  The concept of \"do most optimizations in the compiler\" really rings true to where compiler tech has started going to now-a-days.  The problem with it is that compilers weren't there yet, x86 had too strong of a hold on everything, and the x86 to IA64 translation resulted in applications with anywhere from 10%-&gt;50% performance penalties.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57385, "question": "TBH, I feel like Intel's IA64 architecture never really got a fair shake.  The concept of \"do most optimizations in the compiler\" really rings true to where compiler tech has started going to now-a-days.  The problem with it is that compilers weren't there yet, x86 had too strong of a hold on everything, and the x86 to IA64 translation resulted in applications with anywhere from 10%-&gt;50% performance penalties.", "aSentId": 57386, "answer": "Itanium was honestly just a really hard architecture to write a compiler for. It tried to go a good direction, but it didn't go far enough- it still did register renaming and out of order execution underneath all the explicit parallelism.\n\nLook at DSPs for an example of taking that idea to the extreme. For the type of workloads they're designed for, they absolutely destroy a typical superscalar/OoO CPU. Also, obligatory [Mill reference](http://millcomputing.com).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57387, "question": "Itanium was honestly just a really hard architecture to write a compiler for. It tried to go a good direction, but it didn't go far enough- it still did register renaming and out of order execution underneath all the explicit parallelism.\n\nLook at DSPs for an example of taking that idea to the extreme. For the type of workloads they're designed for, they absolutely destroy a typical superscalar/OoO CPU. Also, obligatory [Mill reference](http://millcomputing.com).", "aSentId": 57388, "answer": "&gt; Itanium was honestly just a really hard architecture to write a compiler for.\n\nTrue.  I mean, it really hasn't been until pretty recently (like the past 5 years) that compilers have gotten good at vectorizing.  Something that is pretty essential to get the most performance out of an itanium processor.\n\n&gt; it still did register renaming and out of order execution underneath all the explicit parallelism.\n\nI'm not sure how you would get around register renaming or even OO stuff.  After all, the CPU has a little better idea of how internal resources are currently being used.  It is about the only place that has that kind of information.\n\n&gt; Look at DSPs for taking that idea to the extreme. For the type of workloads they're designed for, they absolutely destroy a typical superscalar/OoO CPU.\n\nThere are a few problems with DSPs.  The biggest is that in order to get the general CPU destroying speeds, you pretty much have to pull out a HDL.  No compiling from C to an HDL will get you that sort of performance.  The reasons these things are so fast is because you can take advantage of the fact that everything happens async by default.\n\nThat being said, I could totally see future CPUs having DSP hardware built into them.  After all. I think the likes of Intel and AMD are running out of ideas on what they can do with x86 stuff to get any faster.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57389, "question": "&gt; Itanium was honestly just a really hard architecture to write a compiler for.\n\nTrue.  I mean, it really hasn't been until pretty recently (like the past 5 years) that compilers have gotten good at vectorizing.  Something that is pretty essential to get the most performance out of an itanium processor.\n\n&gt; it still did register renaming and out of order execution underneath all the explicit parallelism.\n\nI'm not sure how you would get around register renaming or even OO stuff.  After all, the CPU has a little better idea of how internal resources are currently being used.  It is about the only place that has that kind of information.\n\n&gt; Look at DSPs for taking that idea to the extreme. For the type of workloads they're designed for, they absolutely destroy a typical superscalar/OoO CPU.\n\nThere are a few problems with DSPs.  The biggest is that in order to get the general CPU destroying speeds, you pretty much have to pull out a HDL.  No compiling from C to an HDL will get you that sort of performance.  The reasons these things are so fast is because you can take advantage of the fact that everything happens async by default.\n\nThat being said, I could totally see future CPUs having DSP hardware built into them.  After all. I think the likes of Intel and AMD are running out of ideas on what they can do with x86 stuff to get any faster.", "aSentId": 57390, "answer": "Well, both Intel and AMD are already integrating GPUs onto the die, wouldn't be surprised if we start seeing tighter integration between the different cores.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57389, "question": "&gt; Itanium was honestly just a really hard architecture to write a compiler for.\n\nTrue.  I mean, it really hasn't been until pretty recently (like the past 5 years) that compilers have gotten good at vectorizing.  Something that is pretty essential to get the most performance out of an itanium processor.\n\n&gt; it still did register renaming and out of order execution underneath all the explicit parallelism.\n\nI'm not sure how you would get around register renaming or even OO stuff.  After all, the CPU has a little better idea of how internal resources are currently being used.  It is about the only place that has that kind of information.\n\n&gt; Look at DSPs for taking that idea to the extreme. For the type of workloads they're designed for, they absolutely destroy a typical superscalar/OoO CPU.\n\nThere are a few problems with DSPs.  The biggest is that in order to get the general CPU destroying speeds, you pretty much have to pull out a HDL.  No compiling from C to an HDL will get you that sort of performance.  The reasons these things are so fast is because you can take advantage of the fact that everything happens async by default.\n\nThat being said, I could totally see future CPUs having DSP hardware built into them.  After all. I think the likes of Intel and AMD are running out of ideas on what they can do with x86 stuff to get any faster.", "aSentId": 57392, "answer": "&gt;There are a few problems with DSPs. The biggest is that in order to get the general CPU destroying speeds, you pretty much have to pull out a HDL. No compiling from C to an HDL will get you that sort of performance. The reasons these things are so fast is because you can take advantage of the fact that everything happens async by default.\n\nYou're confusing DSPs with FPGAS.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57389, "question": "&gt; Itanium was honestly just a really hard architecture to write a compiler for.\n\nTrue.  I mean, it really hasn't been until pretty recently (like the past 5 years) that compilers have gotten good at vectorizing.  Something that is pretty essential to get the most performance out of an itanium processor.\n\n&gt; it still did register renaming and out of order execution underneath all the explicit parallelism.\n\nI'm not sure how you would get around register renaming or even OO stuff.  After all, the CPU has a little better idea of how internal resources are currently being used.  It is about the only place that has that kind of information.\n\n&gt; Look at DSPs for taking that idea to the extreme. For the type of workloads they're designed for, they absolutely destroy a typical superscalar/OoO CPU.\n\nThere are a few problems with DSPs.  The biggest is that in order to get the general CPU destroying speeds, you pretty much have to pull out a HDL.  No compiling from C to an HDL will get you that sort of performance.  The reasons these things are so fast is because you can take advantage of the fact that everything happens async by default.\n\nThat being said, I could totally see future CPUs having DSP hardware built into them.  After all. I think the likes of Intel and AMD are running out of ideas on what they can do with x86 stuff to get any faster.", "aSentId": 57394, "answer": "&gt;  Something that is pretty essential to get the most performance out of an itanium processor.\n\nThat wasn't vectorizing, it was stuff like modulo scheduling. The Itanium could optimize it with its weird rotating registers. But modulo scheduling really only helps with tight kernels, not with general purpose code like a Python interpreter.\n\nKinda like Sun's Niagara microprocessor. It had 1 FPU for each 8 cores, not a great match when your language's only numeric data type is floating point (as is the case for PHP).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57396, "question": "Are compilers actually good at vectorizing though?  Last time I looked, on MSVC 2012, only the very simplest loops got vectorized.  Certainly anyone who really wants SIMD performance will write it manually and continue to do so for a long time.", "aSentId": 57397, "answer": "&gt; Are compilers actually good at vectorizing though?\n\nNot that bad, really, especially if you use polyhedral vectorisation (e.g., LLVM with Polly).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57385, "question": "TBH, I feel like Intel's IA64 architecture never really got a fair shake.  The concept of \"do most optimizations in the compiler\" really rings true to where compiler tech has started going to now-a-days.  The problem with it is that compilers weren't there yet, x86 had too strong of a hold on everything, and the x86 to IA64 translation resulted in applications with anywhere from 10%-&gt;50% performance penalties.", "aSentId": 57399, "answer": "It got a much fairer shake than it deserved. It sucked. It was slow. End of story.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57401, "question": "* RISC-V is the new, upcoming awesomeness\n* Itanium was awesome, it just happened before the necessary compiler technology happened, and Intel has never reduced the price to anything approaching attractiveness for an architecture that isn't popular enough to warrant the sky-high price.\n* There's always that Mill architecture that's been floating around in the tech news.\n* ARM and especially ARM's Thumb instruction set is pretty cool.\n\nNot a huge fan of x86 of any flavor, but I was really impressed with AMD's Jaguar for a variety of technical reasons, but they never brought it to its fullest potential. They **absolutely** should have released the 8-core + big GPU chip that they put in the PS4 as a general market chip, and released a 16-core + full-size GPU version as well. It would have been awesome and relatively inexpensive. But, they haven't hired me to plan their chip strategy, so that didn't happen.", "aSentId": 57402, "answer": "at some point there is negative return on packing so many cores", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57404, "question": "That would be quite interesting. Unfortunately it would require a lot of money with potentially no return, so it won't happen. \r\rI do find inspiring how humans build complexity on top of the complexity their previous generations built, it reminds me of a few Asimov novels.", "aSentId": 57405, "answer": "Reminds me of Vernor Vinge's novel A Deepness in the Sky, where \"programmer archaeologists\" work on systems millennia old, going back to the original Unix.\n\nAt one point they describe the \"incredibly complex\" timekeeping code, which uses the first moon landing as its epoch... except it's actually off by a few million seconds because it's the Unix epoch.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57404, "question": "That would be quite interesting. Unfortunately it would require a lot of money with potentially no return, so it won't happen. \r\rI do find inspiring how humans build complexity on top of the complexity their previous generations built, it reminds me of a few Asimov novels.", "aSentId": 57407, "answer": "It's in our politics too and not just our technology.  Each successive reform is instituted to fix the previous reform.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57409, "question": "Honestly, as a compiler writer x86 is perfectly pleasant to deal with. It's very easy actually. ARM is a bit annoying because it is verbose, but otherwise is ok.\r\rSome level of abstraction is necessary to allow chipmakers to make perf improvements without requiring different binaries. Adding new instructions takes a very long time. Compiling with sse2 is only starting to happen now, despite sse2 coming out well over a decade ago.", "aSentId": 57410, "answer": "&gt; Compiling with sse2 is only starting to happen now, despite sse2 coming out well over a decade ago.\n\nYou amaze me, but then I don't know much about how the architecture changed since I learned on the 8088.  It was already a kludged up eight bit machine then and now there must be geological strata of kludges on top of kludges.  Why aren't compiler writers more eager to exploit every possible optimization?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57411, "question": "&gt; Compiling with sse2 is only starting to happen now, despite sse2 coming out well over a decade ago.\n\nYou amaze me, but then I don't know much about how the architecture changed since I learned on the 8088.  It was already a kludged up eight bit machine then and now there must be geological strata of kludges on top of kludges.  Why aren't compiler writers more eager to exploit every possible optimization?", "aSentId": 57412, "answer": "We are! The problem is that if you use new instructions, then your software won't run on older machines. The most natural way to gate it is by OS version, but Windows 7 supports non-SSE2 machines. So it's difficult to make SSE2 a \"default\" compile option, not to mention SSE4 or AVX. You have to leave it up to the programmer of your source language to decide if they want to use newer instruction sets, for example by some compiler flag.\n\nI should note though, that I personally work on a JIT, which means this isn't as much of an issue for us, because we detect what features your CPU has and emit instructions based on your individual capabilities.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57413, "question": "We are! The problem is that if you use new instructions, then your software won't run on older machines. The most natural way to gate it is by OS version, but Windows 7 supports non-SSE2 machines. So it's difficult to make SSE2 a \"default\" compile option, not to mention SSE4 or AVX. You have to leave it up to the programmer of your source language to decide if they want to use newer instruction sets, for example by some compiler flag.\n\nI should note though, that I personally work on a JIT, which means this isn't as much of an issue for us, because we detect what features your CPU has and emit instructions based on your individual capabilities.", "aSentId": 57414, "answer": "hence one of the strengths of JIT is that it can be platform agnostic.. but as for gating per OS, I find it more due to linkage codes.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57416, "question": "Modern MIPS! The Mill!", "aSentId": 57417, "answer": "Is there an actual Mill prototype anywhere? All I've seen about it is talk, not even a VM-like playground", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57418, "question": "Is there an actual Mill prototype anywhere? All I've seen about it is talk, not even a VM-like playground", "aSentId": 57419, "answer": "They apparently have running simulators, but don't release that stuff into the wild.\n\nI guess it's a patent issue, in one of the videos Ivan said something to the effect of \"yeah I'll talk about that topic in some upcoming video as soon as the patents are filed\", and then complained about first-to-file vs. first-to-invent.\n\nThe simulator, by its nature, would contain practically all secret sauce.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57420, "question": "They apparently have running simulators, but don't release that stuff into the wild.\n\nI guess it's a patent issue, in one of the videos Ivan said something to the effect of \"yeah I'll talk about that topic in some upcoming video as soon as the patents are filed\", and then complained about first-to-file vs. first-to-invent.\n\nThe simulator, by its nature, would contain practically all secret sauce.", "aSentId": 57421, "answer": "Ah well, patent issues would make sense I guess, too bad", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57416, "question": "Modern MIPS! The Mill!", "aSentId": 57423, "answer": "mips &lt;/3", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57425, "question": "The ATMega 8-bit instructuon set is very nice.", "aSentId": 57426, "answer": "You mean AVR? And no.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57427, "question": "You mean AVR? And no.", "aSentId": 57428, "answer": "Why not? I have written MSP430, MIPS, x86 and AVR Assembly. AVR is the nicest of them all.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57429, "question": "Why not? I have written MSP430, MIPS, x86 and AVR Assembly. AVR is the nicest of them all.", "aSentId": 57430, "answer": "Because AVR lacks basic memory management and protection concepts that are essential to running a modern OS with anything resembling security. AVR assembly may be nice to write, but as far as architectures, it is woefully incomplete for a computer, and I merely listed one deficiency. The discussion here is not \"what is a great, easy to use assembly language?\" but \"what would an ideal, modern architecture look like?\" I believe, since we're discussing where x86 fails, and how unpredictable the timing is. You could even design a high-level assembly-like language that feels like AVR and compiles down to something else if you wanted, but that doesn't relate to the problem at hand.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57433, "question": "Something close to SSA (single static assignment) form as in LLVM, leaving register allocation to the CPU.", "aSentId": 57434, "answer": "How would you encode an infinite number of pseudo-registers into finite number of bits in your instructions? We're already at a stage where there is much more physical registers than logical, due to the encoding constraints.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57438, "question": "The Itanium, by Intel and HP, did that pretty well IMO. Then it failed in the market :(", "aSentId": 57439, "answer": "It failed to deliver on its performance promises. It's not clear that even if it had been executed better that it could have delivered.\n\nIn a wide variety of ways, Itanium was a testament to Intel's failure to understand what had made x86 successful (i.e., from a performance standpoint and a market adoption standpoint).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57257, "question": "I think \"x86 is a virtual machine\" might be more accurate. It's still a machine language, just the machine is abstracted on the cpu.", "aSentId": 57442, "answer": "That distinction doesn't really matter in this context though.  Rob isn't just playing word games.  Rob breaks crypto for a living, so all of this is in the context of sidechannel attacks.  To prevent side channel attacks, all crypto operations *must* be constant time (or else getting the 3rd character wrong in a password might take a shorter amount of time to return than getting the 8th character wrong).  People think that by doing constant time operations, they are avoiding the pitfalls of compiler optimizations, and he's pointing out that even if you hard code in the operations, you can still be exposed by the optimizations done by the processor itself.\n\nThese discrepancies may very well allow an attacker on AWS steal SSL keys from other instances running on the same machine.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57257, "question": "I think \"x86 is a virtual machine\" might be more accurate. It's still a machine language, just the machine is abstracted on the cpu.", "aSentId": 57444, "answer": "I don't know that you can truly call x86 assembly a machine language. There are 9 different opcodes for `add`. A naive assembler couldn't handle that.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57255, "question": "x86 is a high-level language", "aSentId": 57446, "answer": "Every time I think I'm starting to understand how a computer works someone posts something like this.  ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57447, "question": "Every time I think I'm starting to understand how a computer works someone posts something like this.  ", "aSentId": 57448, "answer": "Abstraction is a beautiful thing.  Every time you think you've figured it out, you get a little glimpse of the genius built into what you take for granted.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57449, "question": "Abstraction is a beautiful thing.  Every time you think you've figured it out, you get a little glimpse of the genius built into what you take for granted.", "aSentId": 57450, "answer": "To code a program from scratch, you must first create the universe.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57451, "question": "To code a program from scratch, you must first create the universe.", "aSentId": 57452, "answer": "You should get that tattooed somewhere on your body. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57451, "question": "To code a program from scratch, you must first create the universe.", "aSentId": 57454, "answer": "Space and time is a network of complexities.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57447, "question": "Every time I think I'm starting to understand how a computer works someone posts something like this.  ", "aSentId": 57456, "answer": "Honestly? Just don't sweat it. Read the article, enjoy your new-found understanding, with the additional understanding that whatever you understand now will be wrong in a week.\n\nJust focus on algorithmic efficiency. Once you've got your asymptotic time as small as theoretically possible, then focus on which instruction takes how many clock cycles.\n\nMake it work. Make it work right. Make it work fast.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57457, "question": "Honestly? Just don't sweat it. Read the article, enjoy your new-found understanding, with the additional understanding that whatever you understand now will be wrong in a week.\n\nJust focus on algorithmic efficiency. Once you've got your asymptotic time as small as theoretically possible, then focus on which instruction takes how many clock cycles.\n\nMake it work. Make it work right. Make it work fast.", "aSentId": 57458, "answer": "It doesn't change that fast really. OoOE has been around since the 60's, though it wasn't nearly as powerful back then (no register renaming yet). The split front-end/back-end (you can always draw a line I suppose, but a *real* split with \u00b5ops) of modern x86 microarchs has been around since PPro. What has changed is scale - bigger physical register files, bigger execution windows, more tricks in the front-end, more execution units, wider SIMD and more special instructions.\n\nBut not much has changed fundamentally in a long time, a week from now surely nothing will have changed.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57459, "question": "It doesn't change that fast really. OoOE has been around since the 60's, though it wasn't nearly as powerful back then (no register renaming yet). The split front-end/back-end (you can always draw a line I suppose, but a *real* split with \u00b5ops) of modern x86 microarchs has been around since PPro. What has changed is scale - bigger physical register files, bigger execution windows, more tricks in the front-end, more execution units, wider SIMD and more special instructions.\n\nBut not much has changed fundamentally in a long time, a week from now surely nothing will have changed.", "aSentId": 57460, "answer": "Yup, more lost now than ever.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57459, "question": "It doesn't change that fast really. OoOE has been around since the 60's, though it wasn't nearly as powerful back then (no register renaming yet). The split front-end/back-end (you can always draw a line I suppose, but a *real* split with \u00b5ops) of modern x86 microarchs has been around since PPro. What has changed is scale - bigger physical register files, bigger execution windows, more tricks in the front-end, more execution units, wider SIMD and more special instructions.\n\nBut not much has changed fundamentally in a long time, a week from now surely nothing will have changed.", "aSentId": 57462, "answer": "The concepts don't change, of course. If you're compiling to machine code, you should be aware that the processor might change your execution order, branch prediction, memory access latency, cache, etc. The general concepts are important to understand if you're not going to shoot yourself in the foot.\n\nBut the particulars of the actual chip you're using? Worry about that after your algorithm's already theoretically efficient as possible.\n\nI would say the exception is using domain-specific processor features when you're working in that domain. For instance, if I'm doing linear algebra with 3d and 4d vectors, I'll always use the x86 SIMD instructions (SSE* + AVX, wrapped by the amazing [glm](http://glm.g-truc.net/0.9.6/index.html) library).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57459, "question": "It doesn't change that fast really. OoOE has been around since the 60's, though it wasn't nearly as powerful back then (no register renaming yet). The split front-end/back-end (you can always draw a line I suppose, but a *real* split with \u00b5ops) of modern x86 microarchs has been around since PPro. What has changed is scale - bigger physical register files, bigger execution windows, more tricks in the front-end, more execution units, wider SIMD and more special instructions.\n\nBut not much has changed fundamentally in a long time, a week from now surely nothing will have changed.", "aSentId": 57464, "answer": "I understood about half of the terms you used...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57457, "question": "Honestly? Just don't sweat it. Read the article, enjoy your new-found understanding, with the additional understanding that whatever you understand now will be wrong in a week.\n\nJust focus on algorithmic efficiency. Once you've got your asymptotic time as small as theoretically possible, then focus on which instruction takes how many clock cycles.\n\nMake it work. Make it work right. Make it work fast.", "aSentId": 57466, "answer": "Be careful with asymptotics though... A linear search through a vector will typically blow a binary search out of the water on anything that can fit inside your L1-cache. I'd say pay attention to things such as asymptotic complexity but never neglect to actually measure things.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57467, "question": "Be careful with asymptotics though... A linear search through a vector will typically blow a binary search out of the water on anything that can fit inside your L1-cache. I'd say pay attention to things such as asymptotic complexity but never neglect to actually measure things.", "aSentId": 57468, "answer": "If you're working with things small enough to fit in L1 cache, I'd assume you started with a linear search anyway. Since it never pings your profiler, you never rewrite it with something fancy. So it continues on its merry way, happily fitting in cache lines. :)\n\nI'm never in favor of optimizing something that hasn't been profiled to determine *where* to optimize, at which point you improve those hot spots and profile again. I'm usually in favor of taking the simplest way from the start, increasing complexity only when necessary. Together, these rules ensure that trivial tasks are solved trivially and costly tasks are solved strategically.\n\nThat said, if you've analyzed your task well enough, and you're doing anything complicated at all (graphics, math, science, etc.), there will be places where you should add complexity from the start because you *know* it's going to need those exact optimizations later.\n\nBut if you start writing a function, and your first thought is \"how many clock cycles will this function take?\"... you're doing it wrong.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57469, "question": "If you're working with things small enough to fit in L1 cache, I'd assume you started with a linear search anyway. Since it never pings your profiler, you never rewrite it with something fancy. So it continues on its merry way, happily fitting in cache lines. :)\n\nI'm never in favor of optimizing something that hasn't been profiled to determine *where* to optimize, at which point you improve those hot spots and profile again. I'm usually in favor of taking the simplest way from the start, increasing complexity only when necessary. Together, these rules ensure that trivial tasks are solved trivially and costly tasks are solved strategically.\n\nThat said, if you've analyzed your task well enough, and you're doing anything complicated at all (graphics, math, science, etc.), there will be places where you should add complexity from the start because you *know* it's going to need those exact optimizations later.\n\nBut if you start writing a function, and your first thought is \"how many clock cycles will this function take?\"... you're doing it wrong.", "aSentId": 57470, "answer": "There's a difference between premature optimization and a lolworthy attitude to performance though (like using bogosearch, because who cares about the speed).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57471, "question": "There's a difference between premature optimization and a lolworthy attitude to performance though (like using bogosearch, because who cares about the speed).", "aSentId": 57472, "answer": "I mean, that's a knack for awful performance. It's not like people usually come up with the worst possible solution first, it's usually just reasonable but suboptimal pending profiling and optimization.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57447, "question": "Every time I think I'm starting to understand how a computer works someone posts something like this.  ", "aSentId": 57474, "answer": "Since I began programming I don't believe in miracles. I count on them.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57476, "question": "Don't worry about it. I doubt that anyone here can explain the quantum physics of the field effect or the NP / PN junctions. If you don't understand the physics, you don't understand how transistors work, which means you don't understand how logic gates work, which means you don't understand digital circuits, etc. There are very few people in the world who *really* understand how  a computer works.", "aSentId": 57477, "answer": "You could extend that example though and say there are few people who *really* understand how the world works once you explore the economic, historical, and political realities which have shaped boron mining throughout the world (especially Turkey).\n\nAt a certain point transistors have next to no impact on logic gates outside of their reliability. Obviously that is subject to change as there is research into alternative gate designs using things like tunneling transistors and MEMs relays. But at a certain point deterministic logic machines are deterministic logic machines (until they stop because Intel wants to update their architecture &gt;:( Or until they stop because ANN hardware replaces deterministic systems.\n_______________\nTL-DR: I've utterly failed to make my point, you should probably start reading about the Turkish political climate to make sure we're not going to see a paradigm shift in the open SSL standard.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57255, "question": "x86 is a high-level language", "aSentId": 57481, "answer": "I've been thinking about this for a while; How there's physically no way to get lowest-level machine access any more. It's strange.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57482, "question": "I've been thinking about this for a while; How there's physically no way to get lowest-level machine access any more. It's strange.", "aSentId": 57483, "answer": "After reading this article, I was surprised at how abstract even machine code is. It really is quite strange.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57484, "question": "After reading this article, I was surprised at how abstract even machine code is. It really is quite strange.", "aSentId": 57485, "answer": "At this point the machine-code language for x86 is mostly just still there for compatibility. It's not practical to change the machine-code language for x86, the only real option for updating is to add new opcodes. I bet that if you go back to the 8086, x86 machine code probably maps extremely well to what the CPU is actually doing. But, at this point CPU's are so far removed from the 8086 that newer Intel CPU's are basically just 'emulating' x86 code on a better instruction set. The big advantage to keeping it a secret instruction set is that Intel is free to make any changes they want to the underlying instruction set to fit it to the hardware design and speed things up, and the computer won't see anything different.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57486, "question": "At this point the machine-code language for x86 is mostly just still there for compatibility. It's not practical to change the machine-code language for x86, the only real option for updating is to add new opcodes. I bet that if you go back to the 8086, x86 machine code probably maps extremely well to what the CPU is actually doing. But, at this point CPU's are so far removed from the 8086 that newer Intel CPU's are basically just 'emulating' x86 code on a better instruction set. The big advantage to keeping it a secret instruction set is that Intel is free to make any changes they want to the underlying instruction set to fit it to the hardware design and speed things up, and the computer won't see anything different.", "aSentId": 57487, "answer": "Yup that's why AMD64 beat IA64 so handily (well, that and it's extremely difficult to write a good compiler targeting IA64). Backwards compatibility is *huge*.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57486, "question": "At this point the machine-code language for x86 is mostly just still there for compatibility. It's not practical to change the machine-code language for x86, the only real option for updating is to add new opcodes. I bet that if you go back to the 8086, x86 machine code probably maps extremely well to what the CPU is actually doing. But, at this point CPU's are so far removed from the 8086 that newer Intel CPU's are basically just 'emulating' x86 code on a better instruction set. The big advantage to keeping it a secret instruction set is that Intel is free to make any changes they want to the underlying instruction set to fit it to the hardware design and speed things up, and the computer won't see anything different.", "aSentId": 57489, "answer": "it's the javascript of assembly language", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57490, "question": "it's the javascript of assembly language", "aSentId": 57491, "answer": "I liked Gary Bernhardt's idea for making a fork of the Linux kernel that runs asm.js as its native executable format. It would make architecture-specific binaries a thing of the past.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57486, "question": "At this point the machine-code language for x86 is mostly just still there for compatibility. It's not practical to change the machine-code language for x86, the only real option for updating is to add new opcodes. I bet that if you go back to the 8086, x86 machine code probably maps extremely well to what the CPU is actually doing. But, at this point CPU's are so far removed from the 8086 that newer Intel CPU's are basically just 'emulating' x86 code on a better instruction set. The big advantage to keeping it a secret instruction set is that Intel is free to make any changes they want to the underlying instruction set to fit it to the hardware design and speed things up, and the computer won't see anything different.", "aSentId": 57493, "answer": "&gt; The big advantage to keeping it a secret instruction set is that Intel is free to make any changes they want to the underlying instruction set to fit it to the hardware design and speed things up, and the computer won't see anything different.\n\nThat's what microcode is for. Which is essentially what's been happening anyways. Old instructions got faster because the processor offers new capabilities which have to be exploited through using new microcode. But the 20 year old program wouldn't notice. New exotic instructions get added because new microcode can support it.\n\nLet people program in microcode and you'll see wizardry. Load 3 registers from the bus at a time? Why not. Open up 3 registers to the data bus? Buy a new CPU.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57482, "question": "I've been thinking about this for a while; How there's physically no way to get lowest-level machine access any more. It's strange.", "aSentId": 57497, "answer": "The CPU is better at optimizing the CPU than you.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57498, "question": "The CPU is better at optimizing the CPU than you.", "aSentId": 57499, "answer": "I prefer to add Speedup Loops to show the CPU who is boss.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57500, "question": "I prefer to add Speedup Loops to show the CPU who is boss.", "aSentId": 57501, "answer": "I put a heater in my case just in case he gets uppity ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57500, "question": "I prefer to add Speedup Loops to show the CPU who is boss.", "aSentId": 57503, "answer": "I just press the TURBO button.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57498, "question": "The CPU is better at optimizing the CPU than you.", "aSentId": 57505, "answer": "The algorithm behind branch prediction how much much of a difference it made in speed when it was implemented always amazes me.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57498, "question": "The CPU is better at optimizing the CPU than you.", "aSentId": 57507, "answer": "Almost certainly, but it could be interesting to see what kind of differences could be had with an optimising compiler that uses benchmarks to work out what really is the fastest way to do various things.  Though, the current system of opcodes signalling intent, and CPU deciphering that into doing only what matters, when it matters, seems to work pretty well, too.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57482, "question": "I've been thinking about this for a while; How there's physically no way to get lowest-level machine access any more. It's strange.", "aSentId": 57510, "answer": "with things like pipelining and multi core architectures, it's probably for the best that most programmers dont get access to micro code. Most programmers don't even have a clue how the processor works let alone how pipelining works and how to handle the different types of hazards.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57511, "question": "with things like pipelining and multi core architectures, it's probably for the best that most programmers dont get access to micro code. Most programmers don't even have a clue how the processor works let alone how pipelining works and how to handle the different types of hazards.", "aSentId": 57512, "answer": "With out of order and all the reordering going on, plus all the optimization to prevent stalls due to cache accesses and other hazards, it would be an absolute disaster for programmers trying to code at such a low level on modern CPUs. It would be a huge step back. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57513, "question": "With out of order and all the reordering going on, plus all the optimization to prevent stalls due to cache accesses and other hazards, it would be an absolute disaster for programmers trying to code at such a low level on modern CPUs. It would be a huge step back. ", "aSentId": 57514, "answer": "For the very vast majority of programmers (myself absolutely included), I agree. But there are some people out there who excel at that kind of stuff. They'd be having loads of fun.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57515, "question": "For the very vast majority of programmers (myself absolutely included), I agree. But there are some people out there who excel at that kind of stuff. They'd be having loads of fun.", "aSentId": 57516, "answer": "something something Mel Kaye", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57518, "question": "&gt;  How there's physically no way to get lowest-level machine access any more. \n\nRegular programmers might be denied access but isn't the micro-code that's running inside the processors working at that lowest-level?\n", "aSentId": 57519, "answer": "Sure, but when you start thinking about that, personally I always begin to wonder, \"I'll bet I could do this better in Verilog on an FPGA\".  But, not everyone likes *that* low of a level.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57520, "question": "Sure, but when you start thinking about that, personally I always begin to wonder, \"I'll bet I could do this better in Verilog on an FPGA\".  But, not everyone likes *that* low of a level.", "aSentId": 57521, "answer": "/r/FPGAmasterrace", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57522, "question": "/r/FPGAmasterrace", "aSentId": 57523, "answer": "I'm disappointed this isn't a thing.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57524, "question": "I'm disappointed this isn't a thing.", "aSentId": 57525, "answer": "The top comment on every thread would be:\n\n\"Yeah, but can it run Crysis?\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57526, "question": "The top comment on every thread would be:\n\n\"Yeah, but can it run Crysis?\"", "aSentId": 57527, "answer": "\"after extensive configuration, an FPGA the size of a pocket calculator can run Crysis very well, but won't be particularly good at anything else\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57528, "question": "\"after extensive configuration, an FPGA the size of a pocket calculator can run Crysis very well, but won't be particularly good at anything else\"", "aSentId": 57529, "answer": "It also takes more than a year to synthesize. And then you forgot to connect the output to anything so it just optimized everything away in the end anyway.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57530, "question": "It also takes more than a year to synthesize. And then you forgot to connect the output to anything so it just optimized everything away in the end anyway.", "aSentId": 57531, "answer": "... it optimized away *everything* and still took a year?!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57532, "question": "... it optimized away *everything* and still took a year?!", "aSentId": 57533, "answer": "Optimizing compilers can be a bit slow.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57532, "question": "... it optimized away *everything* and still took a year?!", "aSentId": 57535, "answer": "Welcome to VHDL synthesizers. They're not very fast.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57532, "question": "... it optimized away *everything* and still took a year?!", "aSentId": 57537, "answer": "Hello Xilinx Vivado!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57526, "question": "The top comment on every thread would be:\n\n\"Yeah, but can it run Crysis?\"", "aSentId": 57539, "answer": "This is how far I have to scroll down to start understanding any of this mumbo jumbo.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57526, "question": "The top comment on every thread would be:\n\n\"Yeah, but can it run Crysis?\"", "aSentId": 57541, "answer": "I'm still constantly impressed that my Nvidia Shield portable runs that (remotely) so damn well.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57524, "question": "I'm disappointed this isn't a thing.", "aSentId": 57543, "answer": "\"Virtex [f]our - be gentle\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57544, "question": "\"Virtex [f]our - be gentle\"", "aSentId": 57545, "answer": "FPGAsgonewild?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57546, "question": "FPGAsgonewild?", "aSentId": 57547, "answer": "If this ever becomes a thing, I would definitely have OC to share.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57524, "question": "I'm disappointed this isn't a thing.", "aSentId": 57549, "answer": "it is, it's just on a BBS", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57522, "question": "/r/FPGAmasterrace", "aSentId": 57551, "answer": "This whole /r/&lt;something&gt;masterrace is starting to become annoying. I've seen it in so many threads over the last couple of days.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57520, "question": "Sure, but when you start thinking about that, personally I always begin to wonder, \"I'll bet I could do this better in Verilog on an FPGA\".  But, not everyone likes *that* low of a level.", "aSentId": 57553, "answer": "Skip Verilog, make your webpage from discrete transistors.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57554, "question": "Skip Verilog, make your webpage from discrete transistors.", "aSentId": 57555, "answer": "LED's and tinfoil is the wave of the new future", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57518, "question": "&gt;  How there's physically no way to get lowest-level machine access any more. \n\nRegular programmers might be denied access but isn't the micro-code that's running inside the processors working at that lowest-level?\n", "aSentId": 57557, "answer": "The micro-code gets subjected to out-of-order execution, so it doesn't really help with the OP's problem of predictability.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57518, "question": "&gt;  How there's physically no way to get lowest-level machine access any more. \n\nRegular programmers might be denied access but isn't the micro-code that's running inside the processors working at that lowest-level?\n", "aSentId": 57559, "answer": "Isn't this obvious? I mean, how could it not be true?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57561, "question": "This is talking about how the x86 spec is implemented in the chip. It's not code that is doing this but transistors. All you can tell the chip is I want this blob of x86 ran and it decides what the output is, in the case of a modern CPU it doesn't really care what order you asked for them in, it just makes sure all the dependency chains that affect that instruction are completed before it finishes the instruction.", "aSentId": 57562, "answer": "&gt;  It's not code that is doing this but transistors\n\nOn a facile level, this was true of Intel's 4004, as well. There was a decode table in the CPU that mapped individual opcodes to particular digital circuits within the CPU. The decode table grew as the the number of instructions and the width of registers grew.\n\nThe article's point is that there is no longer a decode table that maps x86 instructions to digital circuits. Instead, opcodes are translated to microcode, and somewhere in the bowels of the CPU, there is a decode table that translates from microcode opcodes to individual digital circuits.\n\n**TL;DR:** What was opcode ==&gt; decode table ==&gt; circuits is now opcode ==&gt; decode table ==&gt; decode table ==&gt; circuits.\n\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57563, "question": "&gt;  It's not code that is doing this but transistors\n\nOn a facile level, this was true of Intel's 4004, as well. There was a decode table in the CPU that mapped individual opcodes to particular digital circuits within the CPU. The decode table grew as the the number of instructions and the width of registers grew.\n\nThe article's point is that there is no longer a decode table that maps x86 instructions to digital circuits. Instead, opcodes are translated to microcode, and somewhere in the bowels of the CPU, there is a decode table that translates from microcode opcodes to individual digital circuits.\n\n**TL;DR:** What was opcode ==&gt; decode table ==&gt; circuits is now opcode ==&gt; decode table ==&gt; decode table ==&gt; circuits.\n\n", "aSentId": 57564, "answer": "There are still transistors in my CPU right?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57565, "question": "There are still transistors in my CPU right?", "aSentId": 57566, "answer": "Yep. Every digital circuit is a just a collection of transistors. Though I've lost track of how they're made, anymore. When I was a kid, it was all about the PN and NP junctions, and FETs were the up and coming Cool New Thing (tm).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57567, "question": "Yep. Every digital circuit is a just a collection of transistors. Though I've lost track of how they're made, anymore. When I was a kid, it was all about the PN and NP junctions, and FETs were the up and coming Cool New Thing (tm).", "aSentId": 57568, "answer": "Wow, really? Because CMOS rolled out in 1963, which was pretty much the first LSI fabrication technology using MOSFETs. If what you're saying is true, I'd love to see history through your eyes.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57569, "question": "Wow, really? Because CMOS rolled out in 1963, which was pretty much the first LSI fabrication technology using MOSFETs. If what you're saying is true, I'd love to see history through your eyes.", "aSentId": 57570, "answer": "Heh. To clarify, when I was a kid I read books (because there wasn't an Internet, yet) and those books had been published years or decades before.\n\nI was reading about electronics in the late 70s, and the discrete components that I played with were all bipolar junction transistors. Looking back, it occurs to me that *of course* MOS technologies were a thing - because there was a company called \"MOS Technologies\" (they made the CPU that Apple used,) but my recollection is of the books that talked about the new field effect transistors that were coming onto the market in integrated circuits.\n\nAnd now I feel old.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57571, "question": "Heh. To clarify, when I was a kid I read books (because there wasn't an Internet, yet) and those books had been published years or decades before.\n\nI was reading about electronics in the late 70s, and the discrete components that I played with were all bipolar junction transistors. Looking back, it occurs to me that *of course* MOS technologies were a thing - because there was a company called \"MOS Technologies\" (they made the CPU that Apple used,) but my recollection is of the books that talked about the new field effect transistors that were coming onto the market in integrated circuits.\n\nAnd now I feel old.", "aSentId": 57572, "answer": "That's okay. When I was a teen in the early 2000s all the books I had were from the late 70s. The cycle continues. I'm super into computer history, so don't feel old on my behalf. I think that must've been a cool time, so feel wise instead!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57563, "question": "&gt;  It's not code that is doing this but transistors\n\nOn a facile level, this was true of Intel's 4004, as well. There was a decode table in the CPU that mapped individual opcodes to particular digital circuits within the CPU. The decode table grew as the the number of instructions and the width of registers grew.\n\nThe article's point is that there is no longer a decode table that maps x86 instructions to digital circuits. Instead, opcodes are translated to microcode, and somewhere in the bowels of the CPU, there is a decode table that translates from microcode opcodes to individual digital circuits.\n\n**TL;DR:** What was opcode ==&gt; decode table ==&gt; circuits is now opcode ==&gt; decode table ==&gt; decode table ==&gt; circuits.\n\n", "aSentId": 57574, "answer": "I thought the point was about crypto side channel attacks do to an inability to control low level timings.  Fifteen years ago timing analysis and power analysis (including differential power analysis) were a big deal in the smart card world since you could pull the keys out of a chip that was supposed to be secure.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57561, "question": "This is talking about how the x86 spec is implemented in the chip. It's not code that is doing this but transistors. All you can tell the chip is I want this blob of x86 ran and it decides what the output is, in the case of a modern CPU it doesn't really care what order you asked for them in, it just makes sure all the dependency chains that affect that instruction are completed before it finishes the instruction.", "aSentId": 57576, "answer": "&gt; It's not code that is doing this but transistors.\n\nI really can't wrap my head around what you are trying to say here. Do you think the transistors magically understand x86 and just do what they are supposed to do? There is a state machine in the processor that is  responsible for translating x86 instructions (i also think there is an extra step where x86 is translated into it's risc equivalent) into it's microcode which is responsible for telling the data path what to do.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57577, "question": "&gt; It's not code that is doing this but transistors.\n\nI really can't wrap my head around what you are trying to say here. Do you think the transistors magically understand x86 and just do what they are supposed to do? There is a state machine in the processor that is  responsible for translating x86 instructions (i also think there is an extra step where x86 is translated into it's risc equivalent) into it's microcode which is responsible for telling the data path what to do.", "aSentId": 57578, "answer": "As I understand processors of yore, indeed it was just transistors that magically understand x86.  Assembly had 1-to-1 binary equivalents for the instruction path, activating the transistor paths on the ALU, etc. operating on the data path.\n\nI think what sunzoo is saying is that Intel can do whatever the hell it wants, whether it uses microcode or transistors, as long as the x86 standard is adhered to.  It's a black-box.  \n\nTherein is the point of the article... for days-of-yore processors, timing was less black-box-ish cuz there was no microcode.\n\nTo answer sunzoo's parent's question ('is microcode working at the lowest level'), indeed it is.  However, you wouldn't want to modify the microcode cuz you might break the x86 spec and say bye-bye to Windows cuz now your ADD instruction stops working.  If you're in that line of work... have at it.  The microcode instructions are probably some x86_microcode spec, not the x86 spec.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57579, "question": "As I understand processors of yore, indeed it was just transistors that magically understand x86.  Assembly had 1-to-1 binary equivalents for the instruction path, activating the transistor paths on the ALU, etc. operating on the data path.\n\nI think what sunzoo is saying is that Intel can do whatever the hell it wants, whether it uses microcode or transistors, as long as the x86 standard is adhered to.  It's a black-box.  \n\nTherein is the point of the article... for days-of-yore processors, timing was less black-box-ish cuz there was no microcode.\n\nTo answer sunzoo's parent's question ('is microcode working at the lowest level'), indeed it is.  However, you wouldn't want to modify the microcode cuz you might break the x86 spec and say bye-bye to Windows cuz now your ADD instruction stops working.  If you're in that line of work... have at it.  The microcode instructions are probably some x86_microcode spec, not the x86 spec.", "aSentId": 57580, "answer": "IIRC the RISCS were the first to have instructions directly decoded.  Prior to that, everything was microcoded (the state machine /u/penprog mentions).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57581, "question": "IIRC the RISCS were the first to have instructions directly decoded.  Prior to that, everything was microcoded (the state machine /u/penprog mentions).", "aSentId": 57582, "answer": "Some early microprocessors had direct decoding. I had the most experience with the 6502 and it definitely had no microcode. I believe the 6809 did have microcode for some instructions (e.g. multiply and divide).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57583, "question": "Some early microprocessors had direct decoding. I had the most experience with the 6502 and it definitely had no microcode. I believe the 6809 did have microcode for some instructions (e.g. multiply and divide).", "aSentId": 57584, "answer": "I'm not familiar with the 6502, but it probably \"directly decoded\" into microcode.  There are usually 20-40 bits of signals you need to drive - that's what microcode was originally.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57585, "question": "I'm not familiar with the 6502, but it probably \"directly decoded\" into microcode.  There are usually 20-40 bits of signals you need to drive - that's what microcode was originally.", "aSentId": 57586, "answer": "Sorry you got downvoted, because even though you're incorrect I understood what you were thinking.\n\nThis is a mistake of semantics; If the instructions are decoded using what boils down to chains of 2-to-4 decoders and combinational logic, as in super old school CPUs and early, cheap MPUs, then that's 'direct decoding'.  \n\nMicrocoding, on the other hand, is when the instruction code becomes an offset into a small CPU-internal memory block whose data lines fan out to the muxes and what have you that the direct-decoding hardware would be toggling in the other model. There's then a counter which steps through a sequence of control signal states at the instruction's offset. This was first introduced by IBM in order to implement the System/360 family and was too expensive for many cheap late-70s/early-80s MCUs to implement.\n\nMicrocode cores are, of course, way more crazy complex than that description lets on in the real silicon produced this day and age.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57581, "question": "IIRC the RISCS were the first to have instructions directly decoded.  Prior to that, everything was microcoded (the state machine /u/penprog mentions).", "aSentId": 57588, "answer": "I remember from comp architecture that back in the mainframe days there would be a big, cumbersome ISA.  Lower end models would do a lot of the ISA in software.  I suppose before the ISA idea was invented everything was programmed for a specific CPU.  Then RISC came out I guess, and now we're sort of back to the mainframe ISA era where lots of the instructions are translated in microcode.  Let's do the timewarp again.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57579, "question": "As I understand processors of yore, indeed it was just transistors that magically understand x86.  Assembly had 1-to-1 binary equivalents for the instruction path, activating the transistor paths on the ALU, etc. operating on the data path.\n\nI think what sunzoo is saying is that Intel can do whatever the hell it wants, whether it uses microcode or transistors, as long as the x86 standard is adhered to.  It's a black-box.  \n\nTherein is the point of the article... for days-of-yore processors, timing was less black-box-ish cuz there was no microcode.\n\nTo answer sunzoo's parent's question ('is microcode working at the lowest level'), indeed it is.  However, you wouldn't want to modify the microcode cuz you might break the x86 spec and say bye-bye to Windows cuz now your ADD instruction stops working.  If you're in that line of work... have at it.  The microcode instructions are probably some x86_microcode spec, not the x86 spec.", "aSentId": 57590, "answer": "how exactly would you go about modifying the microcode?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57577, "question": "&gt; It's not code that is doing this but transistors.\n\nI really can't wrap my head around what you are trying to say here. Do you think the transistors magically understand x86 and just do what they are supposed to do? There is a state machine in the processor that is  responsible for translating x86 instructions (i also think there is an extra step where x86 is translated into it's risc equivalent) into it's microcode which is responsible for telling the data path what to do.", "aSentId": 57592, "answer": "The state machine is implemented in transistors. If there is another processing pipeline running in parallel to the main instruction pipelines, that is implemented in transistors. Microcode, data path, x86, risc... whatever. It all gets turned into voltages, semiconductors, and metals.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57593, "question": "The state machine is implemented in transistors. If there is another processing pipeline running in parallel to the main instruction pipelines, that is implemented in transistors. Microcode, data path, x86, risc... whatever. It all gets turned into voltages, semiconductors, and metals.", "aSentId": 57594, "answer": "Obviously transistors are doing the work but the way it was written was like the transistors were just magically decoding the logic from the code when in reality the code is what controls the logic and the different switches on the datapath.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57595, "question": "Obviously transistors are doing the work but the way it was written was like the transistors were just magically decoding the logic from the code when in reality the code is what controls the logic and the different switches on the datapath.", "aSentId": 57596, "answer": "Well programmers write the code, so really the programmer controls the CPU. \n\nEven when you get down to assembly and say add these two values and put the answer somewhere the chip is doing a ton of work for you still. Even without considering branch prediction and out of order execution it is doing a large amount of work to track the state of its registers and where it is in the list of commands that it needs to execute. The CPU and transistors are hidden from you behind the x86 byte code, which is hidden from you in assembly, which is hidden from you in C, etc. \n\nThe transistors are no more magic then any other step in the process, but in the end they do the work because they were designed to in the same way every other layer in the stack is. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57561, "question": "This is talking about how the x86 spec is implemented in the chip. It's not code that is doing this but transistors. All you can tell the chip is I want this blob of x86 ran and it decides what the output is, in the case of a modern CPU it doesn't really care what order you asked for them in, it just makes sure all the dependency chains that affect that instruction are completed before it finishes the instruction.", "aSentId": 57598, "answer": "&gt; it just makes sure all the dependency chains that affect that instruction are completed before it finishes the instruction\n\nI was extremely confused as to how CPU's could even run sequential code out-of-order until I read your comment, thanks", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57600, "question": "I'm not sure exactly what you mean by \"lowest-level machine access.\"  Processors have pretty much always tried to hide microarchitectural details from the software (e.g., cache hierarchy--software doesn't get direct access to any particular cache, although there are \"helpers\" like prefetching).  Can you give me an example?", "aSentId": 57601, "answer": "It seems people are referring to back-in-the-day when x86 was just the 8086. No such thing as cache in an MPU setting at that point. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57600, "question": "I'm not sure exactly what you mean by \"lowest-level machine access.\"  Processors have pretty much always tried to hide microarchitectural details from the software (e.g., cache hierarchy--software doesn't get direct access to any particular cache, although there are \"helpers\" like prefetching).  Can you give me an example?", "aSentId": 57603, "answer": "Some architectures let you directly access the cache.\n\nI remember MIPS has a *software-managed TLB*. If a virtual address isn't found in the TLB, it doesn't load it from somewhere else... it raises an exception so the kernel can manually fill the TLB and retry.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57605, "question": "I've been wondering how to rewrite the microcode so I can make the processor accept 6502 instructions instead of x86 instructions.", "aSentId": 57606, "answer": "I'm not sure they put that much flexibility into the microcode.\n\nThe Transmeta processors may have been able to, but they've been discontinued AFAICT.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57608, "question": "Maybe not on x86, but how about the current RISC CPUs? ARM, PPC, SPARC...", "aSentId": 57609, "answer": "As someone noted below, pretty much any modern architecture is going to implement similar techniques (e.g., register renaming) in the microarchitecture.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57613, "question": "It would be cool to see a CPU design that removes some of these layers without hurting performance. It would probably need instruction-level parallelism and dependencies to be explicit rather than extracted by the hardware, and expose the backing register file more directly.\n\nOne design that goes in that direction is the [Mill](http://millcomputing.com/topic/introduction-to-the-mill-cpu-programming-model-2/)- instead of accessing registers by name, it accesses instruction results by relative distance from the current instruction; instructions are grouped into sets that can all run together; these groups are all dispatched statically and in-order, and their results drop onto a queue after they're completed.\n\nAn interesting consequence here is that, because the number/type/latency of pipelines is model-specific, instruction encoding is also model-specific. The instructions are the actual bits that get sent to the pipelines, and the groups correspond exactly to the set of pipelines on that model.\n\nSo while these machine layers were created for performance, they're also there for compatibility between versions/tiers of the CPU, and if you're willing to drop that (maybe through an install-time compile step) you can drop the layers for a potentially huge gain in performance or power usage.", "aSentId": 57614, "answer": "&gt; It would be cool to see a CPU design that removes some of these layers without hurting performance.\n\nDifficult, part of the difficulty is that the selection is dynamic, so about all static approaches are doomed not to be able to get the level of OoO in all cases.\n\nMy understanding is that the Mill tries to attack another point of the performance/power trade-off than high end OoO processor (OoO cost a lot of power in detection of //ism and computations which are not finally used).  Slightly less performance, a lot of less power.  Let's try to invoke /u/igodard ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57615, "question": "&gt; It would be cool to see a CPU design that removes some of these layers without hurting performance.\n\nDifficult, part of the difficulty is that the selection is dynamic, so about all static approaches are doomed not to be able to get the level of OoO in all cases.\n\nMy understanding is that the Mill tries to attack another point of the performance/power trade-off than high end OoO processor (OoO cost a lot of power in detection of //ism and computations which are not finally used).  Slightly less performance, a lot of less power.  Let's try to invoke /u/igodard ", "aSentId": 57616, "answer": "The Mill does have one other killer feature to let it keep up with OoO- while most operations are fixed-latency (so they don't actually need to be dynamically scheduled), memory operations are variable-latency, so the Mill's load operations specify which cycle they should retire on. This way the compiler can statically schedule loads as early as possible, without requiring the CPU to look ahead dynamically or keep track of register renaming.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57613, "question": "It would be cool to see a CPU design that removes some of these layers without hurting performance. It would probably need instruction-level parallelism and dependencies to be explicit rather than extracted by the hardware, and expose the backing register file more directly.\n\nOne design that goes in that direction is the [Mill](http://millcomputing.com/topic/introduction-to-the-mill-cpu-programming-model-2/)- instead of accessing registers by name, it accesses instruction results by relative distance from the current instruction; instructions are grouped into sets that can all run together; these groups are all dispatched statically and in-order, and their results drop onto a queue after they're completed.\n\nAn interesting consequence here is that, because the number/type/latency of pipelines is model-specific, instruction encoding is also model-specific. The instructions are the actual bits that get sent to the pipelines, and the groups correspond exactly to the set of pipelines on that model.\n\nSo while these machine layers were created for performance, they're also there for compatibility between versions/tiers of the CPU, and if you're willing to drop that (maybe through an install-time compile step) you can drop the layers for a potentially huge gain in performance or power usage.", "aSentId": 57618, "answer": "Itanium did some of that. You got to explicitly choose 3 instructions to run in parallel. I think it still did register renaming though.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57613, "question": "It would be cool to see a CPU design that removes some of these layers without hurting performance. It would probably need instruction-level parallelism and dependencies to be explicit rather than extracted by the hardware, and expose the backing register file more directly.\n\nOne design that goes in that direction is the [Mill](http://millcomputing.com/topic/introduction-to-the-mill-cpu-programming-model-2/)- instead of accessing registers by name, it accesses instruction results by relative distance from the current instruction; instructions are grouped into sets that can all run together; these groups are all dispatched statically and in-order, and their results drop onto a queue after they're completed.\n\nAn interesting consequence here is that, because the number/type/latency of pipelines is model-specific, instruction encoding is also model-specific. The instructions are the actual bits that get sent to the pipelines, and the groups correspond exactly to the set of pipelines on that model.\n\nSo while these machine layers were created for performance, they're also there for compatibility between versions/tiers of the CPU, and if you're willing to drop that (maybe through an install-time compile step) you can drop the layers for a potentially huge gain in performance or power usage.", "aSentId": 57620, "answer": "I'd be happy with a compiler that targeted microcode.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57623, "question": "As a computer architect, I don't completely agree or disagree with the title of this article. But reading it, the author is arguing that the underlying microarchitecture of most x86 processors is complex, but microarchitecture is completely separate from the x86 ISA. And just about any modern processor has the same complicated underlying microarchitecture to implement the ISA efficiently.", "aSentId": 57624, "answer": "Indeed, I'm also a (former) computer architect here with a similar experience: tons of people, mainly programmers, I have had to work with do not understand that ISA and microarchitecture refer to 2 (very) different things.  \n\nAfter reading the article, I wanna smack the author with a wet sock though. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57625, "question": "Indeed, I'm also a (former) computer architect here with a similar experience: tons of people, mainly programmers, I have had to work with do not understand that ISA and microarchitecture refer to 2 (very) different things.  \n\nAfter reading the article, I wanna smack the author with a wet sock though. ", "aSentId": 57626, "answer": "It would be interesting to hear your gripes about this article.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57627, "question": "It would be interesting to hear your gripes about this article.", "aSentId": 57628, "answer": "I found the \"reasoning\" the author used to reach the conclusion to be baffling, to say the least. Basically any interface to an out-of-order superscalar machine is a \"high level language.\" \n\nInstructions in the ISA do exactly what they say with respect to their retirement. I have no idea what the author is specifically referring to by \"smooth\" or \"predictable\" execution, but neither of those seem to be exclusive issues to modern aggressively out-of-order designs. Which made the whole \"side-channel\" attack claim not very well substantiated IMO.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57623, "question": "As a computer architect, I don't completely agree or disagree with the title of this article. But reading it, the author is arguing that the underlying microarchitecture of most x86 processors is complex, but microarchitecture is completely separate from the x86 ISA. And just about any modern processor has the same complicated underlying microarchitecture to implement the ISA efficiently.", "aSentId": 57630, "answer": "If I were to translate that for everyone, would it be fine to say that even RISC architectures like ARM and their typical implementations can't guarantee predictability. They can probably only guarantee worst-case timings (for use in real-time systems, for example).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57632, "question": "I understood this after taking a class on programing for the 8086. I had taken a class using a crippled 16 bit microcontroller board using assembly the semester before. When I found out that you can do in line multiplication in x86, I audibly exclaimed \"WHAAAA?\".  I realized how far from true low level I was working.", "aSentId": 57633, "answer": "You can do floating point inline multiplication!\n\nThat took a program on the Z80!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57634, "question": "You can do floating point inline multiplication!\n\nThat took a program on the Z80!", "aSentId": 57635, "answer": "And with loops in tandem with that, who needs C!\n\n\n^(I ^do, ^just ^so ^you ^know.)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57634, "question": "You can do floating point inline multiplication!\n\nThat took a program on the Z80!", "aSentId": 57637, "answer": "Psh. What, were you too broke of a schlub to afford installing a whole separate FPU into your system just to handle this stuff?\n\nJesus, there was a day where MMUs were an actual physical addon. We're in the crazy future.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57632, "question": "I understood this after taking a class on programing for the 8086. I had taken a class using a crippled 16 bit microcontroller board using assembly the semester before. When I found out that you can do in line multiplication in x86, I audibly exclaimed \"WHAAAA?\".  I realized how far from true low level I was working.", "aSentId": 57639, "answer": "Multiplication isn't too hard to implement in hardware. Now division, on the other hand, is something I can't figure out how they did it for the life of me.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57640, "question": "Multiplication isn't too hard to implement in hardware. Now division, on the other hand, is something I can't figure out how they did it for the life of me.", "aSentId": 57641, "answer": "I think the point is \"inline\", meaning that in your code you can just write something like 4\\*eax and the computer will multiply 4 by the register eax for you (or something like that).\n\nThis is very weird when you consider that in assembly language you are supposedly controlling each step of what the CPU does, so who does this extra multiplication?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57642, "question": "I think the point is \"inline\", meaning that in your code you can just write something like 4\\*eax and the computer will multiply 4 by the register eax for you (or something like that).\n\nThis is very weird when you consider that in assembly language you are supposedly controlling each step of what the CPU does, so who does this extra multiplication?", "aSentId": 57643, "answer": "The multiplications are only small powers of two, so they're implemented as bit shifts in simple hardware. Some early x86 processors had dedicated address calculation units, separate from the ALU. This made the LEA (load effective address) instruction a lot faster than performing the same operations with adds and shifts, so a lot of assembly code used LEA for general-purpose calculation.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57632, "question": "I understood this after taking a class on programing for the 8086. I had taken a class using a crippled 16 bit microcontroller board using assembly the semester before. When I found out that you can do in line multiplication in x86, I audibly exclaimed \"WHAAAA?\".  I realized how far from true low level I was working.", "aSentId": 57645, "answer": "Yup, ISA before the 80s or so were actually developed with the intention of being written in by humans. It's crazy to think about. For example, the way arrays work in C was originally basically a thin veneer over one of the addressing modes of the PDP-11.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57646, "question": "Yup, ISA before the 80s or so were actually developed with the intention of being written in by humans. It's crazy to think about. For example, the way arrays work in C was originally basically a thin veneer over one of the addressing modes of the PDP-11.", "aSentId": 57647, "answer": "Not just arrays, C is basically a portable assembler for the PDP ISA. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57648, "question": "Not just arrays, C is basically a portable assembler for the PDP ISA. ", "aSentId": 57649, "answer": "Yeah, it was pretty cool when I was reading the student manual on the v6 sources and found out by accident that the reason pre and post increment and decrement became distinct operators in C was because that's how the PDP ISA handles index registers.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57648, "question": "Not just arrays, C is basically a portable assembler for the PDP ISA. ", "aSentId": 57651, "answer": "I remember looking at some x86 assembly a long while back, and going \"oh woah, this doesn't make any sense at all to me.\"  Then eventually I learned C and eventually when I finally grasped all the implications of pointers and all that, I ended up looking at some x86 assembly again and was like \"oh, hey, that makes a lot of sense!  This is actually pretty easy to follow once I look up these opcodes!\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57653, "question": "I'm going to go ahead and *not* send this to my sophomore Assembly Language students. They're having enough trouble keeping track of the stack during procedure calls; I think this will drive them right to drinking.", "aSentId": 57654, "answer": "They aren't already? you're doing something wrong then", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57653, "question": "I'm going to go ahead and *not* send this to my sophomore Assembly Language students. They're having enough trouble keeping track of the stack during procedure calls; I think this will drive them right to drinking.", "aSentId": 57656, "answer": "[But the Ballmer Peak!](http://xkcd.com/323/)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57658, "question": "As a CS student currently taking an x86 course, I finally understood an entire /r/programming link! I might not quite follow all the C++ or Python talk, and stuff over at /r/java might be too advanced, but today I actually feel like I belong in these subreddits instead of just an outsider looking in.\n\nThanks OP!", "aSentId": 57659, "answer": "Just don't go and try to understand how the x86 works. :P 30+ years of new stuff added, with backwards compatibility etc. makes it extremely complex. This is why i like RISC processors such as ARM.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57660, "question": "Just don't go and try to understand how the x86 works. :P 30+ years of new stuff added, with backwards compatibility etc. makes it extremely complex. This is why i like RISC processors such as ARM.", "aSentId": 57661, "answer": "ARM nowadays is just as complex as x86.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57662, "question": "ARM nowadays is just as complex as x86.", "aSentId": 57663, "answer": "I think the easiest way to judge the complexity of a widely used architecture is to look at the LLVM backend code for that architecture. It's the reason why MSP430 is my favorite architecture at the moment.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57664, "question": "I think the easiest way to judge the complexity of a widely used architecture is to look at the LLVM backend code for that architecture. It's the reason why MSP430 is my favorite architecture at the moment.", "aSentId": 57665, "answer": "Hey msp430 is one of my favorites as well but could you explain 'LLVM backend'?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57666, "question": "Hey msp430 is one of my favorites as well but could you explain 'LLVM backend'?", "aSentId": 57667, "answer": "Note: Everything I say is extremely over simplified and possibly incorrect.\n\nSo LLVM is essentially a library to make it easier to develop compilers. If you use something like Clang, it is commonly called a LLVM frontend. It handles all the C/C++/Obj C parsing/lexing to construct an AST. The AST is then converted to \"LLVM IR\". \n\nThe LLVM backend is what converts the generic(it's not really generic) LLVM IR to an architectures specific assembly (or machine code if the backend implements that).\n\nBy looking at the source code for a specific architectures LLVM backend, you can sort of guess how complicated the architecture is.  E.g. when I look at the x86 backend I have pretty much 0 understanding of what is going on.\n\nI spent a while writing a LLVM backend for a fairly simple (but very non-standard) DSP. The best way to currently write a LLVM backend is essentially to copy from existing ones. Out of all the existing LLVM backends, I'd say that the MSP430 is the \"cleanest\" one, at least IMHO.\n\nYou can find the \"in-tree\" LLVM backends here: https://github.com/llvm-mirror/llvm/tree/master/lib/Target", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57668, "question": "Note: Everything I say is extremely over simplified and possibly incorrect.\n\nSo LLVM is essentially a library to make it easier to develop compilers. If you use something like Clang, it is commonly called a LLVM frontend. It handles all the C/C++/Obj C parsing/lexing to construct an AST. The AST is then converted to \"LLVM IR\". \n\nThe LLVM backend is what converts the generic(it's not really generic) LLVM IR to an architectures specific assembly (or machine code if the backend implements that).\n\nBy looking at the source code for a specific architectures LLVM backend, you can sort of guess how complicated the architecture is.  E.g. when I look at the x86 backend I have pretty much 0 understanding of what is going on.\n\nI spent a while writing a LLVM backend for a fairly simple (but very non-standard) DSP. The best way to currently write a LLVM backend is essentially to copy from existing ones. Out of all the existing LLVM backends, I'd say that the MSP430 is the \"cleanest\" one, at least IMHO.\n\nYou can find the \"in-tree\" LLVM backends here: https://github.com/llvm-mirror/llvm/tree/master/lib/Target", "aSentId": 57669, "answer": "&gt;Note: Everything I say is extremely over simplified and possibly incorrect.\n\nI will upvote by pure instinct any comment that begins with anything as uncommonly lucid as this.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57668, "question": "Note: Everything I say is extremely over simplified and possibly incorrect.\n\nSo LLVM is essentially a library to make it easier to develop compilers. If you use something like Clang, it is commonly called a LLVM frontend. It handles all the C/C++/Obj C parsing/lexing to construct an AST. The AST is then converted to \"LLVM IR\". \n\nThe LLVM backend is what converts the generic(it's not really generic) LLVM IR to an architectures specific assembly (or machine code if the backend implements that).\n\nBy looking at the source code for a specific architectures LLVM backend, you can sort of guess how complicated the architecture is.  E.g. when I look at the x86 backend I have pretty much 0 understanding of what is going on.\n\nI spent a while writing a LLVM backend for a fairly simple (but very non-standard) DSP. The best way to currently write a LLVM backend is essentially to copy from existing ones. Out of all the existing LLVM backends, I'd say that the MSP430 is the \"cleanest\" one, at least IMHO.\n\nYou can find the \"in-tree\" LLVM backends here: https://github.com/llvm-mirror/llvm/tree/master/lib/Target", "aSentId": 57671, "answer": "Huh well TIL what an LLVM is thanks for dumpin some knowledge on me. I'm more of a hardware guy so must of my programming experience is with Arm cortex-m/msp430 in C doing fairly simple stuff. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57672, "question": "Huh well TIL what an LLVM is thanks for dumpin some knowledge on me. I'm more of a hardware guy so must of my programming experience is with Arm cortex-m/msp430 in C doing fairly simple stuff. ", "aSentId": 57673, "answer": "It is not \"an LLVM\". LLVM is the name of a project which does the things described above. See: http://llvm.org/", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57666, "question": "Hey msp430 is one of my favorites as well but could you explain 'LLVM backend'?", "aSentId": 57675, "answer": "I assume he means the specific llvm component that would compile llvm instructions to the respective architecture.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57666, "question": "Hey msp430 is one of my favorites as well but could you explain 'LLVM backend'?", "aSentId": 57677, "answer": "LLVM IR is an ~~bytecode~~ intermediate format, which is created from compiling a program in a high level language like C++, but its architecture independent and to actually run it, different compiled versions have to be produced for different architectures. Now if the architecture is simple and reasonable, the code in LLVM required to create binaries in it is going to be compact. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57678, "question": "LLVM IR is an ~~bytecode~~ intermediate format, which is created from compiling a program in a high level language like C++, but its architecture independent and to actually run it, different compiled versions have to be produced for different architectures. Now if the architecture is simple and reasonable, the code in LLVM required to create binaries in it is going to be compact. ", "aSentId": 57679, "answer": "&gt; LLVM is a bytecode format\n\nNo it definitely is not. LLVM IR is an intermediate representation (thus the name) programming language, which is similar to assembly, but slightly higher-level. And it isn\u2019t fully architecture independent, so LLVM frontends still make architecture dependent code.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57680, "question": "&gt; LLVM is a bytecode format\n\nNo it definitely is not. LLVM IR is an intermediate representation (thus the name) programming language, which is similar to assembly, but slightly higher-level. And it isn\u2019t fully architecture independent, so LLVM frontends still make architecture dependent code.", "aSentId": 57681, "answer": "Corrected, thanks. Bit I remember seeing the LLVM IR referred to as Bytecode all the time, even on some of their own old stuff. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57682, "question": "Corrected, thanks. Bit I remember seeing the LLVM IR referred to as Bytecode all the time, even on some of their own old stuff. ", "aSentId": 57683, "answer": "It has a bitcode (not bytecode) representation which is typically used for link-time optimization, along with a textual one. Neither of those is how it's represented in-memory.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57684, "question": "It has a bitcode (not bytecode) representation which is typically used for link-time optimization, along with a textual one. Neither of those is how it's represented in-memory.", "aSentId": 57685, "answer": "&gt; Neither of those is how it's represented in-memory.\n\nWait, wouldn't it be compiled to native before being loaded into memory for execution ?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57686, "question": "&gt; Neither of those is how it's represented in-memory.\n\nWait, wouldn't it be compiled to native before being loaded into memory for execution ?", "aSentId": 57687, "answer": "I'm not talking about the machine code that's eventually executed. The compiler has an in-memory representation of the IR that's distinct from the bitcode and human-readable text serializations.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57664, "question": "I think the easiest way to judge the complexity of a widely used architecture is to look at the LLVM backend code for that architecture. It's the reason why MSP430 is my favorite architecture at the moment.", "aSentId": 57689, "answer": "Thats... kind of clever.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57662, "question": "ARM nowadays is just as complex as x86.", "aSentId": 57691, "answer": "I don't know about \"just as complex\", but certainly any architecture that grows while maintaining backwards compatibility is going to accumulate a bit of cruft.\n\nx86 is backwards compatible to the 8086 and almost backwards compatible to the 8008. There be baggage.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57692, "question": "I don't know about \"just as complex\", but certainly any architecture that grows while maintaining backwards compatibility is going to accumulate a bit of cruft.\n\nx86 is backwards compatible to the 8086 and almost backwards compatible to the 8008. There be baggage.\n", "aSentId": 57693, "answer": "No, it's not. :)\n\nThey removed \"pop cs\" (0x0f) which used to work on the 8086/8088.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57694, "question": "No, it's not. :)\n\nThey removed \"pop cs\" (0x0f) which used to work on the 8086/8088.", "aSentId": 57695, "answer": "Kind of like C then... everything is still there, except for `gets`.\n\nIf `pop cs` was a one-byte opcode, I can see why they'd remove it - it leaves space for another one-byte opcode, and it was a fairly useless instruction.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57692, "question": "I don't know about \"just as complex\", but certainly any architecture that grows while maintaining backwards compatibility is going to accumulate a bit of cruft.\n\nx86 is backwards compatible to the 8086 and almost backwards compatible to the 8008. There be baggage.\n", "aSentId": 57697, "answer": "Doesn't ARM have about a dozen different (not backwards compatible) instruction sets?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57662, "question": "ARM nowadays is just as complex as x86.", "aSentId": 57699, "answer": "And the x86 processors are just converting their complex instructions to risc instructions that run internaly", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57700, "question": "And the x86 processors are just converting their complex instructions to risc instructions that run internaly", "aSentId": 57701, "answer": "Seems a waste of silicon to do something that could be more cheaply and more flexibly done by a compiler.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57702, "question": "Seems a waste of silicon to do something that could be more cheaply and more flexibly done by a compiler.", "aSentId": 57703, "answer": "Probably, but if you have a business critical piece of software made by a now defunct company that costs upwards of 7 digits to replace that is currently functioning perfectly, would you buy a CPU that didn't support x86?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57704, "question": "Probably, but if you have a business critical piece of software made by a now defunct company that costs upwards of 7 digits to replace that is currently functioning perfectly, would you buy a CPU that didn't support x86?", "aSentId": 57705, "answer": "I'd install an emulator.\n\nOr heck, Microsoft would probably include one in the next version of Windows, for exactly that reason. Then I wouldn't need to do anything at all, I could just use it.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57706, "question": "I'd install an emulator.\n\nOr heck, Microsoft would probably include one in the next version of Windows, for exactly that reason. Then I wouldn't need to do anything at all, I could just use it.", "aSentId": 57707, "answer": "The only problem then would be whether the emulator could run efficiently on the new architecture, lemme take you back to the time of Windows NT 5.0's beta on Itanium where Microsoft produced an emulation layer similar to Rosetta on OS X that allowed x86 based Win32 apps to run on the Itanium processor, whilst it worked Microsoft quickly noticed how \"OMGWTFBBQHAX THIS SHIT BE LAGGINS YO!\" and ditched it because emulating x86 on the Itanium took a lot of work and thus was extremely slow and would look bad.\n\nNow whilst modern hardware is much more powerful and even the Itanium got considerably more powerful as it aged, emulation is still pretty resource intensive, you know those Surface RT tablets with the ARM chip and locked down Win8/8.1 OS? They got jailbroken and an emulation layer was made to run x86 Win32 apps on them, yeah read that statement again. \"OMGWTFBBQHAX THIS SHIT BE LAGGINS YO!\"\n\nWhich in a day and age where battery life is everything and a performance inefficient app is also a power inefficient app, yeah probably wouldn't be included.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57704, "question": "Probably, but if you have a business critical piece of software made by a now defunct company that costs upwards of 7 digits to replace that is currently functioning perfectly, would you buy a CPU that didn't support x86?", "aSentId": 57709, "answer": "In reality it should never be that way though...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57710, "question": "In reality it should never be that way though...", "aSentId": 57711, "answer": "In *theory* it should never be that way. In the real world, this is *always* how it plays out. You must've never supported a corporate IT infrastructure before, because legacy support is the name of the game due to sheer real-world logistics.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57704, "question": "Probably, but if you have a business critical piece of software made by a now defunct company that costs upwards of 7 digits to replace that is currently functioning perfectly, would you buy a CPU that didn't support x86?", "aSentId": 57713, "answer": "Whoever is buying software for millions of dollars without source access should be institutionalized. Or hell, to have *any* mission critical software be proprietary. It is so tremendously irresponsible to put so much faith in the continued existence, good will, and mutual goals of another parties complete control of your software.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57714, "question": "Whoever is buying software for millions of dollars without source access should be institutionalized. Or hell, to have *any* mission critical software be proprietary. It is so tremendously irresponsible to put so much faith in the continued existence, good will, and mutual goals of another parties complete control of your software.", "aSentId": 57715, "answer": "&gt; Or hell, to have any mission critical software be proprietary.\n\nNot a Windows fan I see. Ignoring that, I didn't say the software cost &gt;$1mil, I said the costs to replace, which is where we start seeing some decently priced items (50k base) act as a backbone of a system with deep integration with your other systems where you can't really rip it out and replace it with a competitors product overnight, especially if you have like 5 years worth of developers building out of it, it can start adding up fast.\n\nA really common thing too is in locations like machining shops or HVAC systems for really large buildings where the cost of the equipment is the expensive part, the computer is just a cheap dumb terminal running the software to control it. The cost of the computer is nothing, the cost of the software is nothing, you will be able to use this exactly as it is forever because it serves such a simple function, but the expensive equipment needs this very specific version of OS with a very specific version of the program to perform in spec.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57716, "question": "&gt; Or hell, to have any mission critical software be proprietary.\n\nNot a Windows fan I see. Ignoring that, I didn't say the software cost &gt;$1mil, I said the costs to replace, which is where we start seeing some decently priced items (50k base) act as a backbone of a system with deep integration with your other systems where you can't really rip it out and replace it with a competitors product overnight, especially if you have like 5 years worth of developers building out of it, it can start adding up fast.\n\nA really common thing too is in locations like machining shops or HVAC systems for really large buildings where the cost of the equipment is the expensive part, the computer is just a cheap dumb terminal running the software to control it. The cost of the computer is nothing, the cost of the software is nothing, you will be able to use this exactly as it is forever because it serves such a simple function, but the expensive equipment needs this very specific version of OS with a very specific version of the program to perform in spec.", "aSentId": 57717, "answer": "&gt; Not a Windows fan I see.\n\nA Windows fan, I see.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57718, "question": "&gt; Not a Windows fan I see.\n\nA Windows fan, I see.", "aSentId": 57719, "answer": "&gt; HVAC systems\n\n&gt; A Windows fan\n\nHeh heh.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57702, "question": "Seems a waste of silicon to do something that could be more cheaply and more flexibly done by a compiler.", "aSentId": 57721, "answer": "You operate with the infrastructure you have, not the infrastructure you want. \n\nCase in point: Itanium. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57702, "question": "Seems a waste of silicon to do something that could be more cheaply and more flexibly done by a compiler.", "aSentId": 57723, "answer": "Yup. That's why Intel decided to not do that, and created the IA-64 architecture instead. Did you hear what happened? AMD quickly made the x86_64 instruction set which just wastes silicon to emulate the old x86 machines and everyone bought their CPUs instead.\n\nWe really have no one but ourselves to blame for this.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57724, "question": "Yup. That's why Intel decided to not do that, and created the IA-64 architecture instead. Did you hear what happened? AMD quickly made the x86_64 instruction set which just wastes silicon to emulate the old x86 machines and everyone bought their CPUs instead.\n\nWe really have no one but ourselves to blame for this.", "aSentId": 57725, "answer": "IA-64 failed for other reasons. It was almost there, but failed to actually produce the promised performance benefits (as well as being extremely expensive), and AMD capitalized on Intel's mistake. It's not just a case of \"hurr durr dumb consumers don't know what's good for them\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57724, "question": "Yup. That's why Intel decided to not do that, and created the IA-64 architecture instead. Did you hear what happened? AMD quickly made the x86_64 instruction set which just wastes silicon to emulate the old x86 machines and everyone bought their CPUs instead.\n\nWe really have no one but ourselves to blame for this.", "aSentId": 57727, "answer": "IA-64 turned out not to really deliver on the promises it made anyway. (Not that the idea of stripping away the translation hardware is necessarily doomed, it *is* screaming-and-running-the-opposite-direction-from-Transmeta at least :P)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57724, "question": "Yup. That's why Intel decided to not do that, and created the IA-64 architecture instead. Did you hear what happened? AMD quickly made the x86_64 instruction set which just wastes silicon to emulate the old x86 machines and everyone bought their CPUs instead.\n\nWe really have no one but ourselves to blame for this.", "aSentId": 57729, "answer": "The design to translate CISC to RISC was adopted way before AMD64. Actually, The first x86 CPU doing this was the NexGen's Nx586 (1994) followed by the Intel's Pentium Pro (1995) and AMD's K6 (1997, AMD purchased NexGen). ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57734, "question": "Read about Itanium and Transmeta Crusoe and the other VLIW machines.", "aSentId": 57735, "answer": "The radeon, or maybe geforce was vliw apparently for a short time.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57700, "question": "And the x86 processors are just converting their complex instructions to risc instructions that run internaly", "aSentId": 57741, "answer": "They are probably more of a VLIW than RISC.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57742, "question": "They are probably more of a VLIW than RISC.", "aSentId": 57743, "answer": "Only Transmeta. The rest have RISC-like \u00b5ops and dynamic parallelism.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57700, "question": "And the x86 processors are just converting their complex instructions to risc instructions that run internaly", "aSentId": 57745, "answer": "This isn't really true. Load+op decodes into a single uop, which is very unriscy, and at any rate what makes the chips fast is out of order execution, which any modern RISC has to do as well.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57660, "question": "Just don't go and try to understand how the x86 works. :P 30+ years of new stuff added, with backwards compatibility etc. makes it extremely complex. This is why i like RISC processors such as ARM.", "aSentId": 57748, "answer": "Yeah, I don't necessarily need to know the ins and outs of how it does what it does. I just want to figure out how to get my programs out of the development environment!\n\nMany issues I have boil down to \"Well, I know how I'd solve this in (C++/Java), but that doesn't solve the problem for (friend on internet) unless they're also running it in Eclipse.\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57660, "question": "Just don't go and try to understand how the x86 works. :P 30+ years of new stuff added, with backwards compatibility etc. makes it extremely complex. This is why i like RISC processors such as ARM.", "aSentId": 57750, "answer": "One of my hobbies is writing this little toy OS I have going, and it actively stresses me out when I'm working on it how goddamn complex the 386 model I'm writing for is with the knowledge that that's just the tiniest tip of the iceburg in regards to the actual i5 or what have you that the thing is actually running on. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57658, "question": "As a CS student currently taking an x86 course, I finally understood an entire /r/programming link! I might not quite follow all the C++ or Python talk, and stuff over at /r/java might be too advanced, but today I actually feel like I belong in these subreddits instead of just an outsider looking in.\n\nThanks OP!", "aSentId": 57752, "answer": "Outsider looking in for some time now, I'm glad you made it through the door.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57658, "question": "As a CS student currently taking an x86 course, I finally understood an entire /r/programming link! I might not quite follow all the C++ or Python talk, and stuff over at /r/java might be too advanced, but today I actually feel like I belong in these subreddits instead of just an outsider looking in.\n\nThanks OP!", "aSentId": 57754, "answer": "I had the exact same reaction. XD  I'm a big kid now!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57762, "question": "Just because the CPU isn't executing it with constant time constraints doesn't make it not meet the criteria of a low-level language.\n\nGood content, but lousy conclusion.", "aSentId": 57763, "answer": "Couldn't have said it better myself :P", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57762, "question": "Just because the CPU isn't executing it with constant time constraints doesn't make it not meet the criteria of a low-level language.\n\nGood content, but lousy conclusion.", "aSentId": 57765, "answer": "The very amount of translation done from x86 machine code to the actual mOPs executed by the core makes it significantly higher level than a classic, directly executed RISC or VLIW.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57762, "question": "Just because the CPU isn't executing it with constant time constraints doesn't make it not meet the criteria of a low-level language.\n\nGood content, but lousy conclusion.", "aSentId": 57767, "answer": "It's just the author exercising some artistic license with the term \"high-level language.\" \n\nGood content, good conclusion worded in a way that irritates the excessively pedantic (aka everyone that reads this subreddit).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57769, "question": "You conflated two separate points.", "aSentId": 57770, "answer": "Did jhaluska conflate the points or did the article author? ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57771, "question": "Did jhaluska conflate the points or did the article author? ", "aSentId": 57772, "answer": "&gt; The upshot is this: Intel's x86 is a high-level language. Coding everything up according to Agner Fog's instruction timings still won't produce the predictable, constant-time code you are looking for.\n\nI'd argue the article author implied it, but even if I did conflate the points, I just wanted to point out that the ISA itself is implementation agnostic.  Implementations don't always give you exact timings.  Throw in multi threading and you're in even more trouble.  However the trade off for this variable execution time is a drastic increase in overall performance.\n\nIn the other case, just because there are abstraction layers beneath it, doesn't mean it's a HLL from the CS definition.  The content was good, the conclusion was bad.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57773, "question": "&gt; The upshot is this: Intel's x86 is a high-level language. Coding everything up according to Agner Fog's instruction timings still won't produce the predictable, constant-time code you are looking for.\n\nI'd argue the article author implied it, but even if I did conflate the points, I just wanted to point out that the ISA itself is implementation agnostic.  Implementations don't always give you exact timings.  Throw in multi threading and you're in even more trouble.  However the trade off for this variable execution time is a drastic increase in overall performance.\n\nIn the other case, just because there are abstraction layers beneath it, doesn't mean it's a HLL from the CS definition.  The content was good, the conclusion was bad.", "aSentId": 57774, "answer": "Just to clarify: I'm on your team on this one ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57776, "question": "Speeding up processors with transparent techniques such as out of order execution, pipe lining, and the associated branch prediction will indeed never be a constant advantage. Sometimes even a disadvantage. x86 is still backwards compatible, instructions don't disappear.\n\nAs a result, you can treat a subset of the x86 instruction set as a RISC architecture, only using ~30 basic instructions, and none of the fancy uncertainties will affect you *too* much. But you also miss out on the possible speed increases.\n\nWith that being said, machine instructions still map to a list of microcode instructions. So in a sense, machine code has always been high-level.", "aSentId": 57777, "answer": "What ~30 instruction subset?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57778, "question": "What ~30 instruction subset?", "aSentId": 57779, "answer": "I couldn't tell you because I don't write x86 assembler, I write z/Architecture assembler (z/Arch is also CISC). But basically a couple instructions to load and store registers (RX-RX and RX-ST), a couple to load and store addresses (RX-RX and RX-ST) again. Basic arithmetic, basic conditional branching, etc.\n\nYou don't use all of the auto-iterative instructions. For example in z/Arch; MVI moves one byte, MVC moves multiple bytes. But in the background (processor level, it's still one machine instruction), MVC just iterates MVI's.\n\nPerhaps a bit of a bad example. MVC is useful, and you are still very much in control, even though stuff happens in the background. But you don't *need* it. You'd otherwise write ~7 instructions to iterate over an MVI instruction to get the same effect.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57780, "question": "I couldn't tell you because I don't write x86 assembler, I write z/Architecture assembler (z/Arch is also CISC). But basically a couple instructions to load and store registers (RX-RX and RX-ST), a couple to load and store addresses (RX-RX and RX-ST) again. Basic arithmetic, basic conditional branching, etc.\n\nYou don't use all of the auto-iterative instructions. For example in z/Arch; MVI moves one byte, MVC moves multiple bytes. But in the background (processor level, it's still one machine instruction), MVC just iterates MVI's.\n\nPerhaps a bit of a bad example. MVC is useful, and you are still very much in control, even though stuff happens in the background. But you don't *need* it. You'd otherwise write ~7 instructions to iterate over an MVI instruction to get the same effect.", "aSentId": 57781, "answer": "Is it weird that I think it's fucking badass that you specialize in the internals of a system that harkens back to twenty years before x86 was even a thing?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57782, "question": "Is it weird that I think it's fucking badass that you specialize in the internals of a system that harkens back to twenty years before x86 was even a thing?", "aSentId": 57783, "answer": "#", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57784, "question": "#", "aSentId": 57785, "answer": "Damn straight.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57782, "question": "Is it weird that I think it's fucking badass that you specialize in the internals of a system that harkens back to twenty years before x86 was even a thing?", "aSentId": 57787, "answer": "It harkens back to the 60s, but it's been under constant development. IBM used to run a monopoly doing whatever it wanted. At the advent of consumer grade computing, it went head to head with \"publicly-owned\" (for lack of a better term) consortiums on trying to push technologies (token ring vs ethernet, etc) which they always seemed to lose.\n\nSo they just started improving things that did not communicate with the outside world, in a manner transparent to the developer, to great effect. Processor architecture, crazy amounts of virtualisation (you pass like 15 layers of virtualisation between a program and a hard drive, but it's still screaming fast...)\n\nAnd they run ~2 years behind on implementing open technologies. Mainly because they can't be bothered until after everyone stopped fighting over what protocol/topology/whathaveyou everyone should/would use.\n\nI'm 23, and as a result I'm the equivalent of a unicorn in my branch of work. I thoroughly enjoy it, I don't think I would've bothered to learn as much about the inner workings of my system if I was a C# or a Java programmer.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57788, "question": "It harkens back to the 60s, but it's been under constant development. IBM used to run a monopoly doing whatever it wanted. At the advent of consumer grade computing, it went head to head with \"publicly-owned\" (for lack of a better term) consortiums on trying to push technologies (token ring vs ethernet, etc) which they always seemed to lose.\n\nSo they just started improving things that did not communicate with the outside world, in a manner transparent to the developer, to great effect. Processor architecture, crazy amounts of virtualisation (you pass like 15 layers of virtualisation between a program and a hard drive, but it's still screaming fast...)\n\nAnd they run ~2 years behind on implementing open technologies. Mainly because they can't be bothered until after everyone stopped fighting over what protocol/topology/whathaveyou everyone should/would use.\n\nI'm 23, and as a result I'm the equivalent of a unicorn in my branch of work. I thoroughly enjoy it, I don't think I would've bothered to learn as much about the inner workings of my system if I was a C# or a Java programmer.", "aSentId": 57789, "answer": "I'm 25 and have had a raging boner for computer history and deep architecture since I was twelve or so. I understand your unicorniness. You actually made me feel old in that context of my life, which is new.\n\nEdit: The thing that I find coolest, though I'm sure the whole architecture is a nasty pile of cruft at this point, is that it's the direct result, almost sixty years later, of the single decision to create the 360 family.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57776, "question": "Speeding up processors with transparent techniques such as out of order execution, pipe lining, and the associated branch prediction will indeed never be a constant advantage. Sometimes even a disadvantage. x86 is still backwards compatible, instructions don't disappear.\n\nAs a result, you can treat a subset of the x86 instruction set as a RISC architecture, only using ~30 basic instructions, and none of the fancy uncertainties will affect you *too* much. But you also miss out on the possible speed increases.\n\nWith that being said, machine instructions still map to a list of microcode instructions. So in a sense, machine code has always been high-level.", "aSentId": 57791, "answer": "Dropping all those instructions might save some die space but it might not bring as much of a performance increase as you would hope.\n\nRISC was originally a boost because it enabled pipelining, and CISC CPUs took a long time to catch up. Now that clock speeds are so much higher, the bottleneck is memory access, and more compact instruction encodings (i.e. CISC) have the advantage.\n\nIdeally we'd have a more compact instruction encoding where the instructions are still easily pipelined internally- x86 certainly isn't optimal here, but it definitely takes advantage of pipelining and out-of-order execution.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57792, "question": "Dropping all those instructions might save some die space but it might not bring as much of a performance increase as you would hope.\n\nRISC was originally a boost because it enabled pipelining, and CISC CPUs took a long time to catch up. Now that clock speeds are so much higher, the bottleneck is memory access, and more compact instruction encodings (i.e. CISC) have the advantage.\n\nIdeally we'd have a more compact instruction encoding where the instructions are still easily pipelined internally- x86 certainly isn't optimal here, but it definitely takes advantage of pipelining and out-of-order execution.", "aSentId": 57793, "answer": "Dropping the extraneous/miscellaneous instructions saves almost nothing in terms of die space.  Only encoding space matters, and AMD did this already with AMD64 (aka x86-64).  The weird instructions on the x86 are all emulated using micro-code (i.e., the other basic RISC-like instructions) which ultimately takes up very little die area.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57792, "question": "Dropping all those instructions might save some die space but it might not bring as much of a performance increase as you would hope.\n\nRISC was originally a boost because it enabled pipelining, and CISC CPUs took a long time to catch up. Now that clock speeds are so much higher, the bottleneck is memory access, and more compact instruction encodings (i.e. CISC) have the advantage.\n\nIdeally we'd have a more compact instruction encoding where the instructions are still easily pipelined internally- x86 certainly isn't optimal here, but it definitely takes advantage of pipelining and out-of-order execution.", "aSentId": 57795, "answer": "I'm not talking about dropping instructions from the architecture until x86 is RISC. I'm talking about using only a subset of all available x86 instructions so that the out of order execution and pipelining aren't negatively affected by failed branch prediction (which gets harder as you use more complex instructions).\n\nWhat I perceive this to come down to is using a bare minimum of baisc instructions.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57796, "question": "I'm not talking about dropping instructions from the architecture until x86 is RISC. I'm talking about using only a subset of all available x86 instructions so that the out of order execution and pipelining aren't negatively affected by failed branch prediction (which gets harder as you use more complex instructions).\n\nWhat I perceive this to come down to is using a bare minimum of baisc instructions.", "aSentId": 57797, "answer": "&gt; out of order execution and pipelining aren't negatively affected by failed branch prediction \n\nThose problems are present in RISC as well. Though I agree that a certain set of complex instructions (one example could be branching on the value of something present in memory, the cost of recovering from such a misprediction is going to be enormous) negatively affect OOO, but an instruction being complex doesn't necessarily mean it affects OOO. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57798, "question": "&gt; out of order execution and pipelining aren't negatively affected by failed branch prediction \n\nThose problems are present in RISC as well. Though I agree that a certain set of complex instructions (one example could be branching on the value of something present in memory, the cost of recovering from such a misprediction is going to be enormous) negatively affect OOO, but an instruction being complex doesn't necessarily mean it affects OOO. ", "aSentId": 57799, "answer": "OOO, piping and branch prediction go hand in hand. You can't have piping without good branch prediction or you're going to lose all the speed advantage you gain from piping in the first place. Having to empty your pipeline every other branch because your branch prediction failed to make the right prediction defeats the whole purpose.\n\nIn some scenarios, the branch predictor is going to be wrong consistently There has to be a flaw somewhere. If you have that scenario replaying itself in a loop, you're going to have a bad day. But it's impossible to debug that. Even the assembler programmer will not know that the processor's branch prediction is messing things up. It's on another level than a compiler making bad machine code.\n\nI assume (and do note that I assume, I am by no means an expert, just an enthusiast) that results of simple \"basic\" instructions, and the basic branches that are dependent on those results, are much easier to predict. As a consequence, I'd assume that you're going to run in to less branch prediction misses if you stick to basic instructions.\n\nThis is a case of \"There is a time and place for everything.\" though. The amount of time you'd put in to this versus the time and/or money it'd save on CPU time is not really comparable. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57800, "question": "OOO, piping and branch prediction go hand in hand. You can't have piping without good branch prediction or you're going to lose all the speed advantage you gain from piping in the first place. Having to empty your pipeline every other branch because your branch prediction failed to make the right prediction defeats the whole purpose.\n\nIn some scenarios, the branch predictor is going to be wrong consistently There has to be a flaw somewhere. If you have that scenario replaying itself in a loop, you're going to have a bad day. But it's impossible to debug that. Even the assembler programmer will not know that the processor's branch prediction is messing things up. It's on another level than a compiler making bad machine code.\n\nI assume (and do note that I assume, I am by no means an expert, just an enthusiast) that results of simple \"basic\" instructions, and the basic branches that are dependent on those results, are much easier to predict. As a consequence, I'd assume that you're going to run in to less branch prediction misses if you stick to basic instructions.\n\nThis is a case of \"There is a time and place for everything.\" though. The amount of time you'd put in to this versus the time and/or money it'd save on CPU time is not really comparable. ", "aSentId": 57801, "answer": "&gt; You can't have piping without good branch prediction or you're going to lose all the speed advantage you gain from piping in the first place\n\nActually no, this isn't correct. Branch Prediction is a significantly later addition to a processor designer's toolkit than pipelining. For many kinds of numerical computations (vectorized code is the term, I think) branch prediction doesn't matter at all. Even for general computation, I doubt any normal workloads have more than 20-30% branches. Besides, Pipelining provides a boost irrespective of whether there are branches or not.\n\n\nIn general, the reason pipelining helps is that executing an instruction usually requires many different parts of the CPU (decoding instruction, fetching from memory/registers, maths, writing to memory/registers). Pipelining helps by keeping all parts of the CPU busy as far as possible. So while instruction 1 is using the maths part of the CPU, instruction 2 can fetch from memory and instruction 3 is being decoded. Pipelining provides a significant boost over an unpipelined processor even without any sort of speculative execution. (Branch Prediction, Branch Target Buffer, Data Prediction). And while statements like this can obviously produce counter examples to contradict them, Pipelining is a much more massive boost than branch prediction, going from un-pipelined to pipelined without prediction is a multiplicative boost (I think 20x and more has actually happened in the past), whereas for a typical program (say 10% branches), branch prediction (with say 70% accuracy),and pipeline depth of say 20, provides something like a 1.4x boost. One of the reasons that Branch Prediction has become more important recently is because of the ever-increasing pipeline depths (as you mentioned). But then there are fundamental limits to branch prediction, so there is only a certain limit to which we can push it.\n\n&gt; results of simple \"basic\" instructions, and the basic branches that are dependent on those results, are much easier to predict\n\nWhich I agree with, having complex instructions that branch (weird maths mixed in with branching for example) is going to fuck up your branch prediction. But that doesn't actually mean that \nall complex instructions are going to do that. For example adding a maths instruction like fourier transform is not going to affect your branch prediction at all. \n\nBy the way, I think you should definitely pick up a book (hennessy and patterson was used at my college and is pretty decent) on Comp Arch, its one of the easiest fields in CS to understand (and yet so tricky) and you seem to know a lot of stuff already. :D ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57802, "question": "&gt; You can't have piping without good branch prediction or you're going to lose all the speed advantage you gain from piping in the first place\n\nActually no, this isn't correct. Branch Prediction is a significantly later addition to a processor designer's toolkit than pipelining. For many kinds of numerical computations (vectorized code is the term, I think) branch prediction doesn't matter at all. Even for general computation, I doubt any normal workloads have more than 20-30% branches. Besides, Pipelining provides a boost irrespective of whether there are branches or not.\n\n\nIn general, the reason pipelining helps is that executing an instruction usually requires many different parts of the CPU (decoding instruction, fetching from memory/registers, maths, writing to memory/registers). Pipelining helps by keeping all parts of the CPU busy as far as possible. So while instruction 1 is using the maths part of the CPU, instruction 2 can fetch from memory and instruction 3 is being decoded. Pipelining provides a significant boost over an unpipelined processor even without any sort of speculative execution. (Branch Prediction, Branch Target Buffer, Data Prediction). And while statements like this can obviously produce counter examples to contradict them, Pipelining is a much more massive boost than branch prediction, going from un-pipelined to pipelined without prediction is a multiplicative boost (I think 20x and more has actually happened in the past), whereas for a typical program (say 10% branches), branch prediction (with say 70% accuracy),and pipeline depth of say 20, provides something like a 1.4x boost. One of the reasons that Branch Prediction has become more important recently is because of the ever-increasing pipeline depths (as you mentioned). But then there are fundamental limits to branch prediction, so there is only a certain limit to which we can push it.\n\n&gt; results of simple \"basic\" instructions, and the basic branches that are dependent on those results, are much easier to predict\n\nWhich I agree with, having complex instructions that branch (weird maths mixed in with branching for example) is going to fuck up your branch prediction. But that doesn't actually mean that \nall complex instructions are going to do that. For example adding a maths instruction like fourier transform is not going to affect your branch prediction at all. \n\nBy the way, I think you should definitely pick up a book (hennessy and patterson was used at my college and is pretty decent) on Comp Arch, its one of the easiest fields in CS to understand (and yet so tricky) and you seem to know a lot of stuff already. :D ", "aSentId": 57803, "answer": "I hear what you're saying. I'm going to have to revisit a lot of my claims it seems. But I hope you'll allow me to abuse this moment to get something clarified;\n\nSay I have a program with 20 instructions, the 2nd instruction is a branch that may or may not branch to the 12th instruction. For the case of argument, let's say that our pipe is 10 \"slots\" long. After the branch is pushed on to the pipe, the next instruction to be pushed gets decided by the branch predictor. Now if our branch predictor predicted that no branch would occur, then the processor would keep pushing instructions 3,4,... down the pipe. However, let's assume that it gets the definitive result from the branch, it turns out that it did need to branch to instruction 12. This happens 10 steps later, when the pipe is filled with instructiosn 3,4,... Does it now not have to clear the pipe and start pushing from instruction 12? Wasting a bunch of cycles in the process? This is what leads me to believe that branch prediction is so important when you're dealing with piping, and OOO to an extent. \n\nBut I may be mistaken, hence why I'm asking.\n\nI have had no formal (extensive) education on computer architecture, so I'l definitely check out Hennessy &amp; Patterson's work that you referenced.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57804, "question": "I hear what you're saying. I'm going to have to revisit a lot of my claims it seems. But I hope you'll allow me to abuse this moment to get something clarified;\n\nSay I have a program with 20 instructions, the 2nd instruction is a branch that may or may not branch to the 12th instruction. For the case of argument, let's say that our pipe is 10 \"slots\" long. After the branch is pushed on to the pipe, the next instruction to be pushed gets decided by the branch predictor. Now if our branch predictor predicted that no branch would occur, then the processor would keep pushing instructions 3,4,... down the pipe. However, let's assume that it gets the definitive result from the branch, it turns out that it did need to branch to instruction 12. This happens 10 steps later, when the pipe is filled with instructiosn 3,4,... Does it now not have to clear the pipe and start pushing from instruction 12? Wasting a bunch of cycles in the process? This is what leads me to believe that branch prediction is so important when you're dealing with piping, and OOO to an extent. \n\nBut I may be mistaken, hence why I'm asking.\n\nI have had no formal (extensive) education on computer architecture, so I'l definitely check out Hennessy &amp; Patterson's work that you referenced.", "aSentId": 57805, "answer": "Ah, I think I sort of missed putting this in. But you are absolutely right that Branch Predictors are useful. But, pipelining is an improvement that comes before branch prediction. Pipelining is still very useful without prediction. The thing is the benefit of pipelining is impossible to realize if we talk in terms of cycles. Because Pipelining actually made the cycle time (1/(clock frequency)) much lower in the first place.\n\n\nLets say we have an **unpipelined CPU**, so for every instruction, it takes 1 cycle. But the cycle time is going to be large. Say 1000 ns, this means a frequency of **1 Mhz.** \n\n**In your example, it will take: 20*1000ns = 20 ms**\n\nAfter this we introduce a pipeline with 10 \"slot\"s. (stages like ID(Instruction Decode), RF (Register Fetch), MEM(memory), ALU, WB(write back), each of which could be further broken down). \nNow since each stage is doing much less work than the whole unpipelined processor, we can clock them much faster. Lets say **8 Mhz, i.e. 125 ns clock.**\n\nSo, now we have a **pipelined processor without any branch prediction**. So it stalls until the branch is resolved. For the sake of simplicity lets assume the branch instruction has to complete before the branch can be resolved. This takes 29 clock cycles (lets round to 30). **i.e. 3.75 ms**\n\nNow lets add **branch prediction with 80% accuracy**. Now, I am a bit sleepy, so this might be wrong, but on an average this processor will take 22-23 instructions to go through your code. \nwhich adds up to **2.9 ms**\n\nSo, you see branch predicition is definitely a sizeable speedup, but even without it, pipelining is great. To give a bad car analogy, Pipelining is like the internal combustion engine and Branch prediction is aerodynamics. Both help, but one is much more fundamental.\n\n&gt;  I have had no formal (extensive) education on computer architecture\n\nDon't worry, the most I can brag about is one grad level course in Arch during my undergrad. (Which was in EE and not CS :(, I regret that bad choice everyday)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57806, "question": "Ah, I think I sort of missed putting this in. But you are absolutely right that Branch Predictors are useful. But, pipelining is an improvement that comes before branch prediction. Pipelining is still very useful without prediction. The thing is the benefit of pipelining is impossible to realize if we talk in terms of cycles. Because Pipelining actually made the cycle time (1/(clock frequency)) much lower in the first place.\n\n\nLets say we have an **unpipelined CPU**, so for every instruction, it takes 1 cycle. But the cycle time is going to be large. Say 1000 ns, this means a frequency of **1 Mhz.** \n\n**In your example, it will take: 20*1000ns = 20 ms**\n\nAfter this we introduce a pipeline with 10 \"slot\"s. (stages like ID(Instruction Decode), RF (Register Fetch), MEM(memory), ALU, WB(write back), each of which could be further broken down). \nNow since each stage is doing much less work than the whole unpipelined processor, we can clock them much faster. Lets say **8 Mhz, i.e. 125 ns clock.**\n\nSo, now we have a **pipelined processor without any branch prediction**. So it stalls until the branch is resolved. For the sake of simplicity lets assume the branch instruction has to complete before the branch can be resolved. This takes 29 clock cycles (lets round to 30). **i.e. 3.75 ms**\n\nNow lets add **branch prediction with 80% accuracy**. Now, I am a bit sleepy, so this might be wrong, but on an average this processor will take 22-23 instructions to go through your code. \nwhich adds up to **2.9 ms**\n\nSo, you see branch predicition is definitely a sizeable speedup, but even without it, pipelining is great. To give a bad car analogy, Pipelining is like the internal combustion engine and Branch prediction is aerodynamics. Both help, but one is much more fundamental.\n\n&gt;  I have had no formal (extensive) education on computer architecture\n\nDon't worry, the most I can brag about is one grad level course in Arch during my undergrad. (Which was in EE and not CS :(, I regret that bad choice everyday)", "aSentId": 57807, "answer": "Yes, it makes a lot of sense now!\n\nI knew what pipelining meant, how it works, but the way it saves on time didn't punch through (don't ask me how) until just now.\n\nImagining we never have to clear the pipe, we are effectively speeding up execution by a factor equal to the amount of stages (implying each stage always takes the same amount of cycles to complete) in the pipeline.\n\nAnd at that, having to clear the pipe means that the next instruction will be executed as \"slow\" as executing the same instruction on a non pipelined processor. All following instructions benefit from pipelining again.\n\nI just had the ratio of savings to potential losses completely wrong. Depending on the amount of stages, you need a very shitty branch predictor (shittier than coinflipping) to make pipelining not worth it.\n\nThanks a bunch for taking the time to explain this!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57808, "question": "Yes, it makes a lot of sense now!\n\nI knew what pipelining meant, how it works, but the way it saves on time didn't punch through (don't ask me how) until just now.\n\nImagining we never have to clear the pipe, we are effectively speeding up execution by a factor equal to the amount of stages (implying each stage always takes the same amount of cycles to complete) in the pipeline.\n\nAnd at that, having to clear the pipe means that the next instruction will be executed as \"slow\" as executing the same instruction on a non pipelined processor. All following instructions benefit from pipelining again.\n\nI just had the ratio of savings to potential losses completely wrong. Depending on the amount of stages, you need a very shitty branch predictor (shittier than coinflipping) to make pipelining not worth it.\n\nThanks a bunch for taking the time to explain this!", "aSentId": 57809, "answer": "You asked for a coin to be flipped, so I flipped one for you, the result was: **Tails**\n\n ---- \n\n ^This ^bot's ^messages ^aren't ^checked ^often, ^for ^the ^quickest ^response, ^click ^[here](/message/compose?to=lizardsrock4&amp;subject=CoinBot) ^to ^message ^my ^maker \n\n ^Check ^out ^my ^[source](http://github.com/lizardsrock4)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57810, "question": "You asked for a coin to be flipped, so I flipped one for you, the result was: **Tails**\n\n ---- \n\n ^This ^bot's ^messages ^aren't ^checked ^often, ^for ^the ^quickest ^response, ^click ^[here](/message/compose?to=lizardsrock4&amp;subject=CoinBot) ^to ^message ^my ^maker \n\n ^Check ^out ^my ^[source](http://github.com/lizardsrock4)", "aSentId": 57811, "answer": "&gt; Tails\n\nSure, but do we have to clear the pipe now?\n\n*I NEED TO KNOW!*", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57776, "question": "Speeding up processors with transparent techniques such as out of order execution, pipe lining, and the associated branch prediction will indeed never be a constant advantage. Sometimes even a disadvantage. x86 is still backwards compatible, instructions don't disappear.\n\nAs a result, you can treat a subset of the x86 instruction set as a RISC architecture, only using ~30 basic instructions, and none of the fancy uncertainties will affect you *too* much. But you also miss out on the possible speed increases.\n\nWith that being said, machine instructions still map to a list of microcode instructions. So in a sense, machine code has always been high-level.", "aSentId": 57813, "answer": "What speed increases?  Remember Alpha, PA-RISC, MIPS, PowerPC, and Sparc all had their opportunity to show just how wrong Intel was.  And where are they now?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57814, "question": "What speed increases?  Remember Alpha, PA-RISC, MIPS, PowerPC, and Sparc all had their opportunity to show just how wrong Intel was.  And where are they now?", "aSentId": 57815, "answer": "I was talking about the speed increases that OOO, piping and branch prediction offer for both RISC and CISC.\n\nI never said that RISC is faster than CISC (or vice versa). I'm just saying that **if you want complete control** (very big if, rare case) with transparent mechanics in place to speed up execution, you need to make those transparent mechanics as predictable as possible. I'm arguing that by only using certain instructions, you could come close to that.\n\nThe incredibly vast majority of times you won't need this. It doesn't matter if you're tickling the branch predictor in to getting it wrong time after time. But sometimes you need the branch predictor to be right as much as possible, because you need *really* fast execution. And at that point, you want to cut out as much randomness as possible in the mechanics that are out of your control.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57817, "question": "That's not exactly true. Microcode wasn't a thing prior to the System/360.", "aSentId": 57818, "answer": "I'm actually not aware if consumer grade processors have microcode or not. I know the benefits it has for IBM Mainframes (mainly for IBM itself that is). But for earthly consumers not so much.\n\nWith that being said, after having typed out a comment trying to refute your claims, I must concede. Without microcode there is not truly a lower level. It just sits weird with me that these days an instruction can have a different effect on the processor depending on certain variables that the programmer cannot supply. For some reason that automatically must mean to me that there must be a lower level (even though there is none in processors without microcode).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57819, "question": "I'm actually not aware if consumer grade processors have microcode or not. I know the benefits it has for IBM Mainframes (mainly for IBM itself that is). But for earthly consumers not so much.\n\nWith that being said, after having typed out a comment trying to refute your claims, I must concede. Without microcode there is not truly a lower level. It just sits weird with me that these days an instruction can have a different effect on the processor depending on certain variables that the programmer cannot supply. For some reason that automatically must mean to me that there must be a lower level (even though there is none in processors without microcode).", "aSentId": 57820, "answer": "There's always a lower level from decoders and muxes down to gate structures down to transistor layout down to fabrication technology down to semiconductor chemistry. To abstract is to be human.\n\nAlso, the kind of weird quantum shit that makes you uneasy has been a part of CPUs since the late 60s. Shit ain't new. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57821, "question": "There's always a lower level from decoders and muxes down to gate structures down to transistor layout down to fabrication technology down to semiconductor chemistry. To abstract is to be human.\n\nAlso, the kind of weird quantum shit that makes you uneasy has been a part of CPUs since the late 60s. Shit ain't new. ", "aSentId": 57822, "answer": "Out of order execution, branch prediction and pipelining? Don't think that has been in true production systems until very late (90s or later?). Pipelining by itself is very predictable by the way, but in combination with the other two mechanics, it starts becoming tricky.\n\nAbout your first paragraph; I'm talking about whatever the programmer can feasibly alter to tell the machine what to do. For me that stops at machine instructions.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57823, "question": "Out of order execution, branch prediction and pipelining? Don't think that has been in true production systems until very late (90s or later?). Pipelining by itself is very predictable by the way, but in combination with the other two mechanics, it starts becoming tricky.\n\nAbout your first paragraph; I'm talking about whatever the programmer can feasibly alter to tell the machine what to do. For me that stops at machine instructions.", "aSentId": 57824, "answer": "Branch predictors and prefetchers go back to the 50s. Out-of-Order and pipelining were used in production systems in the 60s. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57819, "question": "I'm actually not aware if consumer grade processors have microcode or not. I know the benefits it has for IBM Mainframes (mainly for IBM itself that is). But for earthly consumers not so much.\n\nWith that being said, after having typed out a comment trying to refute your claims, I must concede. Without microcode there is not truly a lower level. It just sits weird with me that these days an instruction can have a different effect on the processor depending on certain variables that the programmer cannot supply. For some reason that automatically must mean to me that there must be a lower level (even though there is none in processors without microcode).", "aSentId": 57826, "answer": "Microcode was an invention of the 60s. By the end of the 80s there really weren't any more static logic control units in existence.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57828, "question": "High-level? I understand the point, but I wouldn't call it that. Hell, I don't consider C high level.", "aSentId": 57829, "answer": "'Contains a layer of abstraction' would probably be a better phrase. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57830, "question": "'Contains a layer of abstraction' would probably be a better phrase. ", "aSentId": 57831, "answer": "Defining \"high-level\" is more a matter of perspective than anything strictly defined. If you're fooling around with logic gates, then machine code is \"high-level\".", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57832, "question": "Defining \"high-level\" is more a matter of perspective than anything strictly defined. If you're fooling around with logic gates, then machine code is \"high-level\".", "aSentId": 57833, "answer": "Logic gates are high level if you are working with transistors.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57834, "question": "Logic gates are high level if you are working with transistors.", "aSentId": 57835, "answer": "Transistors are high-level if you're an electron?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57836, "question": "Transistors are high-level if you're an electron?", "aSentId": 57837, "answer": "Electrons are high level if your a particle physicists. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57838, "question": "Electrons are high level if your a particle physicists. ", "aSentId": 57839, "answer": "Particle physicists are high level if you're an electron.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57838, "question": "Electrons are high level if your a particle physicists. ", "aSentId": 57841, "answer": "Electrons are pretty widely considered to be fundamental (it'd be a massive shock if they turned out not to be). \n\nEven in string theory each electron is made out of exactly one string.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57842, "question": "Electrons are pretty widely considered to be fundamental (it'd be a massive shock if they turned out not to be). \n\nEven in string theory each electron is made out of exactly one string.", "aSentId": 57843, "answer": "Aaaaaaand the buck stops here.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57838, "question": "Electrons are high level if your a particle physicists. ", "aSentId": 57845, "answer": "[And physicists are high level if you're a mathematician.](https://xkcd.com/435/)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57846, "question": "[And physicists are high level if you're a mathematician.](https://xkcd.com/435/)", "aSentId": 57847, "answer": "Other way around, actually.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57848, "question": "Other way around, actually.", "aSentId": 57849, "answer": "I disagree. Physics is based on a foundation of math, just as chemistry is based on a foundation of physics and so on. This parallels higher level languages being based on a foundation of lower level languages down to a base of machine code.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57850, "question": "I disagree. Physics is based on a foundation of math, just as chemistry is based on a foundation of physics and so on. This parallels higher level languages being based on a foundation of lower level languages down to a base of machine code.", "aSentId": 57851, "answer": "I guess it depends on how you look at it - the comic refers to purity. Mathematics is \"pure\" like, say, a functional language - not concerned with implementation. Physics is \"dirtier\" and concerned with the real world, like x86. Thus mathematics would be a higher-level physics.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57836, "question": "Transistors are high-level if you're an electron?", "aSentId": 57853, "answer": "I feel like we need a version of [that \"purity\" xkcd](https://xkcd.com/435/)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57830, "question": "'Contains a layer of abstraction' would probably be a better phrase. ", "aSentId": 57855, "answer": "Except ISAs have always represented an abstraction (e.g. it hides the cache hierarchy ) so really there is nothing new here in that respect.\r\r\"high level\" is about portability, not abstraction.  Portability implies abstraction but not vice versa.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57856, "question": "Except ISAs have always represented an abstraction (e.g. it hides the cache hierarchy ) so really there is nothing new here in that respect.\r\r\"high level\" is about portability, not abstraction.  Portability implies abstraction but not vice versa.", "aSentId": 57857, "answer": "Except, you know, on the systems way back in the day that didn't *have* caches. That's completely irrelevant at this point, but that statement still isn't absolutely true.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57858, "question": "Except, you know, on the systems way back in the day that didn't *have* caches. That's completely irrelevant at this point, but that statement still isn't absolutely true.", "aSentId": 57859, "answer": "That was just an example.  There are many many others.  If not for the abstractions there would be no point to an ISA in the first place -- you would just start over with each new chip.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57860, "question": "That was just an example.  There are many many others.  If not for the abstractions there would be no point to an ISA in the first place -- you would just start over with each new chip.", "aSentId": 57861, "answer": "Yes, but the point would be that it's the lowest level abstraction that makes the pile of gates into a programmable general purpose computer. Which is what you just said. It's essentially the base abstraction.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57828, "question": "High-level? I understand the point, but I wouldn't call it that. Hell, I don't consider C high level.", "aSentId": 57863, "answer": "C is a high level language for close-to-hardware  people. And a low-level language for CS students.\n\nIt depends on your background and concepts.  \n  \n( Good luck writing cache-aware software in F# ;)\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57828, "question": "High-level? I understand the point, but I wouldn't call it that. Hell, I don't consider C high level.", "aSentId": 57865, "answer": "When there are a dozen odd levels, what is high and what is low is pretty much an arbitrary distinction.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57868, "question": "Any language for which the compiler/interpreter can parallelize without the programmers knowing about it is \"high level\" in my book.", "aSentId": 57869, "answer": "All languages fall somewhere on a spectrum of high and low level.  They're more useful as relative terms IMO than trying to pick an absolute definition.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57868, "question": "Any language for which the compiler/interpreter can parallelize without the programmers knowing about it is \"high level\" in my book.", "aSentId": 57871, "answer": "Sooooo all modern assembly then? Every modern architecture is OoO.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57874, "question": "When we got to x86 in our systems course, my world was shattered.\n\nI thought \"binary code,\" all those zeroes and ones, were complex circuit instructions!\n\nI didn't know they encoded high level instructions such as \"do a * b + c,\" all in one instruction.", "aSentId": 57875, "answer": "Well there still is binary code called machine language (i.e., 0's and 1's); it's just that virtually no one programs in it because it's ridiculously painful.  But assembly code (e.g., mov %r1, %r2) is a layer of abstraction above that.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57877, "question": "Glad to have learned assembly on 68k processors.  x86 is a horror show that I could never stomach long enough to really learn it. ", "aSentId": 57878, "answer": "This is why MIPS is still used pretty heavily to teach basic assembly and computer architecture. Trying to teach it starting with x86 leads to a ton of corner cases and optimization techniques which, while applicable to today's technologies, can get in the way of the underlying theory of why things are the way they are today.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57879, "question": "This is why MIPS is still used pretty heavily to teach basic assembly and computer architecture. Trying to teach it starting with x86 leads to a ton of corner cases and optimization techniques which, while applicable to today's technologies, can get in the way of the underlying theory of why things are the way they are today.", "aSentId": 57880, "answer": "MIPS is popular in academia mainly because lots of schools use the same Patterson/Hennessy architecture book, which uses that ISA prolifically for its examples. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57879, "question": "This is why MIPS is still used pretty heavily to teach basic assembly and computer architecture. Trying to teach it starting with x86 leads to a ton of corner cases and optimization techniques which, while applicable to today's technologies, can get in the way of the underlying theory of why things are the way they are today.", "aSentId": 57882, "answer": "I dunno, I have a pretty easy time understanding x86 assembly code and have written some on occasion.  It's a lot better in 64-bit where you get 8 more integer registers.  But its binary instruction format is a hellish nightmare. I feel like x86-64 would be OK for learning assembly but extremely difficult to target with a compiler.  So it makes sense that universities use a different architecture because presumably students will use it as the target architecture in their compiler class next year.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57884, "question": "This should really be titled \"I Just Learned About Abstractions\".", "aSentId": 57885, "answer": "VAX has a `gcc` instruction if I remember right. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57897, "question": "I really need to learn assembly.", "aSentId": 57898, "answer": "What they are saying, it really isn't that difficult depending on the compiler and environment you are working with.  A lot of that is abstracted away by the compiler and the the os and a host of other things.\n\nIf you know C you can easily work with the high-level x86 machine language that they are talking about in the article.\n\n....   \nNow debugging OS code, writing code that might actually go in the linux kernel, that is a different beast.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57897, "question": "I really need to learn assembly.", "aSentId": 57900, "answer": "You should write an assembler, I just wrote one for the 6502, fun times. :)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57902, "question": "I have a hard time calling a language that doesn't even define function call semantics 'high-level', though I get the point the article was trying to make.", "aSentId": 57903, "answer": "&gt; I have a hard time calling a language that doesn't even define function call semantics 'high-level'\n\nIt certainly defines some stuff, like `push`, `pop`, `call`, `ret`, and even `leave` (like `ret` but also adjusts `SP`).\n\nFun fact: going all pedantic is weird in this context because this is about as much function call semantics as Forth defines, and Forth is supposed to be higher-level than the usual C/C++/Java bunch.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57902, "question": "I have a hard time calling a language that doesn't even define function call semantics 'high-level', though I get the point the article was trying to make.", "aSentId": 57905, "answer": "I'm amazed at how many comments are posted purely to take issue with this one throw-away term, instead of looking at the meat of the article.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57906, "question": "I'm amazed at how many comments are posted purely to take issue with this one throw-away term, instead of looking at the meat of the article.", "aSentId": 57907, "answer": "To be fair, that throw-away is the title of the article. And a key part of the introduction. And the conclusion. Hmm, maybe it wasn't a complete throw-away in the first place.\n\nIt might have been more accurate to call it 'highly abstracted' instead, but I agree that the point about the futility of attempting to code constant-time calculations is clear enough either way.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57908, "question": "To be fair, that throw-away is the title of the article. And a key part of the introduction. And the conclusion. Hmm, maybe it wasn't a complete throw-away in the first place.\n\nIt might have been more accurate to call it 'highly abstracted' instead, but I agree that the point about the futility of attempting to code constant-time calculations is clear enough either way.", "aSentId": 57909, "answer": "What, exactly, does a high-level language have that a low-level language does not? I'd say, if pressed for an answer, that \"abstraction\" is a pretty good answer to that question.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57908, "question": "To be fair, that throw-away is the title of the article. And a key part of the introduction. And the conclusion. Hmm, maybe it wasn't a complete throw-away in the first place.\n\nIt might have been more accurate to call it 'highly abstracted' instead, but I agree that the point about the futility of attempting to code constant-time calculations is clear enough either way.", "aSentId": 57911, "answer": "Did it cause you to have any trouble understanding his meaning? Did it throw you off at all? I saw the title, wondered what it meant, then saw the second sentence of the article and understood, and moved on from there.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57912, "question": "Did it cause you to have any trouble understanding his meaning? Did it throw you off at all? I saw the title, wondered what it meant, then saw the second sentence of the article and understood, and moved on from there.", "aSentId": 57913, "answer": "I didn't have any trouble understanding his meaning once I got past the title, mostly because I'm far too familiar with the things he was talking about, but that doesn't mean others won't be confused or left with a flawed understanding of what the author is intending to describe.\n\nI'd have to agree with Hadrosauroidea, when the meat of what the author intended to say is wrapped in the wrong package and comes to a seemingly inexplicable conclusion you question whether or not even the author truly understands what it is they're trying to say. A lot of people who already understand what's going on will no doubt circle back to the title and say \"hey, the sign on the front door doesn't match what's in the house\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57914, "question": "I didn't have any trouble understanding his meaning once I got past the title, mostly because I'm far too familiar with the things he was talking about, but that doesn't mean others won't be confused or left with a flawed understanding of what the author is intending to describe.\n\nI'd have to agree with Hadrosauroidea, when the meat of what the author intended to say is wrapped in the wrong package and comes to a seemingly inexplicable conclusion you question whether or not even the author truly understands what it is they're trying to say. A lot of people who already understand what's going on will no doubt circle back to the title and say \"hey, the sign on the front door doesn't match what's in the house\"", "aSentId": 57915, "answer": "&gt; mostly because I'm far too familiar with the things he was talking about\n\nI'm not very familiar with assembler, and I thought it was a fairly concise, understandable and well-written piece. The clarity of the writing IMHO ensures that the odds of what you describe happening should be low (but are never zero anyway when dealing with potentially conflicting definitions).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57916, "question": "&gt; mostly because I'm far too familiar with the things he was talking about\n\nI'm not very familiar with assembler, and I thought it was a fairly concise, understandable and well-written piece. The clarity of the writing IMHO ensures that the odds of what you describe happening should be low (but are never zero anyway when dealing with potentially conflicting definitions).", "aSentId": 57917, "answer": "So you agree with the author that what he's describing is sufficient to classify x86 assembly a \"high level language\"?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57918, "question": "So you agree with the author that what he's describing is sufficient to classify x86 assembly a \"high level language\"?", "aSentId": 57919, "answer": "I think it's a fairly logical distinction to make, and I think it's clear from the article what was meant. I find it an intuitive use of the term, personally, so I do agree.\n\nOther commenters seem to disagree with this definition of the phrase entirely, at which point the rest of the article may become meaningless to them. Shared definitions, to the best of my knowledge, are absolutely required for meaningful discourse.\n\nThat said, the author could just replace the term throughout the document with something like 'abstracted language' or some other term, and as long as it's generally agreed on, the article would have meaning.\n\nThe problem is that definitions are political in nature. They are not factual. You have to make choices in defining terms, and you'll probably always have conflict about what they mean. I am in a homogenous environment, and I find that most arguments I'm a part of can eventually be reduced to conflicting definitions. Once you find out where that stems from (i.e. what phrase or word is at the very core of the conflict) it may be possible to address it by choosing shared terminology.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57920, "question": "I think it's a fairly logical distinction to make, and I think it's clear from the article what was meant. I find it an intuitive use of the term, personally, so I do agree.\n\nOther commenters seem to disagree with this definition of the phrase entirely, at which point the rest of the article may become meaningless to them. Shared definitions, to the best of my knowledge, are absolutely required for meaningful discourse.\n\nThat said, the author could just replace the term throughout the document with something like 'abstracted language' or some other term, and as long as it's generally agreed on, the article would have meaning.\n\nThe problem is that definitions are political in nature. They are not factual. You have to make choices in defining terms, and you'll probably always have conflict about what they mean. I am in a homogenous environment, and I find that most arguments I'm a part of can eventually be reduced to conflicting definitions. Once you find out where that stems from (i.e. what phrase or word is at the very core of the conflict) it may be possible to address it by choosing shared terminology.", "aSentId": 57921, "answer": "So, in effect, you're saying the term \"high level language\", when considered in a vacuum and free of any prior historical meaning, is a sufficient descriptor for the fact that today's x86 micro-architectures don't do what one might naively expect nor do they do it in a predictable (timing-wise) fashion.\n\nThe problem is that the prior historical meaning of high level language has never included micro-architectural details, and for good reason. It makes no sense to do so because you'll find that practically all assembly languages out there are high level by this definition and there are very few, if any, \"lower level languages\" and the classification is therefore useless.\n\nIt also doesn't make sense to do so because you'll end up in the following pickle: If today's x86 assembly is a high level language, does that mean that older x86 processors with simpler micro-architectures, were lower level languages? If so, how can that be an accurate description of the language? It's the same language after all, the same instructions, in the same order. It doesn't make sense to conflate the micro-architecture of the processor with the expressiveness of the language, one is separate from the other and the \"level\" of a language has always been used to describe and categorize languages based on things like expressiveness.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57922, "question": "So, in effect, you're saying the term \"high level language\", when considered in a vacuum and free of any prior historical meaning, is a sufficient descriptor for the fact that today's x86 micro-architectures don't do what one might naively expect nor do they do it in a predictable (timing-wise) fashion.\n\nThe problem is that the prior historical meaning of high level language has never included micro-architectural details, and for good reason. It makes no sense to do so because you'll find that practically all assembly languages out there are high level by this definition and there are very few, if any, \"lower level languages\" and the classification is therefore useless.\n\nIt also doesn't make sense to do so because you'll end up in the following pickle: If today's x86 assembly is a high level language, does that mean that older x86 processors with simpler micro-architectures, were lower level languages? If so, how can that be an accurate description of the language? It's the same language after all, the same instructions, in the same order. It doesn't make sense to conflate the micro-architecture of the processor with the expressiveness of the language, one is separate from the other and the \"level\" of a language has always been used to describe and categorize languages based on things like expressiveness.", "aSentId": 57923, "answer": "Not at all - that's not my argument. I'm saying *no* term can be automatically relied on to have a shared meaning and my opinion (and that of others) are not universally shared. Historical details don't change that, and the fact that we don't seem to be in agreement about how logical the term is proves that meanings of phrases can't be relied on without synchronizing these meanings.\n\nEven if you don't agree with the term, the point of the author still stands had he used a different phrase to describe the phenomenon.\n\n&gt; If so, how can that be an accurate description of the language?\n\nIt cannot, universally, because it is not an innate property of a language, but a property within a context. Any sequential language that could have constant-time execution of instructions can be executed in breach of that property as well (not sure if the reverse holds).\n\n&gt; It doesn't make sense to conflate the micro-architecture of the processor with the expressiveness of the language, one is separate from the other and the \"level\" of a language has always been used to describe and categorize languages based on things like expressiveness.\n\nI'm unconvinced that use of the term *is* better defined then given the vague nature of 'expressiveness', even if there are ways to attempt to estimate it. But as I said; definitions are political, not factual. They are choices. I cannot logically claim definitions not to be factual while simultaneously arguing against your chosen definitions.\n\nThe better question is, aside from the terms used, are we in disagreement about the underlying subject, the consequences of the inability to rely on constant-time execution in assembler? From what I can tell, we aren't. Is that right?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57924, "question": "Not at all - that's not my argument. I'm saying *no* term can be automatically relied on to have a shared meaning and my opinion (and that of others) are not universally shared. Historical details don't change that, and the fact that we don't seem to be in agreement about how logical the term is proves that meanings of phrases can't be relied on without synchronizing these meanings.\n\nEven if you don't agree with the term, the point of the author still stands had he used a different phrase to describe the phenomenon.\n\n&gt; If so, how can that be an accurate description of the language?\n\nIt cannot, universally, because it is not an innate property of a language, but a property within a context. Any sequential language that could have constant-time execution of instructions can be executed in breach of that property as well (not sure if the reverse holds).\n\n&gt; It doesn't make sense to conflate the micro-architecture of the processor with the expressiveness of the language, one is separate from the other and the \"level\" of a language has always been used to describe and categorize languages based on things like expressiveness.\n\nI'm unconvinced that use of the term *is* better defined then given the vague nature of 'expressiveness', even if there are ways to attempt to estimate it. But as I said; definitions are political, not factual. They are choices. I cannot logically claim definitions not to be factual while simultaneously arguing against your chosen definitions.\n\nThe better question is, aside from the terms used, are we in disagreement about the underlying subject, the consequences of the inability to rely on constant-time execution in assembler? From what I can tell, we aren't. Is that right?", "aSentId": 57925, "answer": "&gt; The better question is, aside from the terms used, are we in disagreement about the underlying subject, the consequences of the inability to rely on constant-time execution in assembler? From what I can tell, we aren't. Is that right?\n\nI don't think that that is the better question because it seems to me that no one disputes or disagrees with the author's observations. On the other hand, there seem to be plenty of people that disagree with the author's use of the term \"high level language\" in a way that is both unconventional and illogical.\n\nSome terms can be relied on to have shared meaning, otherwise we wouldn't be communicating. By the same token, \"low level languages\", \"high level languages\", and the entire gamut there-in has a shared meaning in the english language in the context of programming languages that is completely orthogonal to what the author is describing. Even if we ignored that and completely put aside the established-by-consensus meaning of \"high level language\" we can judge whether or not the author's usage on it's own merits: is his classification of x86 assembly language as a high level language meaningful? No, it is not, because practically every assembly language is therefore a high level language and neither he nor you can find an assembly language that is a \"lower level language\" because based on his criteria no such language can exist, largely because it is a flawed criteria.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57928, "question": "If you only care about constant time and not performance you could just execute a serializing instruction after every instruction, such as CPUID.", "aSentId": 57929, "answer": "Can this be (reasonably) relied on even for future processors? Why?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57940, "question": "Are there seriously people out there who thought they knew how to program a modern CPU with all that jazz going on under the hood?\n\nBeing a person who's first programming language ever was the super high level ML, I have known this since the first time I read about computer architecture (that course started about five weeks after the SML programming started and had us implement Russian multiplication in assembly and construct arithmetic circuits from transistors).", "aSentId": 57941, "answer": "whose*", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57942, "question": "whose*", "aSentId": 57943, "answer": "hose\n\nthanks though", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57940, "question": "Are there seriously people out there who thought they knew how to program a modern CPU with all that jazz going on under the hood?\n\nBeing a person who's first programming language ever was the super high level ML, I have known this since the first time I read about computer architecture (that course started about five weeks after the SML programming started and had us implement Russian multiplication in assembly and construct arithmetic circuits from transistors).", "aSentId": 57945, "answer": "Given that crypto is used by just about everyone, I'm not shocked about the high level (i.e. abstraction between hardware and software) nature of assembler on a modern CPU, what I'm surprised at is the difficulty involved with bypassing this behavior.\n\nIt seems like a wide opening for any well-funded attacker, and I'd rather this be made more difficult, for instance by being able to selectively turn on constant-time execution for a given process, at the cost of crypto performance of course. It seems to me this would need to be on a hardware level.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57949, "question": "Why Go\u2019s design is a disservice to intelligent programmers", "aSentId": 57950, "answer": "One thing I find criticizable is that the word \"simple\" is overused but never defined. I don't know of a new thing that isn't designed with the hope that it will be simple, or at least simpler than what's already existing, but there are many ways to achieve \"simplicity\".\n\nThis can be linked to something I find utterly stupid in some comments: what is this comparing Go to Python? I can't see a common point between the two past the \"simple\" marketing argument.\n\nGo is a language that is easy to apprehend in its entirety in a short time, and it succeeded in that as noted in the article. This is very comparable to C which also has a \"simple\" syntax in that point of view, and the downside is the same: by not offering much high-level constructs and expecting users to redefine everything themselves you and up with a language in which doing simple things is verbose. Of course you see exactly what happens, and of course it is easy to control exactly what happens, but doing things right is difficult.\n\nTake a word-counting program for example. How easy is that to write? As it turns out, fairly easy. We're nowhere near a one-liner but we get the job done. But did our first example managed errors? Adding that is adding to the boiler plate. And is it working with any kind of encoding? Making sure of that is adding to the boiler plate. And are we reading the file in the best way possible? That depends of the kind of file it is, stdin is not the same as a file on disk, and is not the same as special device files in that matter. Making sure of it is also adding to the code. At the end of the day, the code is nowhere near simple. Each line is easy to understand, but the complexity of the code in its entirety is a problem.\n\nOf course that's why we have a standard library for, providing functions to do this kind of things right, but with a function per type it is either too low-level to effectively reduce the complexity of the code, or too complex for programmers not to shoot themselves in the foot by choosing the wrong function or having to implement the same things over and over again for each type. And if it is to reimplement high-level constructs each time, why not provide them from the beginning.\n\nYou can argue with that. Maybe I should have learned more Go to get to discuss it. What you can't argue with is that this is the complete opposite of the Python philosophy.\n\nPython (or D for what its worth... you should take a real look at this language you now) aims at providing a language in which it is not easy to shoot yourself in the foot because it is taking care of complexity for you.  It's idea of simplicity is that writting stuff that works well should be simpler that writting stuff that doesn't, that a programmer's life is too short to think about low constructs all the time and that we have better to do like getting the actual work done. And it works just fine that way.\n\nOf course Python isn't perfect. Nor is Go. I'm not saying that one is better than the other, different work imply different tools, but I think that comparing them is a mistake. I think that advising Python developpers to check out Go because \u00ab It is just like python but faster \u00bb is a mistake. I too think that Go was made by C programmers for C programmers. Pythonistas should learn C-like languages, it's important and enlightning. But D would be a better suited language for most of them (us).\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57951, "question": "One thing I find criticizable is that the word \"simple\" is overused but never defined. I don't know of a new thing that isn't designed with the hope that it will be simple, or at least simpler than what's already existing, but there are many ways to achieve \"simplicity\".\n\nThis can be linked to something I find utterly stupid in some comments: what is this comparing Go to Python? I can't see a common point between the two past the \"simple\" marketing argument.\n\nGo is a language that is easy to apprehend in its entirety in a short time, and it succeeded in that as noted in the article. This is very comparable to C which also has a \"simple\" syntax in that point of view, and the downside is the same: by not offering much high-level constructs and expecting users to redefine everything themselves you and up with a language in which doing simple things is verbose. Of course you see exactly what happens, and of course it is easy to control exactly what happens, but doing things right is difficult.\n\nTake a word-counting program for example. How easy is that to write? As it turns out, fairly easy. We're nowhere near a one-liner but we get the job done. But did our first example managed errors? Adding that is adding to the boiler plate. And is it working with any kind of encoding? Making sure of that is adding to the boiler plate. And are we reading the file in the best way possible? That depends of the kind of file it is, stdin is not the same as a file on disk, and is not the same as special device files in that matter. Making sure of it is also adding to the code. At the end of the day, the code is nowhere near simple. Each line is easy to understand, but the complexity of the code in its entirety is a problem.\n\nOf course that's why we have a standard library for, providing functions to do this kind of things right, but with a function per type it is either too low-level to effectively reduce the complexity of the code, or too complex for programmers not to shoot themselves in the foot by choosing the wrong function or having to implement the same things over and over again for each type. And if it is to reimplement high-level constructs each time, why not provide them from the beginning.\n\nYou can argue with that. Maybe I should have learned more Go to get to discuss it. What you can't argue with is that this is the complete opposite of the Python philosophy.\n\nPython (or D for what its worth... you should take a real look at this language you now) aims at providing a language in which it is not easy to shoot yourself in the foot because it is taking care of complexity for you.  It's idea of simplicity is that writting stuff that works well should be simpler that writting stuff that doesn't, that a programmer's life is too short to think about low constructs all the time and that we have better to do like getting the actual work done. And it works just fine that way.\n\nOf course Python isn't perfect. Nor is Go. I'm not saying that one is better than the other, different work imply different tools, but I think that comparing them is a mistake. I think that advising Python developpers to check out Go because \u00ab It is just like python but faster \u00bb is a mistake. I too think that Go was made by C programmers for C programmers. Pythonistas should learn C-like languages, it's important and enlightning. But D would be a better suited language for most of them (us).\n", "aSentId": 57952, "answer": "Lisp is simple.\n\nForth is simple.\n\nLanguage PCF is simple.\n\nGo is merely *simplistic*.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57953, "question": "Lisp is simple.\n\nForth is simple.\n\nLanguage PCF is simple.\n\nGo is merely *simplistic*.", "aSentId": 57954, "answer": "Lisp and Forth (No idea about Go) might be simple languages, but you can't get by knowing just the language - you have to learn the idioms and standard library, which are not simple.\n\nI'd rather call them \"minimal\" than \"simple\".", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57955, "question": "Lisp and Forth (No idea about Go) might be simple languages, but you can't get by knowing just the language - you have to learn the idioms and standard library, which are not simple.\n\nI'd rather call them \"minimal\" than \"simple\".", "aSentId": 57956, "answer": "That'd be like saying that Dreamweaver is simpler than Notepad because it's \"simpler\" to create webpages in it. I guess we need to define what simple means, really.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57957, "question": "That'd be like saying that Dreamweaver is simpler than Notepad because it's \"simpler\" to create webpages in it. I guess we need to define what simple means, really.\n", "aSentId": 57958, "answer": "Well yeah, but generally when someone says \"Lisp\" or \"Forth\" they mean the language + standard library, or potentially the entire ecosystem.\n\nSo it's like saying Word is simpler than Notepad+LaTeX.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57955, "question": "Lisp and Forth (No idea about Go) might be simple languages, but you can't get by knowing just the language - you have to learn the idioms and standard library, which are not simple.\n\nI'd rather call them \"minimal\" than \"simple\".", "aSentId": 57960, "answer": "&gt; Lisp and Forth (No idea about Go) might be simple languages, but you can't get by knowing just the language - you have to learn the idioms and standard library, which are not simple.\n\nNot any more than you have to learn the idioms and standard library of any language. You've just forgotten how much it took to learn the idioms of procedural programming because it was so long since you learned it.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57961, "question": "&gt; Lisp and Forth (No idea about Go) might be simple languages, but you can't get by knowing just the language - you have to learn the idioms and standard library, which are not simple.\n\nNot any more than you have to learn the idioms and standard library of any language. You've just forgotten how much it took to learn the idioms of procedural programming because it was so long since you learned it.", "aSentId": 57962, "answer": "I would argue that more complicated languages have more intuitive standard libraries than Lisp or Forth - precisely because, in Lisp and Forth, the standard library has to be shoe-horned into fitting in the language - while in a language with more features, it can use whichever features are more natural for a particular situation.\n\nA trivial example is string concatenation. In C++, because operators have special syntax (complex) and can be overloaded (also complex), it can be done in a natural-seeming way - `a + b`. Whereas in Lisp, you have to write at the very least `(+ a b)`, which is not natural.\n\n(And in practice, you have to write `(concatenate 'string a b)` - although I'm not counting that, since it should be a trivial difference from`(+ a b)`)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57951, "question": "One thing I find criticizable is that the word \"simple\" is overused but never defined. I don't know of a new thing that isn't designed with the hope that it will be simple, or at least simpler than what's already existing, but there are many ways to achieve \"simplicity\".\n\nThis can be linked to something I find utterly stupid in some comments: what is this comparing Go to Python? I can't see a common point between the two past the \"simple\" marketing argument.\n\nGo is a language that is easy to apprehend in its entirety in a short time, and it succeeded in that as noted in the article. This is very comparable to C which also has a \"simple\" syntax in that point of view, and the downside is the same: by not offering much high-level constructs and expecting users to redefine everything themselves you and up with a language in which doing simple things is verbose. Of course you see exactly what happens, and of course it is easy to control exactly what happens, but doing things right is difficult.\n\nTake a word-counting program for example. How easy is that to write? As it turns out, fairly easy. We're nowhere near a one-liner but we get the job done. But did our first example managed errors? Adding that is adding to the boiler plate. And is it working with any kind of encoding? Making sure of that is adding to the boiler plate. And are we reading the file in the best way possible? That depends of the kind of file it is, stdin is not the same as a file on disk, and is not the same as special device files in that matter. Making sure of it is also adding to the code. At the end of the day, the code is nowhere near simple. Each line is easy to understand, but the complexity of the code in its entirety is a problem.\n\nOf course that's why we have a standard library for, providing functions to do this kind of things right, but with a function per type it is either too low-level to effectively reduce the complexity of the code, or too complex for programmers not to shoot themselves in the foot by choosing the wrong function or having to implement the same things over and over again for each type. And if it is to reimplement high-level constructs each time, why not provide them from the beginning.\n\nYou can argue with that. Maybe I should have learned more Go to get to discuss it. What you can't argue with is that this is the complete opposite of the Python philosophy.\n\nPython (or D for what its worth... you should take a real look at this language you now) aims at providing a language in which it is not easy to shoot yourself in the foot because it is taking care of complexity for you.  It's idea of simplicity is that writting stuff that works well should be simpler that writting stuff that doesn't, that a programmer's life is too short to think about low constructs all the time and that we have better to do like getting the actual work done. And it works just fine that way.\n\nOf course Python isn't perfect. Nor is Go. I'm not saying that one is better than the other, different work imply different tools, but I think that comparing them is a mistake. I think that advising Python developpers to check out Go because \u00ab It is just like python but faster \u00bb is a mistake. I too think that Go was made by C programmers for C programmers. Pythonistas should learn C-like languages, it's important and enlightning. But D would be a better suited language for most of them (us).\n", "aSentId": 57964, "answer": "My take on it is that simple is the opposite of complex, and complexity is spread between the source code and the language, where complexity in the source code can be lowered or risen depending on the environment (the team size and code base size in relation to the language's attributes like it's type checking or paradigm).\n\nI think some people prefer their software's complexity to be in their source code, and others in their language.\n\nI think a lot of people who have experience with learning more complex languages feel that those complexities pay off in the long run, and find Go to not be complex enough. I also think people badger it more than other simple languages because it's new and people expect newer languages to have more features, especially a language coming from Google, which people expect to produce powerful/advance products.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57949, "question": "Why Go\u2019s design is a disservice to intelligent programmers", "aSentId": 57966, "answer": "Not even C programmers say \"It's fine not to have generics, just use `void*`\".", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57967, "question": "Not even C programmers say \"It's fine not to have generics, just use `void*`\".", "aSentId": 57968, "answer": "Well, C is 45 years old, so the lack of generics is understandable. What I can't understand is how someone could make that mistake in 2011.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57969, "question": "Well, C is 45 years old, so the lack of generics is understandable. What I can't understand is how someone could make that mistake in 2011.", "aSentId": 57970, "answer": "C11 has something pretty practical for generics", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57971, "question": "C11 has something pretty practical for generics", "aSentId": 57972, "answer": "Do tell. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57971, "question": "C11 has something pretty practical for generics", "aSentId": 57974, "answer": "This is a joke right?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57975, "question": "This is a joke right?", "aSentId": 57976, "answer": "Well, C11 *does* have generics of sorts, but they are not as flexible as e.g. C++ templates. They basically just allow you to define a function that \"switches on the type\".", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57975, "question": "This is a joke right?", "aSentId": 57978, "answer": "C macros ;)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57969, "question": "Well, C is 45 years old, so the lack of generics is understandable. What I can't understand is how someone could make that mistake in 2011.", "aSentId": 57980, "answer": "Pike is completely inflexible.  I discussed this language with him early on.  I pointed out that exceptions were extremely useful for exceptional conditions, and adding a return value with an error code makes it really easy to accidentally throw that return value away - particularly if it is added later.\n\nHis response was, basically, \"Exceptions are stupid, use error codes.\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57981, "question": "Pike is completely inflexible.  I discussed this language with him early on.  I pointed out that exceptions were extremely useful for exceptional conditions, and adding a return value with an error code makes it really easy to accidentally throw that return value away - particularly if it is added later.\n\nHis response was, basically, \"Exceptions are stupid, use error codes.\"", "aSentId": 57982, "answer": "Maybe he said it short, because explaining the whole concept for the n-th time - when it's justification is well written in hundred of articles - is just tiresome. I don't know how your talk looked like however and don't know what excatly he was saying.\n\nAnd your point about exceptions over the returning errors - error codes has been choosen from exactly the oposite reason to yours (it's very easy to accidentally forget about an exception, while you must explicitly ignore error if returned).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57969, "question": "Well, C is 45 years old, so the lack of generics is understandable. What I can't understand is how someone could make that mistake in 2011.", "aSentId": 57984, "answer": "Agreed", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57987, "question": "What I never understood was that C - although it looked like a bootstrap-UNIX-and-throw-away language to me - made it into a standard.", "aSentId": 57988, "answer": "Because the language is more or less irrelevant, it's the platform that counts. All the wide spread languages, from C to javascript to SQL to java have all some very deep flaws, but also were all attached to a very successful platform, and spread with it, becoming a standard of their own.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57989, "question": "Because the language is more or less irrelevant, it's the platform that counts. All the wide spread languages, from C to javascript to SQL to java have all some very deep flaws, but also were all attached to a very successful platform, and spread with it, becoming a standard of their own.", "aSentId": 57990, "answer": "In hindsight, it isn't that irrelevant.\n\nI wonder if people are willing to learn from the past once the next JavaScript/C comes up to discussion.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57967, "question": "Not even C programmers say \"It's fine not to have generics, just use `void*`\".", "aSentId": 57992, "answer": "Bad C programmers do.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57993, "question": "Bad C programmers do.", "aSentId": 57994, "answer": "Yeah but good Go programmers say that about Go, while only bad C programmers say that about C.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57967, "question": "Not even C programmers say \"It's fine not to have generics, just use `void*`\".", "aSentId": 57996, "answer": "libraries like klib and glib use a preprocessor to support generics, which is even worse", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57997, "question": "libraries like klib and glib use a preprocessor to support generics, which is even worse", "aSentId": 57998, "answer": "Go advocates this idiocy as well with its generate tool.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57999, "question": "Go advocates this idiocy as well with its generate tool.", "aSentId": 58000, "answer": "agreed, \"generate\" has a lot of people scratching their heads", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58002, "question": "To be fair, Go programmers don't say that either. The Go designers have said they'll implement generics when and if they think they've found the best way to do it. But they're not going to jump the gun with a design they're unhappy with. For now interface{} is what's available but they haven't discounted generics at all.", "aSentId": 58003, "answer": "That's a bit of a misrepresentation. They've said all that, but they've also said \"it's fine that we don't have generics, just use `interface{}`.\" Go read the Go FAQ.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58005, "question": "Some do, just not many.\n\nI'm not counting the bad ones, but you can't show there are *no good C programmers anywhere* who want generics in C - unless you specifically define \"good\" to exclude those programmers, in which case duh.", "aSentId": 58006, "answer": "Yes, there is probably a good C programmer somewhere who's said that, but that's not the point.\n\nThe point is that the *consensus* among good C programmers is that generics are a useful feature that C doesn't have an adequate replacement for.  The other opinion probably exists among good C programmers but it's not what's widely believed, and that's the important thing.  And that's because it's hard to program in C for 10 years without encountering a situation where generics would really have helped you out and `void*` wasn't satisfactory.\n\nGo's strategy was to use the Google marketing machine to convince people that generics don't matter, which I guess worked?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58007, "question": "Yes, there is probably a good C programmer somewhere who's said that, but that's not the point.\n\nThe point is that the *consensus* among good C programmers is that generics are a useful feature that C doesn't have an adequate replacement for.  The other opinion probably exists among good C programmers but it's not what's widely believed, and that's the important thing.  And that's because it's hard to program in C for 10 years without encountering a situation where generics would really have helped you out and `void*` wasn't satisfactory.\n\nGo's strategy was to use the Google marketing machine to convince people that generics don't matter, which I guess worked?", "aSentId": 58008, "answer": "I guess the C attitude is \"Generics would be nice, but we don't want to undermine the simplicity\"?\n\n(If you add all the features people want to C, you get C++; the programmers who think those features are worth it use C++, and so C programmers are the ones who don't)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58005, "question": "Some do, just not many.\n\nI'm not counting the bad ones, but you can't show there are *no good C programmers anywhere* who want generics in C - unless you specifically define \"good\" to exclude those programmers, in which case duh.", "aSentId": 58010, "answer": "I don't specifically define good to include those programmers, I just think any programmer that thinks that throwing out all type safety is a bad programmer.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58011, "question": "I don't specifically define good to include those programmers, I just think any programmer that thinks that throwing out all type safety is a bad programmer.", "aSentId": 58012, "answer": "It's not \"throwing out all type safety\", it's \"throwing out type safety in some specific situations, but making the language simpler as a result\".\n\nThe \"C programmers\" who would rather have a more complex, safer language are using C++ instead. So if you ask C programmers, they're more likely to be the ones who prefer a simple, less safe language.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58014, "question": "What do they say then?", "aSentId": 58015, "answer": "Giant tables of function pointers, written by hand. Have a look through the source of PETSc some time, object-oriented C is a thing.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58016, "question": "Giant tables of function pointers, written by hand. Have a look through the source of PETSc some time, object-oriented C is a thing.", "aSentId": 58017, "answer": "&gt; object-oriented C is a thing.\n\nofcourse it is, but that still wouldn't give me vector&lt;int&gt; would it?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58018, "question": "&gt; object-oriented C is a thing.\n\nofcourse it is, but that still wouldn't give me vector&lt;int&gt; would it?", "aSentId": 58019, "answer": "It can do. Giant tables of function pointers don't necessarily mean OOP - generics are often implemented a similar way, as are Haskell typeclasses (which aren't OOP classes though there's some similarity with OOP and generic interfaces). Of course as well as having the manually-written vtables for operations on `int` you'll also need to pass your `int` by `void*` reference.\n\nAfter telling you that this is how they write re-usable code in C, some programmers will go on to say how good programming style isn't reserved for xxx language. Apparently we're meant to believe that doing the compilers job for it (but vastly slower and with far more virtually-impossible-to-debug errors) is good programming style.\n\nOf course I should confess I've done the same myself, but that was in a code-generating DSL. The generated code is C++, but supports some tricks that C++ doesn't - with a price to pay for that of course. One of the benefits is multiple dispatch, one of the main prices is that the code generator must know all relevant classes up front to generate the dispatch logic - you lose separate compilation for those classes.\n\nIn my view generated code is target code, not source code. It's nice if you can read it but you're not meant to maintain it directly, you're meant to maintain the source code. That said, there's criticism of \"preprocessors\" for generic code here, and that might not be that far removed from my tool.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58020, "question": "It can do. Giant tables of function pointers don't necessarily mean OOP - generics are often implemented a similar way, as are Haskell typeclasses (which aren't OOP classes though there's some similarity with OOP and generic interfaces). Of course as well as having the manually-written vtables for operations on `int` you'll also need to pass your `int` by `void*` reference.\n\nAfter telling you that this is how they write re-usable code in C, some programmers will go on to say how good programming style isn't reserved for xxx language. Apparently we're meant to believe that doing the compilers job for it (but vastly slower and with far more virtually-impossible-to-debug errors) is good programming style.\n\nOf course I should confess I've done the same myself, but that was in a code-generating DSL. The generated code is C++, but supports some tricks that C++ doesn't - with a price to pay for that of course. One of the benefits is multiple dispatch, one of the main prices is that the code generator must know all relevant classes up front to generate the dispatch logic - you lose separate compilation for those classes.\n\nIn my view generated code is target code, not source code. It's nice if you can read it but you're not meant to maintain it directly, you're meant to maintain the source code. That said, there's criticism of \"preprocessors\" for generic code here, and that might not be that far removed from my tool.\n", "aSentId": 58021, "answer": "&gt;Apparently we're meant to believe that doing the compilers job for it (but vastly slower and with far more virtually-impossible-to-debug errors) is good programming style.\n\nTell that to the lispers...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58022, "question": "&gt;Apparently we're meant to believe that doing the compilers job for it (but vastly slower and with far more virtually-impossible-to-debug errors) is good programming style.\n\nTell that to the lispers...", "aSentId": 58023, "answer": "Hi. Lisper here. Could you expand a bit on what you mean? I think I've got an idea (in which case I do agree to an extent) but I'd rather not assume.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58018, "question": "&gt; object-oriented C is a thing.\n\nofcourse it is, but that still wouldn't give me vector&lt;int&gt; would it?", "aSentId": 58025, "answer": "No, I think you need to provide your own function pointers for any new type you want to extend something to. Which isn't all that different than `void *`ing everything, I guess.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58016, "question": "Giant tables of function pointers, written by hand. Have a look through the source of PETSc some time, object-oriented C is a thing.", "aSentId": 58027, "answer": "This is how the interface for Linux filesystems is implemented (correct me if I'm wrong). It's pretty funky and hard to keep track of IMO, but I do think it's successful in providing a object-oriented way of programming in C, if that's something you need to be able to do.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58014, "question": "What do they say then?", "aSentId": 58029, "answer": "In my experience, something along the lines of \"generics would be nice to have but we're using C rather than C++ because of compiler support, not for fun.\"\n\nPeople generally use C these days because the project is already in C or because they're on embedded platforms without C++ compilers. In the latter case, the C++ compilers don't exist precisely because you have to implement stuff like templates.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58030, "question": "In my experience, something along the lines of \"generics would be nice to have but we're using C rather than C++ because of compiler support, not for fun.\"\n\nPeople generally use C these days because the project is already in C or because they're on embedded platforms without C++ compilers. In the latter case, the C++ compilers don't exist precisely because you have to implement stuff like templates.", "aSentId": 58031, "answer": "&gt; embedded platforms without C++ compilers\n\nWhich platforms are these? Even PIC has a C++ compiler now.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58032, "question": "&gt; embedded platforms without C++ compilers\n\nWhich platforms are these? Even PIC has a C++ compiler now.", "aSentId": 58033, "answer": "Not necessarily GOOD c++ compilers. There's a lot of implicit behaviour in C++ that naive compilers put in there that actually isn't necessary. An optimising compiler can make C++ fast, but a naive C++ compiler is MUCH worse than a naive C compiler.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58032, "question": "&gt; embedded platforms without C++ compilers\n\nWhich platforms are these? Even PIC has a C++ compiler now.", "aSentId": 58035, "answer": "dspace will sell you a \"C++ compiler kit\" addon for a few grand, but it won't do exceptions or RTTI, so there goes a bunch of C++ libraries you might want to use. These are language features you maybe don't want to be using in an embedded context, but it would be nice to at least have a modern toolchain that gives you the choice.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 57949, "question": "Why Go\u2019s design is a disservice to intelligent programmers", "aSentId": 58037, "answer": "I for one think this is a reasonable article. Go's simplicity is a double edged sword. I personally think it's patronising to suggest that the majority of developers can't deal with anything more complex. I do find Go as a language a bit limiting and I'm much more interested in languages such as F#, Haskell and Clojure. Furthermore I'm far from a great developer, I'm very much an average developer.\n\nI mean no disrespect to Rob Pike, clearly the guy is super clever and super talented and go solves the concurrency \"problem\". But when I look at Go I see what it's missing rather than what is has. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58038, "question": "I for one think this is a reasonable article. Go's simplicity is a double edged sword. I personally think it's patronising to suggest that the majority of developers can't deal with anything more complex. I do find Go as a language a bit limiting and I'm much more interested in languages such as F#, Haskell and Clojure. Furthermore I'm far from a great developer, I'm very much an average developer.\n\nI mean no disrespect to Rob Pike, clearly the guy is super clever and super talented and go solves the concurrency \"problem\". But when I look at Go I see what it's missing rather than what is has. ", "aSentId": 58039, "answer": "&gt; [...] Rob Pike, clearly the guy is super clever and super talented [...]\n\nWell, to be perfectly honest, he has had his share of high-profile projects that most people have heard of and no one ever uses. Let's see if he has learned from that or if Go is going the same way.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58040, "question": "&gt; [...] Rob Pike, clearly the guy is super clever and super talented [...]\n\nWell, to be perfectly honest, he has had his share of high-profile projects that most people have heard of and no one ever uses. Let's see if he has learned from that or if Go is going the same way.", "aSentId": 58041, "answer": "Agreed. Who uses UTF-8 anyway?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58040, "question": "&gt; [...] Rob Pike, clearly the guy is super clever and super talented [...]\n\nWell, to be perfectly honest, he has had his share of high-profile projects that most people have heard of and no one ever uses. Let's see if he has learned from that or if Go is going the same way.", "aSentId": 58043, "answer": "Umm, what? \n\nGo is already pretty high profile and people are building things in it.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58044, "question": "Umm, what? \n\nGo is already pretty high profile and people are building things in it.", "aSentId": 58045, "answer": "Its usage really still is tiny though. I could see it being a fad that is quickly abandoned at this point. I can name at least on Go project but...that's it. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58038, "question": "I for one think this is a reasonable article. Go's simplicity is a double edged sword. I personally think it's patronising to suggest that the majority of developers can't deal with anything more complex. I do find Go as a language a bit limiting and I'm much more interested in languages such as F#, Haskell and Clojure. Furthermore I'm far from a great developer, I'm very much an average developer.\n\nI mean no disrespect to Rob Pike, clearly the guy is super clever and super talented and go solves the concurrency \"problem\". But when I look at Go I see what it's missing rather than what is has. ", "aSentId": 58047, "answer": "&gt; and go solves the concurrency \"problem\"\n\nExcept that it doesn't really. It still allows for passing mutable state between goroutines, meaning it does not prevent data races. Check out Rust for a superior approach.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58048, "question": "&gt; and go solves the concurrency \"problem\"\n\nExcept that it doesn't really. It still allows for passing mutable state between goroutines, meaning it does not prevent data races. Check out Rust for a superior approach.", "aSentId": 58049, "answer": "I like the idea of Rust, but it's probably too much of a \"systems\" language for me and is probably too procedural. I'm favouring immutability and functional programming more and more recently and I think from what I've read that Rust is firmly imperative or even OO.\n\nI do think Go had a fantastic approach to concurrency and although some mutable state may be possible, it should be discouraged and developers should be credited with the nous to avoid it. I don't think that Go has the final say on concurrency as for example **Erlang** is pretty amazing in this respect and I believe that Haskell has a number of mature concurrency options.  ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58050, "question": "I like the idea of Rust, but it's probably too much of a \"systems\" language for me and is probably too procedural. I'm favouring immutability and functional programming more and more recently and I think from what I've read that Rust is firmly imperative or even OO.\n\nI do think Go had a fantastic approach to concurrency and although some mutable state may be possible, it should be discouraged and developers should be credited with the nous to avoid it. I don't think that Go has the final say on concurrency as for example **Erlang** is pretty amazing in this respect and I believe that Haskell has a number of mature concurrency options.  ", "aSentId": 58051, "answer": "&gt; from what I've read that Rust is firmly imperative or even OO.\n\nCouldn't be further from the truth. rust is immutable by default and has tons of support for functional programming. The hard part about rust is the ownership system.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58052, "question": "&gt; from what I've read that Rust is firmly imperative or even OO.\n\nCouldn't be further from the truth. rust is immutable by default and has tons of support for functional programming. The hard part about rust is the ownership system.", "aSentId": 58053, "answer": "OO? Naw, Rust has polymorphism via trait objects, but no inheritance. (I wish it did.)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58038, "question": "I for one think this is a reasonable article. Go's simplicity is a double edged sword. I personally think it's patronising to suggest that the majority of developers can't deal with anything more complex. I do find Go as a language a bit limiting and I'm much more interested in languages such as F#, Haskell and Clojure. Furthermore I'm far from a great developer, I'm very much an average developer.\n\nI mean no disrespect to Rob Pike, clearly the guy is super clever and super talented and go solves the concurrency \"problem\". But when I look at Go I see what it's missing rather than what is has. ", "aSentId": 58055, "answer": "I agree with your stance on Go, but I think you're overestimating the average developer. The average developer hasn't even heard of F# or Haskell", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58056, "question": "I agree with your stance on Go, but I think you're overestimating the average developer. The average developer hasn't even heard of F# or Haskell", "aSentId": 58057, "answer": "But that shows a lack of exposure, not competence. You could argue that both go hand in hand, as in, competent programmers are competent because they tend to hang around other programmers to interchange ideas (including programming languages) while incompetent ones just want to get the job done with the tools and languages their employer makes them use and then go home to do something else, without having much outsider perspective. I don't know if I can side with that line of thinking.\n\nMany, many competent programmers have not heard about languages like Haskell or F# (or Clojure, or even Scala) simply because they couldn't care less about them. Now, how can they not care about a language they have not heard about? Because they only care about the \"proven\", industry-adopted languages like Java, C#, C++, Python, PHP, Ruby (recently), JavaScript for front-end webdev, etc etc etc.\n\nYes, maybe there is this new hip language that is perfect for them, but they don't really feel compelled to find it and adopt it now, because that would mean having to sift through several languages that at the end of the day are nicer but not worth the effort of learning them up to a competent level if you only want them to get shit done and paid. If said hip language becomes mainstream in 10 years, and represents a **proven** benefit in any area in comparisson to the languages they already know, they will probably look into it. But they will not look into the other 100 languages that were born around the same time and ultimately failed. We are, in a sense, their beta testers for those languages.\n\nI guess what I'm trying to say is this: just because John has not heard of X and Terry has, it doesn't mean that one year from now John is *[EDIT: not]* not going to be much better at X than Terry is.\n\nYes, I know, both F# and Haskell don't exactly fit the criteria of shiny new, hip language, but still. Haskell, for example, is still largely academic and lacks adoption aside from a handful, specific celebrated cases. You can tell this is true because every time somebody remotely recognizable writes a fart-producing snippet in Haskell you are going to see houndreds of blog posts. I respect their enthusiasm, but I'm still not convinced there is a major adoption of Haskell just beacuse company X and Y use it for the minimal Z task. So I understand John for not caring about it, even if it's a nice language, because chances are John is never going to use it to put food in his mouth.\n\nI really went off on a tangent there.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58056, "question": "I agree with your stance on Go, but I think you're overestimating the average developer. The average developer hasn't even heard of F# or Haskell", "aSentId": 58059, "answer": "They've probably heard of Haskell, not so much F#.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58056, "question": "I agree with your stance on Go, but I think you're overestimating the average developer. The average developer hasn't even heard of F# or Haskell", "aSentId": 58061, "answer": "The same average developer has never heard of Go either. And if that so called average developer is comfortable with Java or C# he will easily be able to tell what a piece of shit Go is as a language.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58062, "question": "The same average developer has never heard of Go either. And if that so called average developer is comfortable with Java or C# he will easily be able to tell what a piece of shit Go is as a language.", "aSentId": 58063, "answer": "When I read this article's title I thought it was talking about the Chinese board game. There's so many languages nowadays. I usually just take a quick look at them and either go \"neat\" or, more frequently, \"not for me.\" But until they get enough traction, I'm not going to spend much time on them. Rust is the only language I'm paying more attention to other than C++.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58065, "question": "&gt; I mean no disrespect to Rob Pike\n\nMe neither, but Unicode turned out to be a major disappointment as well. Technically superior to existing alternatives, but the nightmare to handle it makes it nearly not worth it and the steering committee can be smoked.", "aSentId": 58066, "answer": "Pike invented UTF-8, but I don't think he's guilty of Unicode in general. UTF-8 is a good thing.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58069, "question": "    .reduce!(\"a + b\")\n\nwtf? in a string?  surely there are anon functions or something in d?  what's a template parameter?", "aSentId": 58070, "answer": "That can also be written as\n\n    .reduce!((a, b) =&gt; a + b)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58069, "question": "    .reduce!(\"a + b\")\n\nwtf? in a string?  surely there are anon functions or something in d?  what's a template parameter?", "aSentId": 58072, "answer": "Yes: D does support lambda functions. Using strings to generate lambdas like this is still supported but is now discouraged by the language designers.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58074, "question": "&gt;What a condescending view on your own employees.\n\nHe's just being realistic. If you hire mostly graduates fresh from college you can't really expect them to have 10s of years of experience. \n\nOn Go itself - it's a very opinionated language. If you can fully buy into Pike's vision then you will be pretty happy with it and the community. If your opinions about language design differ though you should leave because you might get a brain aneurysm :)\n", "aSentId": 58075, "answer": "What most of the fresh batch of programmers have is limited experience with designing and writing new programs in a few languages, and little else.\n\nThey have little experience with the full software life-cycle. They may have met and solved a few simply program bugs, but not magnificently elusive bugs that don't crash a program, and can't be reproduced on demand. Likewise, they have little experience with modifying a correct program to add new features without breaking it, or impacting performance. Management might like to think that you can write and deploy a large system for megabucks, and maintain it for pennies, but the reality is that many of those large systems are almost too complex and obscure to maintain at all.\n\nI've often maintained that APL and Perl are write-only languages. It is easier to rewrite a failing program than to repair it, and forget about enhancing it without a rewrite.\n\nC++ with heavy template use and complex class structures, libraries without symbols, etc. can be extraordinarily difficult to understand, even with the best development and debugging tools. And we all know that the best designers and developers do not want to do that job.\n\nRight now, I am looking at some code I wrote more than 15 years ago, in Python 1.5.2 and upgraded to Python 2.0 in 2001. I don't anticipate any problems upgrading it to Python 3.5 soon. By that time, I had learned that good programs seem to last forever, and I may be the one that has to maintain them later.\n\nBut, I have been thinking about migrating to a compiled language. The ability to write simple, easy to understand, easy to modify, and maintain code are my highest priorities. For that, Go looks like the best choice, with C as a runner up.\n\nOOP is a way of thinking about the problem and designing a solution. Go, C, even machine language will let me implement that solution.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58078, "question": "so you're telling me that Go is a tool that will allow mere average programmers to produce better results than they would with a lesser type systems like python or a pile of mistakes like javascript??\n\nso, shouldn't we all be overjoyed?\n\nwho cares what the top 1% do? i know you write machine learning kernels in ARM assembly when you aren't writing your computer vision agent in Coq. being in the 1%, i know you work alone and don't reuse anyone else's (inferior) libraries, so network effects are irrelevant\n\nfor the rest of us, a tool that is approachable by the masses but yields superior results on average is a huge deal. James Gosling got this, and the last dozen Lisp fanatics blew their gasket when Java ate the world....", "aSentId": 58079, "answer": "&gt;James Gosling got this\n\nHah.  But Java is arguably a much more advanced language than Go! ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58080, "question": "&gt;James Gosling got this\n\nHah.  But Java is arguably a much more advanced language than Go! ", "aSentId": 58081, "answer": "Give Go another 10-15 years.  Current Go is sort of a cross between Java 1.2 and C89.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58082, "question": "Give Go another 10-15 years.  Current Go is sort of a cross between Java 1.2 and C89.", "aSentId": 58083, "answer": "Hindsight isn't 20/20 when you don't ever look backwards! ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58082, "question": "Give Go another 10-15 years.  Current Go is sort of a cross between Java 1.2 and C89.", "aSentId": 58085, "answer": "But Go has had 10-15 years, yet its creators chose to ignore any advance and learnings from the programming language field from these past two decades.\n\nGo would have been a killer language in the late 90s.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58080, "question": "&gt;James Gosling got this\n\nHah.  But Java is arguably a much more advanced language than Go! ", "aSentId": 58087, "answer": "i don't have a bone to pick with Java, its a fine solution for lots of people\n\nit has become a big-ish language though, and it isn't as easy to exploit concurrency vs Go's builtins like select...but sure, if you are an awesome happy Java programmer, you might not have your world rocked by Go", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58088, "question": "i don't have a bone to pick with Java, its a fine solution for lots of people\n\nit has become a big-ish language though, and it isn't as easy to exploit concurrency vs Go's builtins like select...but sure, if you are an awesome happy Java programmer, you might not have your world rocked by Go", "aSentId": 58089, "answer": "Does go have something like async/await?\n\nExploiting concurrency is fairly easy with Tasks/Promises, but async is a pain in my ass without language support.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58090, "question": "Does go have something like async/await?\n\nExploiting concurrency is fairly easy with Tasks/Promises, but async is a pain in my ass without language support.", "aSentId": 58091, "answer": "yep it's called goroutines and channels if you want to look it up. It's stupid easy to use and works amazingly. Although i guess this is a bad thing according to the author of the article although he didn't compare D to anything go was made for...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58092, "question": "yep it's called goroutines and channels if you want to look it up. It's stupid easy to use and works amazingly. Although i guess this is a bad thing according to the author of the article although he didn't compare D to anything go was made for...", "aSentId": 58093, "answer": "in the article he clearly says he like go's concurrency support\n\n&gt; apart from the concurrency support (which is excellent by the way)\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58094, "question": "in the article he clearly says he like go's concurrency support\n\n&gt; apart from the concurrency support (which is excellent by the way)\n", "aSentId": 58095, "answer": "Yes he does. Sorry I was referring to code examples as I imagine doing the same concurrency in D would look extremely more complex.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58088, "question": "i don't have a bone to pick with Java, its a fine solution for lots of people\n\nit has become a big-ish language though, and it isn't as easy to exploit concurrency vs Go's builtins like select...but sure, if you are an awesome happy Java programmer, you might not have your world rocked by Go", "aSentId": 58097, "answer": "&gt; it has become a big-ish language though\n\nDo you mean, you think Java has a lot of language features/constructs? ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58098, "question": "&gt; it has become a big-ish language though\n\nDo you mean, you think Java has a lot of language features/constructs? ", "aSentId": 58099, "answer": "compare building and deploying code...that's one example of a problem that is complex in Java relative to Go", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58100, "question": "compare building and deploying code...that's one example of a problem that is complex in Java relative to Go", "aSentId": 58101, "answer": "The dependency versioning critique in the article is a serious problem though, unless you use the only-works-for-Google workflow of forking or controlling every single one of your dependencies.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58100, "question": "compare building and deploying code...that's one example of a problem that is complex in Java relative to Go", "aSentId": 58103, "answer": "Wat.\n\nDeploying code in java is not hard at all.  Of all the many problems with java that's not one of them. \n\nBuilding code however, well, sure, Go is simpler, because Go is just *broken*.   Maven/Ivy is complex because dependency management is complex.  Go's 'solution' of not even trying to solve the problem doesn't make the problem go away.  ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58104, "question": "Wat.\n\nDeploying code in java is not hard at all.  Of all the many problems with java that's not one of them. \n\nBuilding code however, well, sure, Go is simpler, because Go is just *broken*.   Maven/Ivy is complex because dependency management is complex.  Go's 'solution' of not even trying to solve the problem doesn't make the problem go away.  ", "aSentId": 58105, "answer": "go build program.go; \n\nscp program host:/path/.;\n\nI'm a Java user too. Go is better on this", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58106, "question": "go build program.go; \n\nscp program host:/path/.;\n\nI'm a Java user too. Go is better on this", "aSentId": 58107, "answer": "How is this different than copying a jar or war for deployment? same exact thing? ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58108, "question": "How is this different than copying a jar or war for deployment? same exact thing? ", "aSentId": 58109, "answer": "there are no external dependencies, the resulting file does not need a runtime on the target host\n\ntry it, you'll see", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58110, "question": "there are no external dependencies, the resulting file does not need a runtime on the target host\n\ntry it, you'll see", "aSentId": 58111, "answer": "Having a runtime available on the target machine isn't really a valid criticism.  I mean, you set it up _once_... Every subsequent deploy is a JAR/WAR copy.  You can easily bundle dependencies together.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58112, "question": "Having a runtime available on the target machine isn't really a valid criticism.  I mean, you set it up _once_... Every subsequent deploy is a JAR/WAR copy.  You can easily bundle dependencies together.", "aSentId": 58113, "answer": "I didn't say it was awful on Java, but it is better for Go\n\nI don't need ops people managing jre versions. this is even more important when shipping code to those you don't know....what version of jre do they distribute? ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58110, "question": "there are no external dependencies, the resulting file does not need a runtime on the target host\n\ntry it, you'll see", "aSentId": 58115, "answer": "Uh...the overwhelming majority of Java's use is going to be running in a container that will require that runtime to be installed anyway. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58106, "question": "go build program.go; \n\nscp program host:/path/.;\n\nI'm a Java user too. Go is better on this", "aSentId": 58117, "answer": "You're a javauser so your opinion is automatically invalid", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58078, "question": "so you're telling me that Go is a tool that will allow mere average programmers to produce better results than they would with a lesser type systems like python or a pile of mistakes like javascript??\n\nso, shouldn't we all be overjoyed?\n\nwho cares what the top 1% do? i know you write machine learning kernels in ARM assembly when you aren't writing your computer vision agent in Coq. being in the 1%, i know you work alone and don't reuse anyone else's (inferior) libraries, so network effects are irrelevant\n\nfor the rest of us, a tool that is approachable by the masses but yields superior results on average is a huge deal. James Gosling got this, and the last dozen Lisp fanatics blew their gasket when Java ate the world....", "aSentId": 58119, "answer": "&gt; so you're telling me that Go is a tool that will allow mere average programmers to produce better results than they would with a lesser type systems like python or a pile of mistakes like javascript??\n\nThat's not what I read at all, and I can't see how writting the same boilerplate again and again is \u00aballowing mere average programmers to produce better results\u00bb", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58120, "question": "&gt; so you're telling me that Go is a tool that will allow mere average programmers to produce better results than they would with a lesser type systems like python or a pile of mistakes like javascript??\n\nThat's not what I read at all, and I can't see how writting the same boilerplate again and again is \u00aballowing mere average programmers to produce better results\u00bb", "aSentId": 58121, "answer": "I really don't understand this about Go. Arguably it makes it a weaker language right there. And people here are comparing it to *Java*... Nonsense!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58078, "question": "so you're telling me that Go is a tool that will allow mere average programmers to produce better results than they would with a lesser type systems like python or a pile of mistakes like javascript??\n\nso, shouldn't we all be overjoyed?\n\nwho cares what the top 1% do? i know you write machine learning kernels in ARM assembly when you aren't writing your computer vision agent in Coq. being in the 1%, i know you work alone and don't reuse anyone else's (inferior) libraries, so network effects are irrelevant\n\nfor the rest of us, a tool that is approachable by the masses but yields superior results on average is a huge deal. James Gosling got this, and the last dozen Lisp fanatics blew their gasket when Java ate the world....", "aSentId": 58123, "answer": "I hate this article. It wreaks of horrible smugness, that does nothing but piss people off and makes people feel bad for doing cool things because they did it in tool x. Results and accessibility matter, and I do not welcome the hostility towards this.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58124, "question": "I hate this article. It wreaks of horrible smugness, that does nothing but piss people off and makes people feel bad for doing cool things because they did it in tool x. Results and accessibility matter, and I do not welcome the hostility towards this.", "aSentId": 58125, "answer": "I read it mainly as a guy who wanted to advertise 'D' while using the interest / controversy surrounding Go as a way to get people to read the blog post.\n\nThese days, I believe that what drives the success of a language is very much people actually writing lots of useful code in it and sharing that code, whining about language 'X' while trying to paint your favourite language 'Y' as the 'bee's knees' is wasted time in my opinion which instead should be used on writing useful open source code for your favourite language 'Y'.\n\nGo has a lot of actively developed and fully open source projects despite it's quite young age, seems Go developers spend a lot more time actually **developing** than arguing language semantics.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58126, "question": "I read it mainly as a guy who wanted to advertise 'D' while using the interest / controversy surrounding Go as a way to get people to read the blog post.\n\nThese days, I believe that what drives the success of a language is very much people actually writing lots of useful code in it and sharing that code, whining about language 'X' while trying to paint your favourite language 'Y' as the 'bee's knees' is wasted time in my opinion which instead should be used on writing useful open source code for your favourite language 'Y'.\n\nGo has a lot of actively developed and fully open source projects despite it's quite young age, seems Go developers spend a lot more time actually **developing** than arguing language semantics.", "aSentId": 58127, "answer": "&gt; I read it mainly as a guy who wanted to advertise 'D' while using the interest / controversy surrounding Go as a way to get people to read the blog post.\n\nI don't know what to think about it... had he used, say, Python, would you think that he's only trying to advertise Python? I think that the critic of Go stands no matter what (high-level) language is used for the comparison.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58128, "question": "&gt; I read it mainly as a guy who wanted to advertise 'D' while using the interest / controversy surrounding Go as a way to get people to read the blog post.\n\nI don't know what to think about it... had he used, say, Python, would you think that he's only trying to advertise Python? I think that the critic of Go stands no matter what (high-level) language is used for the comparison.", "aSentId": 58129, "answer": "It might have been helpful if the examples had been from a mix of languages, such as C#, D, Python, etc.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58126, "question": "I read it mainly as a guy who wanted to advertise 'D' while using the interest / controversy surrounding Go as a way to get people to read the blog post.\n\nThese days, I believe that what drives the success of a language is very much people actually writing lots of useful code in it and sharing that code, whining about language 'X' while trying to paint your favourite language 'Y' as the 'bee's knees' is wasted time in my opinion which instead should be used on writing useful open source code for your favourite language 'Y'.\n\nGo has a lot of actively developed and fully open source projects despite it's quite young age, seems Go developers spend a lot more time actually **developing** than arguing language semantics.", "aSentId": 58131, "answer": "This is true. We really don't need to argue we already have enough great programmers producing amazing libraries.... Although they are likely to simple for this guy to ever use.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58132, "question": "This is true. We really don't need to argue we already have enough great programmers producing amazing libraries.... Although they are likely to simple for this guy to ever use.", "aSentId": 58133, "answer": "Can confirm:  I am a great programmer", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58124, "question": "I hate this article. It wreaks of horrible smugness, that does nothing but piss people off and makes people feel bad for doing cool things because they did it in tool x. Results and accessibility matter, and I do not welcome the hostility towards this.", "aSentId": 58135, "answer": "*Yeah, fuck that peasant Go language and fuck the unwashed masses who lap it up like warm porridge. Go only slows me and my posse of 10x's (and 100x's) devs. There's no reason to like Go, unless you're dumb or something - you are not dumb, are you?*\n\n*We are the 1%! Fuck Yeah.*", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58137, "question": "I think neither succeeded on their merits; but rather because they were backed by \"giants\".", "aSentId": 58138, "answer": "that matters too, but its not everything. Sun backed a lot of dud tech that went nowhere (applets). Google couldn't make the world use Dart.\n\nthere will never be a perfect tool. Rust hasn't even reached 1.0 and people are blogging about migrating to Nim. at some point, you have to pick. Go is not perfect. there are things i would change. BUT, its here, its stable, it is better than what it seeks to replace, and lots of other people and companies are using it", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58139, "question": "that matters too, but its not everything. Sun backed a lot of dud tech that went nowhere (applets). Google couldn't make the world use Dart.\n\nthere will never be a perfect tool. Rust hasn't even reached 1.0 and people are blogging about migrating to Nim. at some point, you have to pick. Go is not perfect. there are things i would change. BUT, its here, its stable, it is better than what it seeks to replace, and lots of other people and companies are using it", "aSentId": 58140, "answer": "Meanwhile, here I am, [learning Lisp](https://xkcd.com/297/).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58143, "question": "&gt;for the rest of us, a tool that is approachable by the masses but yields superior results on average is a huge deal. James Gosling got this, and the last dozen Lisp fanatics blew their gasket when Java ate the world....\n\nThere are better options that work for both good and bad programmers.", "aSentId": 58144, "answer": "no one is advocating that you only use one programming language", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58145, "question": "no one is advocating that you only use one programming language", "aSentId": 58146, "answer": "Why not? There are massive advantages to using just one programming language if you can do so productively.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58150, "question": "These type of articles have it all wrong (in addition to reading what they want into Rob Pike's quotes).\n\nPeople who chose Go are not choosing looking for an alternative to D or Haskell, they are looking for an alternative to Python, Ruby, JavaScript(Node.js), &amp; Java.\n\nHaskell is rarely going to be a viable alternative for an enterprise over Go, and I'd be surprised if most corporations feel comfortable using D.\n\nInstead, there is a number of problems where Go is better than JavaScript(Node.js), Python, Ruby, &amp; Java. Maybe both D and Haskell are better too, but for a business there are a lot of other wins beyond generics and a better type system that makes Go more attractive.\n\nSo, I'll leave you with another Rob Pike quote, \"Quit complaining and just write the code.\"", "aSentId": 58151, "answer": "&gt; People who chose Go are not choosing looking for an alternative to D or Haskell, they are looking for an alternative to Python, Ruby, JavaScript(Node.js), &amp; Java.\n\nOne of these things (Java, a strongly typed, compiled language) is not like the others (weakly-typed interpreted).\n\nAs someone with a great deal of experience in both C++ and Python, I really don't see the particular similarities between Go and Python.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58152, "question": "&gt; People who chose Go are not choosing looking for an alternative to D or Haskell, they are looking for an alternative to Python, Ruby, JavaScript(Node.js), &amp; Java.\n\nOne of these things (Java, a strongly typed, compiled language) is not like the others (weakly-typed interpreted).\n\nAs someone with a great deal of experience in both C++ and Python, I really don't see the particular similarities between Go and Python.", "aSentId": 58153, "answer": "&gt; I really don't see the particular similarities between Go and Python.\n\n1. User base\n2. Applications for which they're most often used (command-line programs that don't need to do much of anything outside of built-in string or numeric types)\n\nCalling Go a \"compiled scripting language\" is not far off considering what it's good at.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58155, "question": "Go is a programming language made by pragmatic programmers.\nThe remarks about it being made for lesser programmers are unfortunate, because it isn't true that simple is worse.\nDouble return is cool by the way.", "aSentId": 58156, "answer": "&gt; Double return is cool by the way.\n\nSome of us implement that by \"returning a tuple\" or \"returning a struct/record\". But apparently that's too... I don't know, simplistic?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58157, "question": "&gt; Double return is cool by the way.\n\nSome of us implement that by \"returning a tuple\" or \"returning a struct/record\". But apparently that's too... I don't know, simplistic?", "aSentId": 58158, "answer": "Tuples are fine. The problem with returning a struct is that you either end up with a whole bunch of different structs that are slightly different from each other, or one large struct that doesn't fill everything out. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58157, "question": "&gt; Double return is cool by the way.\n\nSome of us implement that by \"returning a tuple\" or \"returning a struct/record\". But apparently that's too... I don't know, simplistic?", "aSentId": 58160, "answer": "I prefer the Go approach.  You can say : \"my function returns an int, and you can also check for errors with the second return value\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58161, "question": "I prefer the Go approach.  You can say : \"my function returns an int, and you can also check for errors with the second return value\"", "aSentId": 58162, "answer": "&gt; You can say : \"my function returns an int, and you can also check for errors with the second return value\"\n\nAs you can do with a struct/tuple.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58163, "question": "&gt; You can say : \"my function returns an int, and you can also check for errors with the second return value\"\n\nAs you can do with a struct/tuple.", "aSentId": 58164, "answer": "Yeah, because I'd want to define a struct for every function I write.\n\nAlternately, I'll buy into your logic and you'll buy into redefining every container method on every data structure due to lack of generics. Apples to apples.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58165, "question": "Yeah, because I'd want to define a struct for every function I write.\n\nAlternately, I'll buy into your logic and you'll buy into redefining every container method on every data structure due to lack of generics. Apples to apples.", "aSentId": 58166, "answer": "Defining a struct for every function you write is unnecessary if...\n\nwait for it...\n\nyour language has generics", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58167, "question": "Defining a struct for every function you write is unnecessary if...\n\nwait for it...\n\nyour language has generics", "aSentId": 58168, "answer": "I don't think that works if we're talking about multiple return. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58163, "question": "&gt; You can say : \"my function returns an int, and you can also check for errors with the second return value\"\n\nAs you can do with a struct/tuple.", "aSentId": 58170, "answer": "What about optimizations? With Go, both return values can be in registers. I don't think any compiler would optimize a struct/tuple to do that.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58171, "question": "What about optimizations? With Go, both return values can be in registers. I don't think any compiler would optimize a struct/tuple to do that.", "aSentId": 58172, "answer": "You'd be surprised.  This is standard and well defined in the [x86-64 Linux ABI](http://www.x86-64.org/documentation/abi.pdf), at least. Have a look at section 3.2.3.  There are two 64-bit integer return registers, and a procedure for determining whether a struct should be passed or returned via registers.  Page 23 has an example of passing one in.\n\nHere's a simple test showing this in action:\n\n    #include &lt;stdint.h&gt;\n    typedef struct { uint32_t a, b; } pair;\n    pair test() {\n        pair result = {0x01234567, 0x89abcdef};\n        return result;\n    }\n\nWith -O1 and above, clang gives me this:\n\n    test:                                   # @test\n            .cfi_startproc\n    # BB#0:\n            movabsq $-8526495043095935641, %rax # imm = 0x89ABCDEF01234567\n            retq\n\nand gcc gives pretty much the same thing. (-O0 is more verbose, but ultimately puts both into %rax as well.)  With uint64_t instead, the first goes in %rax and the second in %rdx, per the ABI:\n\n    test:                                   # @test\n            .cfi_startproc\n    # BB#0:\n            movl    $19088743, %eax         # imm = 0x1234567\n            movl    $2309737967, %edx       # imm = 0x89ABCDEF\n            retq\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58171, "question": "What about optimizations? With Go, both return values can be in registers. I don't think any compiler would optimize a struct/tuple to do that.", "aSentId": 58174, "answer": "On the contrary, *many* compilers would do that", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58171, "question": "What about optimizations? With Go, both return values can be in registers. I don't think any compiler would optimize a struct/tuple to do that.", "aSentId": 58176, "answer": "&gt; I don't think any compiler would optimize a struct/tuple to do that.\n\nWhy not? Seems simple enough. I don't see how multiple returns makes it simpler.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58161, "question": "I prefer the Go approach.  You can say : \"my function returns an int, and you can also check for errors with the second return value\"", "aSentId": 58178, "answer": "And you can say, \"Oh, I added that second return value, and I guess I never told you, and now you're just throwing away errors.\"\n\nThe lack of handling for exceptional conditions is definitely one of Go's weak points.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58179, "question": "And you can say, \"Oh, I added that second return value, and I guess I never told you, and now you're just throwing away errors.\"\n\nThe lack of handling for exceptional conditions is definitely one of Go's weak points.", "aSentId": 58180, "answer": "Actually, you can't.  If a function has 2 return values and you aren't saving (or explicitly ignoring) one, that's a compile time error.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58157, "question": "&gt; Double return is cool by the way.\n\nSome of us implement that by \"returning a tuple\" or \"returning a struct/record\". But apparently that's too... I don't know, simplistic?", "aSentId": 58182, "answer": "Well the struct style return is a pain in the ass because you have to define it first.\n\nI agree with tuples, shame go doesn't have them.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58184, "question": "I think there was a place for a simple native modern language but Go failed. Rust fills the place for a complex native language. Maybe nim could take Go's place. ", "aSentId": 58185, "answer": "I think Go is more aimed at Java and Python's niches than C++ and C's, the way Rust is.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58186, "question": "I think Go is more aimed at Java and Python's niches than C++ and C's, the way Rust is.", "aSentId": 58187, "answer": "The Go people were interested in attracting C++ developers, once upon a time. Maybe in part because the creation story of Go started while they were waiting for a C++ program to compile. But that didn't really pan out.\n\nSo, pretty much how Java began with regards to C++, and turned out.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58188, "question": "The Go people were interested in attracting C++ developers, once upon a time. Maybe in part because the creation story of Go started while they were waiting for a C++ program to compile. But that didn't really pan out.\n\nSo, pretty much how Java began with regards to C++, and turned out.", "aSentId": 58189, "answer": "Go was designed for Google's main use of C++ -servers and network programs. Unfortunately for Go's adoption, this is not what C++ is mainly used for in the wild. For the past while Rust has been designed with the intention of being suitable for everything C++ is, and just as capable.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58190, "question": "Go was designed for Google's main use of C++ -servers and network programs. Unfortunately for Go's adoption, this is not what C++ is mainly used for in the wild. For the past while Rust has been designed with the intention of being suitable for everything C++ is, and just as capable.", "aSentId": 58191, "answer": "&gt; Go was designed for Google's main use of C++ -servers and network programs. \n\n&gt; Unfortunately for Go's adoption, this is not what C++ is mainly used for in the wild.\n\nYes, I guess because most companies' server code doesn't have to take quite the load that some of Google's servers have to take. :o)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58192, "question": "&gt; Go was designed for Google's main use of C++ -servers and network programs. \n\n&gt; Unfortunately for Go's adoption, this is not what C++ is mainly used for in the wild.\n\nYes, I guess because most companies' server code doesn't have to take quite the load that some of Google's servers have to take. :o)", "aSentId": 58193, "answer": "Yo, Erlang called, but it just muttered \"nine nines\" and hung up.  ;)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58186, "question": "I think Go is more aimed at Java and Python's niches than C++ and C's, the way Rust is.", "aSentId": 58195, "answer": "&gt; Java and Python's niches\n\n?\n\nThese languages have completely different \"niches\" - they are about as different as procedural languages can be...!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58184, "question": "I think there was a place for a simple native modern language but Go failed. Rust fills the place for a complex native language. Maybe nim could take Go's place. ", "aSentId": 58197, "answer": "Thanks for mentionning nim, I am going to have a look!\n\nRust sounds like a language from hell, with all that move semantic. When you leave C++, it is fine, but coming from python, I want garbage collection.\n\nI wish the authors of go were more open to extending it. It looks like it could be a useable language, if they added generics. It would be nice if they also allowed operator overloading, in order to support summing big integers with + (linear algebra is also a very important use case).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58198, "question": "Thanks for mentionning nim, I am going to have a look!\n\nRust sounds like a language from hell, with all that move semantic. When you leave C++, it is fine, but coming from python, I want garbage collection.\n\nI wish the authors of go were more open to extending it. It looks like it could be a useable language, if they added generics. It would be nice if they also allowed operator overloading, in order to support summing big integers with + (linear algebra is also a very important use case).", "aSentId": 58199, "answer": "Check out OCaml. It's concise like Python and has an excellent type system. The implementation is both mature and performant (especially when compared to python).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58198, "question": "Thanks for mentionning nim, I am going to have a look!\n\nRust sounds like a language from hell, with all that move semantic. When you leave C++, it is fine, but coming from python, I want garbage collection.\n\nI wish the authors of go were more open to extending it. It looks like it could be a useable language, if they added generics. It would be nice if they also allowed operator overloading, in order to support summing big integers with + (linear algebra is also a very important use case).", "aSentId": 58201, "answer": "Rust is designed for applications that may not have the luxury of a garbage collector, either for performance reasons or because there's no room for it in the environment. It's meant to fill all the same niches that C does, while eliminating some of the most horrible classes of bugs that C is forced to fix by either convention or abstraction.\n\nIt is a fact that you spend a lot of development time in Rust fighting the compiler. But like the exasperated parent of a rebellious teenager, it (mostly) knows best. \n\nYou might be trying do something dangerous or stupid that seems like it should work at the time, when in actuality it could crash the program, or give an attacker access to data they shouldn't know about, or cause undefined behavior that is difficult to reproduce or debug. Rust's compiler can recognize that and keep you from ever running that program.\n\nI came to Rust from Java and I was overjoyed at the idea of never dealing with a `NullPointerException` ever again, because Rust can express nullability at the type level instead of at the language level. This means you always know when a value can be null and the type checker requires you to handle it properly. \n\nIt's a pain in the ass, but 90% of the time, when your program finally compiles, it just *works*. I spend very little time in a debugger when working with Rust, because the compiler has already sorted out most of the problems I'd use a debugger to look for. \n\nAfter a while, it changes how you think: you can see lifetime and move errors in your code without ever running the compiler. It changes how you approach problems in other languages; you start to think about *provable* safety and making sure your code does what it says on the tin. And I think that's pretty awesome.\n\n[1]: http://doc.rust-lang.org/std/ops/trait.Add.html", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58198, "question": "Thanks for mentionning nim, I am going to have a look!\n\nRust sounds like a language from hell, with all that move semantic. When you leave C++, it is fine, but coming from python, I want garbage collection.\n\nI wish the authors of go were more open to extending it. It looks like it could be a useable language, if they added generics. It would be nice if they also allowed operator overloading, in order to support summing big integers with + (linear algebra is also a very important use case).", "aSentId": 58203, "answer": "&gt; It would be nice if they also allowed operator overloading, in order to support summing big integers with + (linear algebra is also a very important use case).\n\nCheck out Julia too, if you want a few more math libraries than what Nim has high-level bindings for at the moment. Operator overloading, BigInts, and world-class linear algebra support are all standard, but you also get multiple dispatch which is a really fun inversion of standard object-oriented ways of thinking.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58204, "question": "&gt; It would be nice if they also allowed operator overloading, in order to support summing big integers with + (linear algebra is also a very important use case).\n\nCheck out Julia too, if you want a few more math libraries than what Nim has high-level bindings for at the moment. Operator overloading, BigInts, and world-class linear algebra support are all standard, but you also get multiple dispatch which is a really fun inversion of standard object-oriented ways of thinking.", "aSentId": 58205, "answer": "+1 for Julia.  It's a bit big right now, though; the current Julia runtime has a lot of external dependencies (the standard library calls for a BLAS implementation among other things), so there are a lot of moving parts that hamper things like porting to new platforms (I still haven't managed to get it to compile for OpenBSD, for example, and the ARM port has been a long and bloody battle), though there's some indication on the mailing lists that this might change eventually now that Julia's getting some attention as a general-purpose programming language rather than a Matlab or R alternative.\n\nIt also has a lot of good concurrency features, along with a significant killer feature of being almost as fast as Fortran at number-crunching (which is pretty much unheard of for a language as high-level and expressive as Julia).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58207, "question": "&gt; but coming from python\n\nFunny. For me python looks like a language from hell. :)", "aSentId": 58208, "answer": "why.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58209, "question": "why.", "aSentId": 58210, "answer": "I personally dislike whitespace significance rather strongly.  This is a common criticism of Python.\n\nIt also doesn't help that I come from a Perl background, so Python's philosophy of \"There's Only One Way To Do It\u2122\" conflicts with my own Perl-inherited philosophy of \"There's More Than One Way To Do It\u2122\".  I can see why that might have merits, but I prefer expressiveness.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58184, "question": "I think there was a place for a simple native modern language but Go failed. Rust fills the place for a complex native language. Maybe nim could take Go's place. ", "aSentId": 58212, "answer": "the difference is that you can get a job programming in Go\n\nNim's features won't do you much good when you have to settle for PHP in order to get paid", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58213, "question": "the difference is that you can get a job programming in Go\n\nNim's features won't do you much good when you have to settle for PHP in order to get paid", "aSentId": 58214, "answer": "&gt; you can get a job programming in Go\n\nYou could get a job programming in Perl 15 years ago. Times change.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58215, "question": "&gt; you can get a job programming in Go\n\nYou could get a job programming in Perl 15 years ago. Times change.", "aSentId": 58216, "answer": "sure, and I had a perl job 15 years ago. and I did change. but I changed when there was a point and a motivation.\n\nits not like there is this massive swell of activity in Nim that knuckle draggers like me are missing out on. outside of proggit and hacker news, Nim is unheard of. if it takes off, I will have been aware of it for years longer than most, I will pick it up at the appropriate time", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58217, "question": "sure, and I had a perl job 15 years ago. and I did change. but I changed when there was a point and a motivation.\n\nits not like there is this massive swell of activity in Nim that knuckle draggers like me are missing out on. outside of proggit and hacker news, Nim is unheard of. if it takes off, I will have been aware of it for years longer than most, I will pick it up at the appropriate time", "aSentId": 58218, "answer": "That's an absolutely reasonable point of view. The criticisms of the popular choices aren't wrong, but the better choices right now are either niche, or only for early adopters, or made some strange tradeoffs that will hurt in the long run.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58220, "question": "Rust's only compiler keeps changing. Not ready for production.\n\nNim's only compiler is full of bugs. Not ready for production.\n\nNode.js is rock solid native systems programming.", "aSentId": 58221, "answer": "I think you're earning downvotes for calling an interpreted language a native systems programming tool.\n\nYou're going to have to explain that one.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58222, "question": "I think you're earning downvotes for calling an interpreted language a native systems programming tool.\n\nYou're going to have to explain that one.", "aSentId": 58223, "answer": "Node.js code is native on webscale mongo OS written in pure node", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58225, "question": "That's because you're not thinking BIG and solving BIG problems. Generics and const-correctness are two concepts that did not originate from Bell Labs, therefore they're by default not worthy of Go's *simplicity*^TM and *pragmatism*. Why would you burden the Go compiler with compile-time safety checks and template code generation when instead you as a programmer can do the dangerous and repetitious job manually. \n\nRemember, think BIG. Think the Go Way^TM.", "aSentId": 58226, "answer": "Just generated it, yo ;-)\n\n/No seriously, that is their answer. User a pre-processor pass to generate your code for you. Just don't call it macros, cause that's not cool.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58228, "question": "Go is not designed to be simple for its own sake. Go's simplicity does achieve a very concrete goal: fast compile time.\n\nYou need a bit of context to understand this. Medium Google project written in C++ takes 10 hours to compile from scratch on a single machine. (Of course, that's why they use distributed build.) Fast compile time is very important for Google, and not at all important for nearly everyone else. So it is somewhat inevitable that Google makes weird decisions, because its requirement is so far removed from the rest of the industry.", "aSentId": 58229, "answer": "D compiles faster than go with equivalent code, so where does that leave Go?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58230, "question": "D compiles faster than go with equivalent code, so where does that leave Go?", "aSentId": 58231, "answer": "Without a generics.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58235, "question": "Simple examples really do a disservice to reality of the matter. They make explicit code look unnecessary and implicit code to look natural. Code readability, maintainability, and debugability  is far more important than the 5 minutes you spend writing something. Thus, explicit code, even in languages with very powerful features, is typically mandated by companies to preserve the quality of the code.  We're not talking about a prototyping language like python, we're talking about a *product*, and products benefit from details being explicit.", "aSentId": 58236, "answer": "&gt; We're not talking about a prototyping language like python,\n\nPeople do paid production work on Python all the time...!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58237, "question": "&gt; We're not talking about a prototyping language like python,\n\nPeople do paid production work on Python all the time...!", "aSentId": 58238, "answer": "So? People do a *lot* of insane things in paid production code. Just read thedailywtf.com", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58235, "question": "Simple examples really do a disservice to reality of the matter. They make explicit code look unnecessary and implicit code to look natural. Code readability, maintainability, and debugability  is far more important than the 5 minutes you spend writing something. Thus, explicit code, even in languages with very powerful features, is typically mandated by companies to preserve the quality of the code.  We're not talking about a prototyping language like python, we're talking about a *product*, and products benefit from details being explicit.", "aSentId": 58240, "answer": "i agree with you completely, but just curious, how old are you", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58241, "question": "i agree with you completely, but just curious, how old are you", "aSentId": 58242, "answer": "31, why?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58243, "question": "31, why?", "aSentId": 58244, "answer": "i'm around the same age as you.  was just wondering if it was an old school line of thought... esp with java 8 using lambda expression", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58248, "question": "I think you lack perspective. The opposite end of this \"dumb language\" is something like Perl or C++: complex languages which attract \"smart\" programmers who write incomprehensible code.\n\nGo, C, Python: these languages try to be simple, the problems that they are solving are already complex enough.", "aSentId": 58249, "answer": "Need the performance that Java can't provide?  You don't need to be crazy computer scientist snob to hit this limitation, I have hobby game projects that wouldn't be feasible in a VM-based language.\n\nWould you like more complex forms of abstraction than functions and structs of function pointers?  Sure, you can write just about anything in C, but there's a reason most new projects aren't often started in it.  Lambdas, first-class-functions, objects, etc, these are all very nice, and a reason to move to something more modern than C.\n\nWant a large number of libraries to use?  Don't want to run into obscure bugs because you're building on Windows?  Want multiple choices for anything from UIs, to images libraries?  As fun and cool as they are, it might be best to avoid the smaller or newer languages like D, Rust, and Nim.\n\nThat pretty much leaves C++.  It isn't some snob language which only attracts \"smart\" programmers who write incomprehensible code.  It's a very practical choice for anyone who needs performance and lower-level access in what they're working on.  Not that I'm a fan of this article, it's extremely condescending and overlooks how useful Go can be for certain projects.  But the C++ hate in this thread is ridiculous.  It's a very useful language for a wide range of purposes, not some exclusive domain of \"1% programmers.\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58250, "question": "Need the performance that Java can't provide?  You don't need to be crazy computer scientist snob to hit this limitation, I have hobby game projects that wouldn't be feasible in a VM-based language.\n\nWould you like more complex forms of abstraction than functions and structs of function pointers?  Sure, you can write just about anything in C, but there's a reason most new projects aren't often started in it.  Lambdas, first-class-functions, objects, etc, these are all very nice, and a reason to move to something more modern than C.\n\nWant a large number of libraries to use?  Don't want to run into obscure bugs because you're building on Windows?  Want multiple choices for anything from UIs, to images libraries?  As fun and cool as they are, it might be best to avoid the smaller or newer languages like D, Rust, and Nim.\n\nThat pretty much leaves C++.  It isn't some snob language which only attracts \"smart\" programmers who write incomprehensible code.  It's a very practical choice for anyone who needs performance and lower-level access in what they're working on.  Not that I'm a fan of this article, it's extremely condescending and overlooks how useful Go can be for certain projects.  But the C++ hate in this thread is ridiculous.  It's a very useful language for a wide range of purposes, not some exclusive domain of \"1% programmers.\"", "aSentId": 58251, "answer": "&gt; But the C++ hate in this thread is ridiculous.\n\nC++ hasn't been mentioned all that many times here... You can't deny C++ can be highly unpleasant to work in if you prefer writing in a higher-level language that makes things easier for you.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58248, "question": "I think you lack perspective. The opposite end of this \"dumb language\" is something like Perl or C++: complex languages which attract \"smart\" programmers who write incomprehensible code.\n\nGo, C, Python: these languages try to be simple, the problems that they are solving are already complex enough.", "aSentId": 58253, "answer": "A programming language has two jobs:\n\n - To make it easy to express a good solution a problem.\n - To make it easy to modify an existing solution.\n\nSimply by not having any way to express abstract data structures without manually simulating dynamic typing, Go does a poor job at both criteria.\n\nGo is great for rewriting an existing Python script and getting a 10x speedup. For any more complicated project, you run into the fact that you *need* things like Java's TreeMap or ConcurrentMap. Go doesn't have them. Go doesn't let you write them in any reasonable way. If you write them in an unreasonable way, Go makes it hard to use them.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58248, "question": "I think you lack perspective. The opposite end of this \"dumb language\" is something like Perl or C++: complex languages which attract \"smart\" programmers who write incomprehensible code.\n\nGo, C, Python: these languages try to be simple, the problems that they are solving are already complex enough.", "aSentId": 58255, "answer": "You're just creating a false dichotomy here. There's also a big difference between a language being complex and well designed.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58258, "question": "What I'm missing in this article is a review of the concurrency features of Go. There are other languages with this, in fact, Pike himself acknowledges they took the channel/go-routine setup from others, but none as accessible as Go. Once you consider concurrency, much of the rest of the language falls into place. All/most languages have concurrency in one form or another, what's unique about Go is that the whole language is designed to enable you to reason about it.\n\nAs for generics: yes, I would probably enjoy it too, but it's not nearly enough outside of contrived examples to motivate leaving the concurrency features.\n\nThe big thing I would like to see in Go is C++-style const guarantees. (And yes, I know D has pure functions)\n", "aSentId": 58259, "answer": "Go doesn't have the tools for reasonable concurrency, because you can't abstract over its built in tools.\n\nGo has maps, but you can't build a locked map. Go has channels, but you can't build a queue. Well, you can build one of each - but you can't make a library and if you need it again you have to build a new one.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58260, "question": "Go doesn't have the tools for reasonable concurrency, because you can't abstract over its built in tools.\n\nGo has maps, but you can't build a locked map. Go has channels, but you can't build a queue. Well, you can build one of each - but you can't make a library and if you need it again you have to build a new one.", "aSentId": 58261, "answer": "why can't you build a locked map?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58262, "question": "why can't you build a locked map?", "aSentId": 58263, "answer": "You can't build a generic one I think is what he's saying? You can build one specialized to a type OR specialize it to interface{} and call it a day", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58264, "question": "You can't build a generic one I think is what he's saying? You can build one specialized to a type OR specialize it to interface{} and call it a day", "aSentId": 58265, "answer": "why can't you just put it in a struct with a lock and hide it behind getters and setters?\n\nthis is similar to how it would be done in Haskell and C++ also, their maps are not thread safe by default. Java has ConcurrentHashMap\n\nas for types and genericity, any INSTANTIATED map is specialized in any language with a meaningful type system. its not like Java or C++ or Haskell will let you instantiate a map of arbitrary types...you still have to declare key and value types, just like Go\n\nI'm not sure what interface{} has to do with this at all", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58266, "question": "why can't you just put it in a struct with a lock and hide it behind getters and setters?\n\nthis is similar to how it would be done in Haskell and C++ also, their maps are not thread safe by default. Java has ConcurrentHashMap\n\nas for types and genericity, any INSTANTIATED map is specialized in any language with a meaningful type system. its not like Java or C++ or Haskell will let you instantiate a map of arbitrary types...you still have to declare key and value types, just like Go\n\nI'm not sure what interface{} has to do with this at all", "aSentId": 58267, "answer": "Either you're being intentionally dishonest, or you don't even understand the language you spend so much time making excuses for. Go ahead, build me a generic locked map in Go without using the `interface{}` abomination.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58258, "question": "What I'm missing in this article is a review of the concurrency features of Go. There are other languages with this, in fact, Pike himself acknowledges they took the channel/go-routine setup from others, but none as accessible as Go. Once you consider concurrency, much of the rest of the language falls into place. All/most languages have concurrency in one form or another, what's unique about Go is that the whole language is designed to enable you to reason about it.\n\nAs for generics: yes, I would probably enjoy it too, but it's not nearly enough outside of contrived examples to motivate leaving the concurrency features.\n\nThe big thing I would like to see in Go is C++-style const guarantees. (And yes, I know D has pure functions)\n", "aSentId": 58269, "answer": "The problem with `const` is that you have no idea whether somebody else has a mutable reference to the object, so while you can't mutate it, you still can't rely on it not changing.\n\nYes, some sort of immutability seems like a no-brainer for Go, but not the C++ sort.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58270, "question": "The problem with `const` is that you have no idea whether somebody else has a mutable reference to the object, so while you can't mutate it, you still can't rely on it not changing.\n\nYes, some sort of immutability seems like a no-brainer for Go, but not the C++ sort.", "aSentId": 58271, "answer": "D has full transitive immutability and const. Take a look: http://dlang.org/const3.html", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58272, "question": "D has full transitive immutability and const. Take a look: http://dlang.org/const3.html", "aSentId": 58273, "answer": "I am from the planet Haskell. Your opt-in immutability is powerless against me. /robovoice\n\n...\n\nSorry, I just wanted to be snarky. I'm glad D has this!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58274, "question": "I am from the planet Haskell. Your opt-in immutability is powerless against me. /robovoice\n\n...\n\nSorry, I just wanted to be snarky. I'm glad D has this!", "aSentId": 58275, "answer": "For the record, I actually am a Haskell user, and was making a serious point in a snarky way :P.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58270, "question": "The problem with `const` is that you have no idea whether somebody else has a mutable reference to the object, so while you can't mutate it, you still can't rely on it not changing.\n\nYes, some sort of immutability seems like a no-brainer for Go, but not the C++ sort.", "aSentId": 58277, "answer": "Hmm, damn, you're right. \n\nWell, the C++ const is more about conveying information about the function than about object, which is not without it's merit regardless. I agree though, if you had to choose immutable objects are probably more valuable.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58270, "question": "The problem with `const` is that you have no idea whether somebody else has a mutable reference to the object, so while you can't mutate it, you still can't rely on it not changing.\n\nYes, some sort of immutability seems like a no-brainer for Go, but not the C++ sort.", "aSentId": 58279, "answer": "With C or C++, you can declare the variable itself const, and then it is truly immutable.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58280, "question": "With C or C++, you can declare the variable itself const, and then it is truly immutable.", "aSentId": 58281, "answer": "Well yeah, if you're in charge of declaring the variable. If you're writing a function that takes an argument, all the function can do is promise not to modify the argument; it can't receive any guarantees about the argument itself, except in the case of value types of course.\n\nIt's useful, but I want more haha.\n\nEDIT: also... `const_cast`?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58282, "question": "Well yeah, if you're in charge of declaring the variable. If you're writing a function that takes an argument, all the function can do is promise not to modify the argument; it can't receive any guarantees about the argument itself, except in the case of value types of course.\n\nIt's useful, but I want more haha.\n\nEDIT: also... `const_cast`?", "aSentId": 58283, "answer": "`const_cast` is like `unsafePerformIO`. It's an escape hatch that you can actually assume is never used incorrectly. i.e: constness is more than a weak convention.\n\nAlso, `const` on parameters is much more for the caller than the callee.  That the callee cannot change the variable mostly gives guarantees that are *very* useful for the caller.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58282, "question": "Well yeah, if you're in charge of declaring the variable. If you're writing a function that takes an argument, all the function can do is promise not to modify the argument; it can't receive any guarantees about the argument itself, except in the case of value types of course.\n\nIt's useful, but I want more haha.\n\nEDIT: also... `const_cast`?", "aSentId": 58285, "answer": "You misunderstand the point of functions taking const arguments. The whole point of a function taking const reference arguments is to communicate to the caller that they will not modify it.\n\nIt's the only sensible meaning to assign, in a language that is all about taking mutable references of things and modifying them.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58258, "question": "What I'm missing in this article is a review of the concurrency features of Go. There are other languages with this, in fact, Pike himself acknowledges they took the channel/go-routine setup from others, but none as accessible as Go. Once you consider concurrency, much of the rest of the language falls into place. All/most languages have concurrency in one form or another, what's unique about Go is that the whole language is designed to enable you to reason about it.\n\nAs for generics: yes, I would probably enjoy it too, but it's not nearly enough outside of contrived examples to motivate leaving the concurrency features.\n\nThe big thing I would like to see in Go is C++-style const guarantees. (And yes, I know D has pure functions)\n", "aSentId": 58287, "answer": "To learn more about the concurrency features of Go, look up Communicating Sequential Processes, and the languages it's influenced. Go has particularly nice syntax for it, but other langs do it well, too. I've used Clojure's version, core.async, and it makes it super-easy to write correct async code that appears synchronous is structure.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58290, "question": "&gt; func Reduce(in interface{}, memo interface{}, fn func(interface{}, interface{}) interface{}) \n\nSo Python but more verbose. \n\nIf you want types, just use C#, Java, D, Rust, Haskell, OCaml -- something with a more expressive type system. Not sure what Go gives you then", "aSentId": 58291, "answer": "I don't think anyone who uses Go does so for the type system.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58295, "question": "Over the course of the past few months I\u2019ve been using Go to implement a proof of concept program in my spare time. This was done in part to learn Go and to also see if such a program would work. The program itself is very simplistic and not the focus of this article but my experience of using Go is worth writing a few words. Go is shaping up to be a popular language for doing serious large scale work and a language created by Google is not to be sniffed at. With all that said, I honestly think Go\u2019s design a disservice to intelligent programmers.", "aSentId": 58296, "answer": "Your code sample is unnecessarily verbose and has poor use of the standard library; here's a much simpler implementation:\n\n    package main\n    \n    import (\n    \t\"fmt\"\n    \t\"io/ioutil\"\n    \t\"log\"\n    \t\"os\"\n    )\n    \n    func main() {\n    \tvar text []byte\n    \tvar err error\n    \n    \tif len(os.Args) &gt; 1 {\n    \t\ttext, err = ioutil.ReadFile(os.Args[1])\n    \t} else {\n    \t\ttext, err = ioutil.ReadAll(os.Stdin)\n    \t}\n    \n    \tif err != nil {\n    \t\tlog.Fatal(err)\n    \t}\n    \n    \tfmt.Println(text)\n    }\n\nIt's only slightly longer than the D sample and (I would argue) much easier to read.\n\nBesides that, simplicity is something to strive for. As soon as you've read someone else's code in a large codebase you'll appreciate that having a million features at your disposal results in code that's far more complicated and far harder to understand.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58297, "question": "Your code sample is unnecessarily verbose and has poor use of the standard library; here's a much simpler implementation:\n\n    package main\n    \n    import (\n    \t\"fmt\"\n    \t\"io/ioutil\"\n    \t\"log\"\n    \t\"os\"\n    )\n    \n    func main() {\n    \tvar text []byte\n    \tvar err error\n    \n    \tif len(os.Args) &gt; 1 {\n    \t\ttext, err = ioutil.ReadFile(os.Args[1])\n    \t} else {\n    \t\ttext, err = ioutil.ReadAll(os.Stdin)\n    \t}\n    \n    \tif err != nil {\n    \t\tlog.Fatal(err)\n    \t}\n    \n    \tfmt.Println(text)\n    }\n\nIt's only slightly longer than the D sample and (I would argue) much easier to read.\n\nBesides that, simplicity is something to strive for. As soon as you've read someone else's code in a large codebase you'll appreciate that having a million features at your disposal results in code that's far more complicated and far harder to understand.", "aSentId": 58298, "answer": "I'm a C++ programmer and this code is neat and nice... Go seems like a very nice language.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58295, "question": "Over the course of the past few months I\u2019ve been using Go to implement a proof of concept program in my spare time. This was done in part to learn Go and to also see if such a program would work. The program itself is very simplistic and not the focus of this article but my experience of using Go is worth writing a few words. Go is shaping up to be a popular language for doing serious large scale work and a language created by Google is not to be sniffed at. With all that said, I honestly think Go\u2019s design a disservice to intelligent programmers.", "aSentId": 58300, "answer": "Rob Pike's comments directly addressed the on-the-job inexperience of freshly minted college grads and had nothing to do with their intelligence. They will understand in time after they gain experience. Go is an attempt to shorten the time they need to become productive.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58295, "question": "Over the course of the past few months I\u2019ve been using Go to implement a proof of concept program in my spare time. This was done in part to learn Go and to also see if such a program would work. The program itself is very simplistic and not the focus of this article but my experience of using Go is worth writing a few words. Go is shaping up to be a popular language for doing serious large scale work and a language created by Google is not to be sniffed at. With all that said, I honestly think Go\u2019s design a disservice to intelligent programmers.", "aSentId": 58302, "answer": "Have you tried APL instead?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58305, "question": "Go's source code also looks ugly as hell.  It's like some weird bastard hybrid of Python and C#.  Thanks; I'll pass.\n\nAnd BTW, what's with capitalizing method names?  Did Google just copy this awful C# convention to woo Microsoft developers?  Yeah, I know, it's about visibility, but a more sensible (and industry-standard) approach would have been:\n\n    public()\n    _private()\n\ninstead of:\n\n    Public()\n    private()\n \n\n\n", "aSentId": 58306, "answer": "underscore prefixes are retarded in any language", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58307, "question": "underscore prefixes are retarded in any language", "aSentId": 58308, "answer": "Oh, thanks for clearing that up. (rolls eyes)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58307, "question": "underscore prefixes are retarded in any language", "aSentId": 58310, "answer": "So ~~it~~ is any capitalisation at all.\n\nEDIT: spelling, derp", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58311, "question": "So ~~it~~ is any capitalisation at all.\n\nEDIT: spelling, derp", "aSentId": 58312, "answer": "english much?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58305, "question": "Go's source code also looks ugly as hell.  It's like some weird bastard hybrid of Python and C#.  Thanks; I'll pass.\n\nAnd BTW, what's with capitalizing method names?  Did Google just copy this awful C# convention to woo Microsoft developers?  Yeah, I know, it's about visibility, but a more sensible (and industry-standard) approach would have been:\n\n    public()\n    _private()\n\ninstead of:\n\n    Public()\n    private()\n \n\n\n", "aSentId": 58314, "answer": "What about a more standard and sensible approach of this:\n\n    public foo();\n    private foo();\n\nSeriously, just have keywords for it. It's not hard.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58315, "question": "What about a more standard and sensible approach of this:\n\n    public foo();\n    private foo();\n\nSeriously, just have keywords for it. It's not hard.", "aSentId": 58316, "answer": "Go is not big on keywords. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58317, "question": "Go is not big on keywords. ", "aSentId": 58318, "answer": "Keywords are in the definition which means you need to find the definition first. But whenever you see a variable in go you see right away if it's exported or not. \n\nAlso it forces people to use coding style standards. No matter what company you are working in the code will just have that in common", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58315, "question": "What about a more standard and sensible approach of this:\n\n    public foo();\n    private foo();\n\nSeriously, just have keywords for it. It's not hard.", "aSentId": 58320, "answer": "Whenever you use the names or see the name in code, you will see if they are exported or not. Keywords don't give you that. \n\nBut if go was to use keywords, it would be something like exported and unexported, since that is actually slightly different from public and private in other languages.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58321, "question": "Whenever you use the names or see the name in code, you will see if they are exported or not. Keywords don't give you that. \n\nBut if go was to use keywords, it would be something like exported and unexported, since that is actually slightly different from public and private in other languages.", "aSentId": 58322, "answer": "nah you just do this:\n\n    export int exported_function() {\n        return 5;\n    }\n\n    int internal_function() {\n        return 5;\n    }\n\nto use the syntax that most likely will be in C++17.\n\nor:\n\n    int exported_function() {\n        return 5;\n    }\n\n    static int internal_function() {\n        return 5;\n    }\n\nto use C syntax.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58323, "question": "nah you just do this:\n\n    export int exported_function() {\n        return 5;\n    }\n\n    int internal_function() {\n        return 5;\n    }\n\nto use the syntax that most likely will be in C++17.\n\nor:\n\n    int exported_function() {\n        return 5;\n    }\n\n    static int internal_function() {\n        return 5;\n    }\n\nto use C syntax.", "aSentId": 58324, "answer": "If it were enforced, that would be one thing. But if you're reading someone else's code, and they don't adhere to that then you're back at square one.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58325, "question": "If it were enforced, that would be one thing. But if you're reading someone else's code, and they don't adhere to that then you're back at square one.", "aSentId": 58326, "answer": "That doesn't make any sense. If they don't 'adhere' to that then it's the same result as them not 'adhering' to the Go naming conventions: their code has different meaning. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58315, "question": "What about a more standard and sensible approach of this:\n\n    public foo();\n    private foo();\n\nSeriously, just have keywords for it. It's not hard.", "aSentId": 58328, "answer": "Nobody said it was hard, it's just unnecessary when the language can encourage you to program in a way that is standard across it.\n\n    private Foo();\n    private foo();\n\nThe above seems dumb to me, but is valid in most programming languages.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58329, "question": "Nobody said it was hard, it's just unnecessary when the language can encourage you to program in a way that is standard across it.\n\n    private Foo();\n    private foo();\n\nThe above seems dumb to me, but is valid in most programming languages.", "aSentId": 58330, "answer": "if you want Java, just use it", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58329, "question": "Nobody said it was hard, it's just unnecessary when the language can encourage you to program in a way that is standard across it.\n\n    private Foo();\n    private foo();\n\nThe above seems dumb to me, but is valid in most programming languages.", "aSentId": 58332, "answer": "Any capital letters in any non-template-parameter non-macro-constant identifers seems pretty retarded to me.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58333, "question": "Any capital letters in any non-template-parameter non-macro-constant identifers seems pretty retarded to me.", "aSentId": 58334, "answer": "Fortunately for the rest of the world, nobody really cares what you have to say on the matter. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58337, "question": "Agreed, but the Go fanboys have declared that Their Language Is Perfect And Beautiful, until of course it's replaced by The Next Big Thing.  Hey, anyone remember \"Dart\"?  It's backed by Google so you know it'll be around forever!!!\n\nMeanwhile, you gotta love a language that didn't even have exception handling, and then only included it after saying it shouldn't be used.  Hey, let's have two-arg returns and error checking everywhere!  That'll make things easy to read!\n \nYeeeeeah.  For getting stuff done I'll stick with C++/Java/Python.  ", "aSentId": 58338, "answer": "&gt; Hey, anyone remember \"Dart\"? It's backed by Google so you know it'll be around forever!!!\n\nI guess one benefit of Go's simplicity is that it doesn't really matter if Google stops *backing* it soon: it's already feature-complete by many gophers' standards.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58337, "question": "Agreed, but the Go fanboys have declared that Their Language Is Perfect And Beautiful, until of course it's replaced by The Next Big Thing.  Hey, anyone remember \"Dart\"?  It's backed by Google so you know it'll be around forever!!!\n\nMeanwhile, you gotta love a language that didn't even have exception handling, and then only included it after saying it shouldn't be used.  Hey, let's have two-arg returns and error checking everywhere!  That'll make things easy to read!\n \nYeeeeeah.  For getting stuff done I'll stick with C++/Java/Python.  ", "aSentId": 58340, "answer": "I like to program in go but I'd never call it perfect. If only there was a perfect language...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58305, "question": "Go's source code also looks ugly as hell.  It's like some weird bastard hybrid of Python and C#.  Thanks; I'll pass.\n\nAnd BTW, what's with capitalizing method names?  Did Google just copy this awful C# convention to woo Microsoft developers?  Yeah, I know, it's about visibility, but a more sensible (and industry-standard) approach would have been:\n\n    public()\n    _private()\n\ninstead of:\n\n    Public()\n    private()\n \n\n\n", "aSentId": 58342, "answer": "Neither of those are C# conventions.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58345, "question": "Wow, I'm impressed!\n\nThe authors of Go have, between them, created Plan 9 and Inferno, wrote the first windowing system for Unix, UTF-8, the B programming language and the C programming language.\n\nThier awards include: National Academy of Engineering, Turing award, Fellow of the Computer History Museum, National Medal of Technology as well as the Silver medal in Archery (1980 Olympics).\n\nBut they way speak of their shortcomings as language designers leads me to believe that you are far more accomplished and able as a developer.\n\nThis being the case, I'm amazed I haven't heard of you yet. Could you please share with me some of your accomplishments that have led you to feel free to dismiss their work and success with Go?", "aSentId": 58346, "answer": "I'm getting *real* fucking sick of the appeals to authority on the topic of golang. \n\n*Well maybe this part of the language is not good.... but who am I to question the infallibility of the Unix wizards. And who are you, really? They have done so much and achieved so much, they are beyond reproach!*\n\nNo, implementing operating systems and encodings doesn't imply that you'll also be a good language designer. Sorry. The B and C languages are more relevant. But they're both languages from the 70s (or 60s in B's case?), and one of the primary complaints of Go is that the language design is stuck three decades in the past (in a bad way, arguably ignoring the good and proven things that have come out of PL research).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58347, "question": "I'm getting *real* fucking sick of the appeals to authority on the topic of golang. \n\n*Well maybe this part of the language is not good.... but who am I to question the infallibility of the Unix wizards. And who are you, really? They have done so much and achieved so much, they are beyond reproach!*\n\nNo, implementing operating systems and encodings doesn't imply that you'll also be a good language designer. Sorry. The B and C languages are more relevant. But they're both languages from the 70s (or 60s in B's case?), and one of the primary complaints of Go is that the language design is stuck three decades in the past (in a bad way, arguably ignoring the good and proven things that have come out of PL research).", "aSentId": 58348, "answer": "You're going to see a lot of them: that's the only possible defense for golang's terrible design.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58345, "question": "Wow, I'm impressed!\n\nThe authors of Go have, between them, created Plan 9 and Inferno, wrote the first windowing system for Unix, UTF-8, the B programming language and the C programming language.\n\nThier awards include: National Academy of Engineering, Turing award, Fellow of the Computer History Museum, National Medal of Technology as well as the Silver medal in Archery (1980 Olympics).\n\nBut they way speak of their shortcomings as language designers leads me to believe that you are far more accomplished and able as a developer.\n\nThis being the case, I'm amazed I haven't heard of you yet. Could you please share with me some of your accomplishments that have led you to feel free to dismiss their work and success with Go?", "aSentId": 58350, "answer": "Holy shit, they made C *and* Plan 9?  You've convinced me that they are people I want to emulate in all of my programming decisions!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58351, "question": "Holy shit, they made C *and* Plan 9?  You've convinced me that they are people I want to emulate in all of my programming decisions!", "aSentId": 58352, "answer": "Not to mention the archery. (Which is probably just a gag, anyway.)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58345, "question": "Wow, I'm impressed!\n\nThe authors of Go have, between them, created Plan 9 and Inferno, wrote the first windowing system for Unix, UTF-8, the B programming language and the C programming language.\n\nThier awards include: National Academy of Engineering, Turing award, Fellow of the Computer History Museum, National Medal of Technology as well as the Silver medal in Archery (1980 Olympics).\n\nBut they way speak of their shortcomings as language designers leads me to believe that you are far more accomplished and able as a developer.\n\nThis being the case, I'm amazed I haven't heard of you yet. Could you please share with me some of your accomplishments that have led you to feel free to dismiss their work and success with Go?", "aSentId": 58354, "answer": "Appeal to authority doesn't work when the majority of your examples are failures.\n\nPlan 9 and Inferno effectively don't exist for the majority of programmers.  B obviously got replaced by C.  Unix only succeeded because it was free when nothing else was.\n\nSo, the only real successes in that list are C and UTF-8.  And a whole bunch of Asians don't really agree that UTF-8 is good.  And a whole bunch of programming languages seem to argue that C isn't very good.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58357, "question": "D 2.067 released with 506 improvements across compiler, runtime, stdlib, and website", "aSentId": 58358, "answer": "\"D rocks\".writeln;", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58359, "question": "\"D rocks\".writeln;", "aSentId": 58360, "answer": "Is `writeln` really a method on strings?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58361, "question": "Is `writeln` really a method on strings?", "aSentId": 58362, "answer": "No. It is a result of Uniform function call syntax (UFCS): http://ddili.org/ders/d.en/ufcs.html\n\ni.e. that is the same with writeln(\"D rocks\");", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58361, "question": "Is `writeln` really a method on strings?", "aSentId": 58364, "answer": "See [Universai Function Call Syntax](http://ddili.org/ders/d.en/ufcs.html).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58357, "question": "D 2.067 released with 506 improvements across compiler, runtime, stdlib, and website", "aSentId": 58366, "answer": "Can someone with experience with D and Rust comment on why one would pick one over the other? I see benefits to both but my casual observation is that Rust has a lot more community around it.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58367, "question": "Can someone with experience with D and Rust comment on why one would pick one over the other? I see benefits to both but my casual observation is that Rust has a lot more community around it.", "aSentId": 58368, "answer": "D and Nim are more similar than D to Rust. Although, all three languages are fantastic!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58369, "question": "D and Nim are more similar than D to Rust. Although, all three languages are fantastic!", "aSentId": 58370, "answer": "&gt; all three languages are fantastic\n\nI don't know if you're just saying that to avoid a language war.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58367, "question": "Can someone with experience with D and Rust comment on why one would pick one over the other? I see benefits to both but my casual observation is that Rust has a lot more community around it.", "aSentId": 58372, "answer": "I find that D and Rust are compared a lot but actually end up appealing to different demographics.\n\nFor a personal project, I would pick one over the other based entirely on your preferred style. Rust seems to appeal to functional programmers more, and has lots of obvious ML-influence. D is much more like a cleaned up C++.\n\nI don't think you'll go wrong with either, they're both under active development and have lively communities. Choice is good.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58367, "question": "Can someone with experience with D and Rust comment on why one would pick one over the other? I see benefits to both but my casual observation is that Rust has a lot more community around it.", "aSentId": 58374, "answer": "Rust's all-consuming mantra is \"memory safety without garbage collection\". If you don't particularly value memory safety or if you don't have a problem with garbage collection, then many of the tradeoffs that it makes may not make sense for your application. If you disregard that, Rust and D have a very different \"feel\", which tends to mean that people end up identifying with one or the other out of sheer personal preference rather than for technical reasons. I suggest you try both and see which one you prefer.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58357, "question": "D 2.067 released with 506 improvements across compiler, runtime, stdlib, and website", "aSentId": 58377, "answer": "Can someone comment on how the situation looks on the GCC and LLVM compilers? I was told they were less trigger happy on regressions and better optimizing, which sounds plausible, given the amount of work already put in their backends.\n\nHow long does it usually take until such an update is supported?\n\nHow does the situation look for interfacing C++ code with these compilers? If I recall correctly DMD can interface to classes and templates.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58378, "question": "Can someone comment on how the situation looks on the GCC and LLVM compilers? I was told they were less trigger happy on regressions and better optimizing, which sounds plausible, given the amount of work already put in their backends.\n\nHow long does it usually take until such an update is supported?\n\nHow does the situation look for interfacing C++ code with these compilers? If I recall correctly DMD can interface to classes and templates.", "aSentId": 58379, "answer": "LDC(D's LLVM compiler) already has a working 2.067 branch that should be merged into master soon. GDC moves much slower.\n\nAnd yes, both are much better at optimizing. DMD is a reference implementation.\n\nMy two cents: use LDC. It has a lot more supporters volunteering, is more frequently updated, has better documentation. GDC sometimes produces better code when you write code in C-style, but when you prefer D-style(aka, ranges/functional) LDC generally runs circles around it.\n\nAll of the working D compilers(DMD,LDC,GDC) share the same frontend so there's actually surprisingly little code duplication. DDMD is a port of the frontend to D and is nearing stable, and SDC is a complete rewrite that is coming along nicely AFAIK.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58380, "question": "LDC(D's LLVM compiler) already has a working 2.067 branch that should be merged into master soon. GDC moves much slower.\n\nAnd yes, both are much better at optimizing. DMD is a reference implementation.\n\nMy two cents: use LDC. It has a lot more supporters volunteering, is more frequently updated, has better documentation. GDC sometimes produces better code when you write code in C-style, but when you prefer D-style(aka, ranges/functional) LDC generally runs circles around it.\n\nAll of the working D compilers(DMD,LDC,GDC) share the same frontend so there's actually surprisingly little code duplication. DDMD is a port of the frontend to D and is nearing stable, and SDC is a complete rewrite that is coming along nicely AFAIK.", "aSentId": 58381, "answer": "It's good to hear the alternatives are really that usable.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58378, "question": "Can someone comment on how the situation looks on the GCC and LLVM compilers? I was told they were less trigger happy on regressions and better optimizing, which sounds plausible, given the amount of work already put in their backends.\n\nHow long does it usually take until such an update is supported?\n\nHow does the situation look for interfacing C++ code with these compilers? If I recall correctly DMD can interface to classes and templates.", "aSentId": 58383, "answer": "D can now interface to C++ namespaces.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58384, "question": "D can now interface to C++ namespaces.", "aSentId": 58385, "answer": "That's nice to hear.\nFrom the perspective of a compiler writer it sounds pretty challenging keeping up with the C++ compilers ABI, once it changes in another release.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58388, "question": "Michael Stonebraker wins $1 million Turing Award. This year marks the first time that the Turing Award comes with a Google-funded $1 million prize.", "aSentId": 58389, "answer": "All I could think of is that, that would be a good Skyrim name.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58391, "question": "Apple Acquires Durable Database Company FoundationDB", "aSentId": 58392, "answer": "You guys will now work and maintain Filemaker.\n\n\n\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58391, "question": "Apple Acquires Durable Database Company FoundationDB", "aSentId": 58394, "answer": "Thank You, Community, for all your help on this.  \n\nWe are now taking the product you helped us build, selling it to Apple, taking the money and see you around!!!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58391, "question": "Apple Acquires Durable Database Company FoundationDB", "aSentId": 58396, "answer": "Now I'm gladder than ever that I built all my stuff on top of open-source databases with large, active communities.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58399, "question": "Any reason why someone would use this in place of more \"credible alternatives\"? ", "aSentId": 58400, "answer": "According to the article no not really it just looks like Apple wants their own proprietary database to run their own cloud services rather than being a client of somebody else.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58399, "question": "Any reason why someone would use this in place of more \"credible alternatives\"? ", "aSentId": 58402, "answer": "Better performance", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58399, "question": "Any reason why someone would use this in place of more \"credible alternatives\"? ", "aSentId": 58404, "answer": "I almost used it for a project. It's NoSQL but it's also ACID and has good performance. Which is a nice combo, and I don't think anything in the open source world really matches it.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58405, "question": "I almost used it for a project. It's NoSQL but it's also ACID and has good performance. Which is a nice combo, and I don't think anything in the open source world really matches it.", "aSentId": 58406, "answer": "CockroachDB may eventually get there.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58407, "question": "CockroachDB may eventually get there.", "aSentId": 58408, "answer": "Where your data checks in but then you can't get it back out.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58409, "question": "Where your data checks in but then you can't get it back out.", "aSentId": 58410, "answer": "You mean dev/null? I hear it's pretty webscale...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58411, "question": "You mean dev/null? I hear it's pretty webscale...", "aSentId": 58412, "answer": "It's the roach motel of databases. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58407, "question": "CockroachDB may eventually get there.", "aSentId": 58414, "answer": "That's interesting, thanks for the link. Seems like it has a bunch of the things I would want from foundation or datomic without the closed part, which is awesome.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58421, "question": "In January, Fortune magazine reported that Apple has, so far, $158,000,000,000 in 'overseas cash reserves', foreign earnings Apple was keeping offshore to avoid paying a 35% U.S. capital gains repatriation tax.  That's up about $58,000,000,000 from the $100,000,000,000 it had just two years ago.  I can't help wondering if Apple used some of that money to buy FoundationDB.   And that is profit, not sales.  Big difference.\n\nApple has increased its amount in this cash horde at the rate of about $110,000 a minute over the past two years.  It could have bought this company with just a few hours, or even just a few minutes of its offshore earnings, not even touching its onshore earnings.", "aSentId": 58422, "answer": "What's your point ?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58423, "question": "What's your point ?", "aSentId": 58424, "answer": "Programmers love it when programmers pretend to play dumb thinking it proves they're smarter than everyone else.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58425, "question": "Programmers love it when programmers pretend to play dumb thinking it proves they're smarter than everyone else.", "aSentId": 58426, "answer": "No really, reread your post, it's not relevant to the topic and just states that Apple has money. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58427, "question": "No really, reread your post, it's not relevant to the topic and just states that Apple has money. ", "aSentId": 58428, "answer": "No, really, money is what companies use to acquire control and power.  ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58429, "question": "No, really, money is what companies use to acquire control and power.  ", "aSentId": 58430, "answer": "...yes, as is logical. What's your point ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58431, "question": "...yes, as is logical. What's your point ", "aSentId": 58432, "answer": "Recursion offers the answer you seek.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58421, "question": "In January, Fortune magazine reported that Apple has, so far, $158,000,000,000 in 'overseas cash reserves', foreign earnings Apple was keeping offshore to avoid paying a 35% U.S. capital gains repatriation tax.  That's up about $58,000,000,000 from the $100,000,000,000 it had just two years ago.  I can't help wondering if Apple used some of that money to buy FoundationDB.   And that is profit, not sales.  Big difference.\n\nApple has increased its amount in this cash horde at the rate of about $110,000 a minute over the past two years.  It could have bought this company with just a few hours, or even just a few minutes of its offshore earnings, not even touching its onshore earnings.", "aSentId": 58434, "answer": "This just in, companies use money to buy things.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58435, "question": "This just in, companies use money to buy things.", "aSentId": 58436, "answer": "This just in:\n\nCompanies have so much money they have no idea what to do with it.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58421, "question": "In January, Fortune magazine reported that Apple has, so far, $158,000,000,000 in 'overseas cash reserves', foreign earnings Apple was keeping offshore to avoid paying a 35% U.S. capital gains repatriation tax.  That's up about $58,000,000,000 from the $100,000,000,000 it had just two years ago.  I can't help wondering if Apple used some of that money to buy FoundationDB.   And that is profit, not sales.  Big difference.\n\nApple has increased its amount in this cash horde at the rate of about $110,000 a minute over the past two years.  It could have bought this company with just a few hours, or even just a few minutes of its offshore earnings, not even touching its onshore earnings.", "aSentId": 58438, "answer": "This made me think back to my childhood, remembering that giant money ticker Claw had on his desk in Inspector Gadget. Guess that wasn't really as crazy as it looked.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58440, "question": "The lesson for me here is that if the license of a tool is anything other than GPL or APL, I can be assured that it can go away anytime. Nothing that can really be done about it as well.", "aSentId": 58441, "answer": "No, even with a copy-free license like BSD, someone could re-upload the source.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58440, "question": "The lesson for me here is that if the license of a tool is anything other than GPL or APL, I can be assured that it can go away anytime. Nothing that can really be done about it as well.", "aSentId": 58443, "answer": "Or MIT, MPL, BSD...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58440, "question": "The lesson for me here is that if the license of a tool is anything other than GPL or APL, I can be assured that it can go away anytime. Nothing that can really be done about it as well.", "aSentId": 58445, "answer": "The license doesn't really matter, since they can remove the product entirely regardless of the license. Any free/open source license would allow someone to upload the latest version again, be it GPL, MIT or anything else.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58447, "question": "Not programming.", "aSentId": 58448, "answer": "Very relevant to anyone who may be using this DB system.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58450, "question": "The Racket Manifesto", "aSentId": 58451, "answer": "\"Hence, Racket is a programming language for creating new programming languages.\"\n\nWait, Racket is now aimed as a language to build other languages now?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58452, "question": "\"Hence, Racket is a programming language for creating new programming languages.\"\n\nWait, Racket is now aimed as a language to build other languages now?", "aSentId": 58453, "answer": "i asked the same question on the #racket irc channel. samth replied:\n\n    the point of the manifesto is that a language-building tool is what you want for general-purpose programming\n    racket has lots of goals -- at least as many goals as developers", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58452, "question": "\"Hence, Racket is a programming language for creating new programming languages.\"\n\nWait, Racket is now aimed as a language to build other languages now?", "aSentId": 58455, "answer": "\"Now?\" What was it before? (and when was \"before\"?)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58456, "question": "\"Now?\" What was it before? (and when was \"before\"?)", "aSentId": 58457, "answer": "I thought it was a web framework with server support for \"continuations\". I guess I was mistaken.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58458, "question": "I thought it was a web framework with server support for \"continuations\". I guess I was mistaken.", "aSentId": 58459, "answer": "/r/programming sucks", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58458, "question": "I thought it was a web framework with server support for \"continuations\". I guess I was mistaken.", "aSentId": 58461, "answer": "This comment is amazing", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58452, "question": "\"Hence, Racket is a programming language for creating new programming languages.\"\n\nWait, Racket is now aimed as a language to build other languages now?", "aSentId": 58463, "answer": "With racket you're not bound to the rules of the language designer. You can extend your own control flows as you please. For example through macros you could add structs/records that automatically add getters and setters, pattern matching and Erlang style binary matching. The downside is that if you get too creative other programmers won't be able to read what the fuck you're doing. What is the best programming language? Well in racket you can pick and choose what features you like and still get decent performance. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58466, "question": "IntelliJ IDEA 14.1 is Here!", "aSentId": 58467, "answer": "The IDEA is now way faster, nice work guys!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58468, "question": "The IDEA is now way faster, nice work guys!", "aSentId": 58469, "answer": "&gt; way faster\n\nI've seen this claim on _literally_ every new release, but it's still no where near vim or a reasonable emacs setup. Does your \"way faster\" mean 0.1% faster?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58470, "question": "&gt; way faster\n\nI've seen this claim on _literally_ every new release, but it's still no where near vim or a reasonable emacs setup. Does your \"way faster\" mean 0.1% faster?", "aSentId": 58471, "answer": "Yo, this is not comparable. Faster is relative to what it was before. This is just flame baiting. Chill out.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58475, "question": "Coding by the Book: 7 Books Every Software Developer Should Read", "aSentId": 58476, "answer": "Mythical Man Month &amp; Pragmatic Programmer - omg yes, absolutely. Not a day at work goes by that the lessons in those books don't repeat, like history.\n\nCode - not so much. I read it. It's cool. It's for beginners, not veterans.\n\nFinally \"The Art of Computer Programming by Donald Knuth\".. is one of those books we all know we should read and fully understand. Some of us even pretend to... but we also know very few of us have both read and fully understood Knuth's books.\n\nWill have to look into 'Working Effectively with Legacy Code'.. I have this slowly developing theory that the next area ripe for big improvement is not \"How to write brand new perfect code\" but actually \"How to deal with the reality of constantly refactoring legacy code into something good\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58475, "question": "Coding by the Book: 7 Books Every Software Developer Should Read", "aSentId": 58478, "answer": "Surprised no one immediately mentioned SICP.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58479, "question": "Surprised no one immediately mentioned SICP.", "aSentId": 58480, "answer": "SICP is far more useful to the working programmer than TAOCP.  I won't even pretend to have read TAOCP (I've dipped into it when I needed very detailed instructions on particular topics - mostly random number generation).  SICP probably did more to teach me how to reason about computation than anything else I've ever read.  It is the closest thing to a masterpiece in Computer Science.  I would easily boot TAOCP off that list in favour of a really useful book like SICP.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58475, "question": "Coding by the Book: 7 Books Every Software Developer Should Read", "aSentId": 58482, "answer": "I doubt there are more than 100 people worldwide who have read TAOCP from end-to-end and understood it. It's an extremely dense set of tomes with massive prerequisites to understanding.\n\nIn general, this list is pretty flabby. I'd recommend *Code Complete* and *The Cathedral and the Bazaar* long before half the books on here.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58483, "question": "I doubt there are more than 100 people worldwide who have read TAOCP from end-to-end and understood it. It's an extremely dense set of tomes with massive prerequisites to understanding.\n\nIn general, this list is pretty flabby. I'd recommend *Code Complete* and *The Cathedral and the Bazaar* long before half the books on here.\n", "aSentId": 58484, "answer": "Definitely with you on The Cathedral and the Bazaar, really points out the differences in open source and closed source development.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58483, "question": "I doubt there are more than 100 people worldwide who have read TAOCP from end-to-end and understood it. It's an extremely dense set of tomes with massive prerequisites to understanding.\n\nIn general, this list is pretty flabby. I'd recommend *Code Complete* and *The Cathedral and the Bazaar* long before half the books on here.\n", "aSentId": 58486, "answer": "You will find *many* arrogant assholes who claim to have read through it though.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58488, "question": "I'll add \"The Art of Unix Programming\". A compendium of lessons the Unix Culture has deliver to the whole Software Developing World.", "aSentId": 58489, "answer": "I would add Clean Code by Robert C. Martin.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58490, "question": "I would add Clean Code by Robert C. Martin.", "aSentId": 58491, "answer": "Ugh, that book is riddled with bias inmho", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58492, "question": "Ugh, that book is riddled with bias inmho", "aSentId": 58493, "answer": "Bias?  Towards what?  Do you mean it's highly opinionated?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58494, "question": "Bias?  Towards what?  Do you mean it's highly opinionated?", "aSentId": 58495, "answer": "Yes, that would be a better word. I guess I meant bias towards his opinion, so that is totally a better word. \n\nI do find a lot of it to be idealist and not pragmatic in a lot of cases. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58499, "question": "Nobody reads 2 and 3 end to end.\n\nI'd replace them with Programming Pearls and Code Complete.", "aSentId": 58500, "answer": "Maybe not \"fully end to end with no gaps\", but #3 (CLRS) is pretty approachable if you're willing to skip the \"mathematically prove the Big-O\" bits.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58501, "question": "Maybe not \"fully end to end with no gaps\", but #3 (CLRS) is pretty approachable if you're willing to skip the \"mathematically prove the Big-O\" bits.", "aSentId": 58502, "answer": "But the Big-O bits a really important. If you can get good at the mathematical proofs you can learn to quickly sniff out a poor design choice almost out of instinct latter. Then, when someone argues against your sniff test you can crush them with a proof, muahahahaha!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58503, "question": "But the Big-O bits a really important. If you can get good at the mathematical proofs you can learn to quickly sniff out a poor design choice almost out of instinct latter. Then, when someone argues against your sniff test you can crush them with a proof, muahahahaha!", "aSentId": 58504, "answer": "\"Tune in for our Fight-Night exclusive: Proof vs. Benchmark in a battle of epic intractability! *Who will win!?*\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58506, "question": "Mike Stonebraker wins 2015 Turing Award", "aSentId": 58507, "answer": "Is anyone else getting 403 Forbidden on this link, or is it just me?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58508, "question": "Is anyone else getting 403 Forbidden on this link, or is it just me?", "aSentId": 58509, "answer": "Not me.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58506, "question": "Mike Stonebraker wins 2015 Turing Award", "aSentId": 58511, "answer": "I've met Stonebraker before (another context, long story), he's a nice guy + has done a huge amount of work in the field. I guess though that I'm ambivalent about giving large cash awards to established high-profile academics who are well connected in the business world. I'd like to see $$$ given to people that have done a lot of difficult, essential work in a field and haven't seen the financial rewards that a Silicon Valley founder or MIT professor would get from their work anyway.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58506, "question": "Mike Stonebraker wins 2015 Turing Award", "aSentId": 58513, "answer": "**Well**, well deserved!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58516, "question": "Seems like this always goes to theoretical academics - nice to see them recognize somebody who's done a lot to put his ideas into practice. ", "aSentId": 58517, "answer": "While I agree with the conclusion, it's the _Turing_ award. It's intended to go to people who make foundational theoretical breakthroughs like Turing did. Now, of course, Turing also did applied work, especially during WWII, and thank God for that. But what he's rightly remembered for in computer science is his theoretical work.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58518, "question": "While I agree with the conclusion, it's the _Turing_ award. It's intended to go to people who make foundational theoretical breakthroughs like Turing did. Now, of course, Turing also did applied work, especially during WWII, and thank God for that. But what he's rightly remembered for in computer science is his theoretical work.", "aSentId": 58519, "answer": "The Turing Award can go to anyone who makes a solid contribution to computer science. For purely theory, we have the Godel Prize.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58521, "question": "The OpenGL Extension Wrangler Library on github", "aSentId": 58522, "answer": "why post this? doesnt everybody using opengl know about this first day?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58523, "question": "why post this? doesnt everybody using opengl know about this first day?", "aSentId": 58524, "answer": "&gt; doesnt everybody using opengl know about this first day?\n\nno", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58523, "question": "why post this? doesnt everybody using opengl know about this first day?", "aSentId": 58526, "answer": "I'm also confused as to why this is posted. However, I don't recommend GLEW to any OpenGL beginner. Rather, GLFW gets my vote. It is trivial to set up and really easy to use to create your quick graphics engine.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58527, "question": "I'm also confused as to why this is posted. However, I don't recommend GLEW to any OpenGL beginner. Rather, GLFW gets my vote. It is trivial to set up and really easy to use to create your quick graphics engine.", "aSentId": 58528, "answer": "? GLFW and GLEW are seperate entities. GLFW simply creates a window/context; it doesn't do any of the extension fetching that GLEW does.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58529, "question": "? GLFW and GLEW are seperate entities. GLFW simply creates a window/context; it doesn't do any of the extension fetching that GLEW does.", "aSentId": 58530, "answer": "That's what I was trying to get at. GLEW requires that you have a context up and running, so using it without GLFW or freeglut will give the beginner erroneous errors that they know nothing about. So my advice is, simply use GLFW and naked openGL", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58531, "question": "That's what I was trying to get at. GLEW requires that you have a context up and running, so using it without GLFW or freeglut will give the beginner erroneous errors that they know nothing about. So my advice is, simply use GLFW and naked openGL", "aSentId": 58532, "answer": "&gt; naked openGL.\n\nEh, care to define what that is? From what it sounds like, you're referring to 1.0...\n\nSo, I'm not sure what the deal is on Linux these days (glext.h, maybe?). My understanding is also that OSX has its own specific way of handling this, but GLEW does one thing and one thing only: load function pointers. Some of these are ARB'd, and thus core to the GL standard. \n\nOthers are actual extensions which may or may not be available (read: GLEW allows users to perform runtime checks for arbitrary extensions via their API).\n\nBut, that's the thing: Windows by default is only aware of functionality which is available to uber-old-90s-era GL. To gain any additional functionality, function pointers have to be individually loaded via the driver vendor/implementor's .dll at runtime - each pointer references an OpenGL function, be it core or extension.\n\nLast I remember, `wglGetProcAddress` is what's used for that.\n\n So, here's the thing: you either use an extension loading library of some sort, hand craft *each function declaration and constant flag/enum by hand*, or use oldskool obsolete GL.\n\nGLEW is a library which provides this functionality, among others.\n\nI have not heard of any other means of using modern or otherwise post 1.x functionality. I'd be happy to be enlightened on this though.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58527, "question": "I'm also confused as to why this is posted. However, I don't recommend GLEW to any OpenGL beginner. Rather, GLFW gets my vote. It is trivial to set up and really easy to use to create your quick graphics engine.", "aSentId": 58534, "answer": "There's no conflict there, so no need for \"rather\". I use both and everything's good. Don't have to handle creating my own contexts or dealing with extensions.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58536, "question": "Dart for the Entire Web [Official Blog]", "aSentId": 58537, "answer": "So what's the advantage of Dart over TypeScript now ?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58538, "question": "So what's the advantage of Dart over TypeScript now ?", "aSentId": 58539, "answer": "What was the advantage of Dart over TypeScript yesterday?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58540, "question": "What was the advantage of Dart over TypeScript yesterday?", "aSentId": 58541, "answer": "None.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58538, "question": "So what's the advantage of Dart over TypeScript now ?", "aSentId": 58543, "answer": "I would also like to know.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58536, "question": "Dart for the Entire Web [Official Blog]", "aSentId": 58545, "answer": "GWT 2.0...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58536, "question": "Dart for the Entire Web [Official Blog]", "aSentId": 58547, "answer": "There's a gazillion languages that compile to javascript nowadays. From shitty ones like Coffescript to really good ones like PureScript, ClojureScript and Elm. Heck, even major languages like Scala and Haskell compile to js.\n\nThis announcement knocks off Dart from its pedestal of a potential javascript rival to the crowd of LanguageX2js compilers. And in that crowd Dart does not stand out.\n \n\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58549, "question": "I think I would opt for [HaXe](http://haxe.org) over Dart.", "aSentId": 58550, "answer": "It's written \"Haxe\" instead of \"haXe\" nowadays.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58549, "question": "I think I would opt for [HaXe](http://haxe.org) over Dart.", "aSentId": 58552, "answer": "Damn, that looks interesting. Anybody used it for anything non-trivial? Experiences?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58553, "question": "Damn, that looks interesting. Anybody used it for anything non-trivial? Experiences?", "aSentId": 58554, "answer": "Haven't used it but my biggest disappointment with it is that its actually really crappy to call haxe code from within the native environment, its only nice the other way around. This seems like a trivial distinction but it isnt; it means that writing the ui natively is a mess when it relays anything into haxe. They pitch it as a \"toolkit\" because includes a cross platform library that's probably got its own slew of issues. Haven't used it though, so I could be wrong.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58553, "question": "Damn, that looks interesting. Anybody used it for anything non-trivial? Experiences?", "aSentId": 58556, "answer": "I have used it with great success creating small interactive canvas activities for eLearning clients. This stuff would have been easy to develop 5 years ago when Flash was in heavy use, but I was recently having a lot of trouble creating a nice and reusable codebase using Javascript.\n\nUsing Haxe + Haxe Sublime Plugin has greatly alleviated so much development pain. For an example a few weeks ago I did an hour long refactor of my codebase, not leaving the text editor once. Of course Haxe let me know about any errors at compile time. I spent maybe 20 minutes going through and cleaning up all of the compile errors. Once the program successfully compiled I tested it in the browser it worked first go, no runtime errors. There is not a chance that I would have this sort of luck working with Javascript. The typing system is extremely powerful and you really trust it.\n\nI would say Haxe is my favorite language. I find it allows me to create extremely elegant code. It is just a shame that the community isn't very large. I think what Haxe needs is a killer (well documented) web framework to get some more recognition, because at the moment it doesn't get much use outside of games.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58558, "question": "I'm excited about the potential of Dart. Is there anyone using it day to day that would like to share their experiences?", "aSentId": 58559, "answer": "I use it for hobby projects almost every day. The best thing about Dart is the ease of use. You can find a lot of stuff in the language like HTML API, Unit Test, Async, Isolates and Documentation generation.\n\nDart is kind of an all rounder in web development.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58558, "question": "I'm excited about the potential of Dart. Is there anyone using it day to day that would like to share their experiences?", "aSentId": 58561, "answer": "Did you read the blog? They are discontinuing the Dart VM: \"We have decided not to integrate the Dart VM into Chrome.\", and spend the next few paragraphs claiming how much Dart is used for critical services, so the rest of Dart \"won't go away, for sure!\". They wouldn't need to do that, if there weren't valid concerns about the this.\n\nThere is no potential for Dart. If you want to use the web's native language, it will be only JavaScript now. If you want to use a language which doesn't suck, you have plenty of alternatives already, which are much better than Dart.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58562, "question": "Did you read the blog? They are discontinuing the Dart VM: \"We have decided not to integrate the Dart VM into Chrome.\", and spend the next few paragraphs claiming how much Dart is used for critical services, so the rest of Dart \"won't go away, for sure!\". They wouldn't need to do that, if there weren't valid concerns about the this.\n\nThere is no potential for Dart. If you want to use the web's native language, it will be only JavaScript now. If you want to use a language which doesn't suck, you have plenty of alternatives already, which are much better than Dart.", "aSentId": 58563, "answer": "You must have read a different announcement.  The Dart VM is alive and well, and will continue to be supported.  What is being dropped is trying to get the Dart VM pushed into Chromium.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58564, "question": "You must have read a different announcement.  The Dart VM is alive and well, and will continue to be supported.  What is being dropped is trying to get the Dart VM pushed into Chromium.", "aSentId": 58565, "answer": "The Dart VM will continue to be supported, but there is no chance it will get the same focus it would get if it were shipping in Chrome, which now it won't.\n\nI would expect the team to scale down, but not disappear.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58566, "question": "The Dart VM will continue to be supported, but there is no chance it will get the same focus it would get if it were shipping in Chrome, which now it won't.\n\nI would expect the team to scale down, but not disappear.", "aSentId": 58567, "answer": "I'm not aware of any plans to shrink the VM team. While they won't be landing their VM in Chrome, they are still developing it for command-line and server apps, as well as investigating a bunch of other platforms that might benefit from it.\n\nIt's a *very* impressive piece of software, and it would be foolish not to find cool places to take advantage of it. And, of course, the Dart platform itself uses it heavily. Almost all of our tools\u2014package manager, dart-&gt;JS compiler, etc.\u2014are self-hosted and written in Dart. They run on top of the VM.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58568, "question": "I'm not aware of any plans to shrink the VM team. While they won't be landing their VM in Chrome, they are still developing it for command-line and server apps, as well as investigating a bunch of other platforms that might benefit from it.\n\nIt's a *very* impressive piece of software, and it would be foolish not to find cool places to take advantage of it. And, of course, the Dart platform itself uses it heavily. Almost all of our tools\u2014package manager, dart-&gt;JS compiler, etc.\u2014are self-hosted and written in Dart. They run on top of the VM.", "aSentId": 58569, "answer": "&gt; I'm not aware of any plans to shrink the VM team\n\nThis is Google we're talking about. They'll pretend to maintain an active interest in it, then one day finally admit they're bored of the project and End-of-Lifeing it in 6-12 months.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58564, "question": "You must have read a different announcement.  The Dart VM is alive and well, and will continue to be supported.  What is being dropped is trying to get the Dart VM pushed into Chromium.", "aSentId": 58571, "answer": "What does Dart offer compared to TypeScript now that they are going toe to toe in JS transpiler space?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58572, "question": "What does Dart offer compared to TypeScript now that they are going toe to toe in JS transpiler space?", "aSentId": 58573, "answer": "A bunch of stuff:\n\n* **Dart's type system is more nominal and less structural.** There are advantages to either approach, but nominal types are simpler, lead to simpler error messages, and are more familiar to users since most other static type systems are nominally typed.\n\n* **Dart is generally easier to statically analyze.** The TypeScript folks have done an impressive job, but they have an uphill battle since the language they're analyzing is fundamentally mutable and imperative. In Dart, the structure of your program\u2014the functions, classes, and methods it defines\u2014are all statically declared. You don't have to do a lot of dataflow heavy lifting to reconstitute what is effectively declarative code anyway like you do in TS/Flow.\n\n* **Dart subtracts out a lot of JS misfeatures:** implicit conversions, wacky `this` binding, function hoisting, ugly function syntax, lame `var` scope, etc. TypeScript can *add* to JS but it can't substract. Since Dart is a separate language and not a superset of JS, it can start with a clean slate.\n\n* **Dart has a meaningful distinction between methods and functions.** Most of the confusing behavior in JS around `this` is because JS doesn't actually have methods. It just has functions attached to object invoked a certain way.\n\n    Dart does have methods, which means it can make `this` behavior correctly when you use it in a closure. In JS, you have to do lots of weird workarounds (`Function.bind()`, `var that = this`, fat arrow functions, etc.). In Dart it just works.\n\n* **Dart has a meaningful distinction between objects and data structures.** JS's \"everything is a property bag\" is neat from a minimalism point of view, but I think it's too primitive for large applications. It leads to lots to confusing error messages on missing properties, confusing stuff like `getOwnedProperties()`, etc. Dart treats objects as objects and has a Map type when you want a bag of data.\n\n* **Dart has a new runtime library.** We have a really nice full-featured set of collections with all of the higher-order functions you know and love, base classes that can be inherited or mixed in, etc. There's no need for something like underscore in Dart.\n\n    Likewise, Futures and Streams are baked into the language and core libraries, so you don't have to deal with a slew of incompatible ways of doing asynchrony. Futures and Streams are the canonical objects for it, and `async/await` is the syntax to work with them.\n\n**Then there's just a long list of syntax and semantic niceties.** Dart really is its own language. If you sit down and write a few thousand lines of code, you'll find a lot of fun little features that make your code neater, cleaner, and easier to maintain. I really enjoy spending my day writing Dart code.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58574, "question": "A bunch of stuff:\n\n* **Dart's type system is more nominal and less structural.** There are advantages to either approach, but nominal types are simpler, lead to simpler error messages, and are more familiar to users since most other static type systems are nominally typed.\n\n* **Dart is generally easier to statically analyze.** The TypeScript folks have done an impressive job, but they have an uphill battle since the language they're analyzing is fundamentally mutable and imperative. In Dart, the structure of your program\u2014the functions, classes, and methods it defines\u2014are all statically declared. You don't have to do a lot of dataflow heavy lifting to reconstitute what is effectively declarative code anyway like you do in TS/Flow.\n\n* **Dart subtracts out a lot of JS misfeatures:** implicit conversions, wacky `this` binding, function hoisting, ugly function syntax, lame `var` scope, etc. TypeScript can *add* to JS but it can't substract. Since Dart is a separate language and not a superset of JS, it can start with a clean slate.\n\n* **Dart has a meaningful distinction between methods and functions.** Most of the confusing behavior in JS around `this` is because JS doesn't actually have methods. It just has functions attached to object invoked a certain way.\n\n    Dart does have methods, which means it can make `this` behavior correctly when you use it in a closure. In JS, you have to do lots of weird workarounds (`Function.bind()`, `var that = this`, fat arrow functions, etc.). In Dart it just works.\n\n* **Dart has a meaningful distinction between objects and data structures.** JS's \"everything is a property bag\" is neat from a minimalism point of view, but I think it's too primitive for large applications. It leads to lots to confusing error messages on missing properties, confusing stuff like `getOwnedProperties()`, etc. Dart treats objects as objects and has a Map type when you want a bag of data.\n\n* **Dart has a new runtime library.** We have a really nice full-featured set of collections with all of the higher-order functions you know and love, base classes that can be inherited or mixed in, etc. There's no need for something like underscore in Dart.\n\n    Likewise, Futures and Streams are baked into the language and core libraries, so you don't have to deal with a slew of incompatible ways of doing asynchrony. Futures and Streams are the canonical objects for it, and `async/await` is the syntax to work with them.\n\n**Then there's just a long list of syntax and semantic niceties.** Dart really is its own language. If you sit down and write a few thousand lines of code, you'll find a lot of fun little features that make your code neater, cleaner, and easier to maintain. I really enjoy spending my day writing Dart code.\n", "aSentId": 58575, "answer": "Dart has covariant subtyping even when it doesn't make sense. I don't think TypeScript repeated that absurd design decision.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58576, "question": "Dart has covariant subtyping even when it doesn't make sense. I don't think TypeScript repeated that absurd design decision.", "aSentId": 58577, "answer": "Funny enough Java's native arrays are covariant, which is often cited as something that holds java back fundamentally, especially in runtime performance. Dart can't claim that they weren't warned. Apparently it was just something done for simplicity in beta that didn't get fixed in time for history not to repeat itself.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58574, "question": "A bunch of stuff:\n\n* **Dart's type system is more nominal and less structural.** There are advantages to either approach, but nominal types are simpler, lead to simpler error messages, and are more familiar to users since most other static type systems are nominally typed.\n\n* **Dart is generally easier to statically analyze.** The TypeScript folks have done an impressive job, but they have an uphill battle since the language they're analyzing is fundamentally mutable and imperative. In Dart, the structure of your program\u2014the functions, classes, and methods it defines\u2014are all statically declared. You don't have to do a lot of dataflow heavy lifting to reconstitute what is effectively declarative code anyway like you do in TS/Flow.\n\n* **Dart subtracts out a lot of JS misfeatures:** implicit conversions, wacky `this` binding, function hoisting, ugly function syntax, lame `var` scope, etc. TypeScript can *add* to JS but it can't substract. Since Dart is a separate language and not a superset of JS, it can start with a clean slate.\n\n* **Dart has a meaningful distinction between methods and functions.** Most of the confusing behavior in JS around `this` is because JS doesn't actually have methods. It just has functions attached to object invoked a certain way.\n\n    Dart does have methods, which means it can make `this` behavior correctly when you use it in a closure. In JS, you have to do lots of weird workarounds (`Function.bind()`, `var that = this`, fat arrow functions, etc.). In Dart it just works.\n\n* **Dart has a meaningful distinction between objects and data structures.** JS's \"everything is a property bag\" is neat from a minimalism point of view, but I think it's too primitive for large applications. It leads to lots to confusing error messages on missing properties, confusing stuff like `getOwnedProperties()`, etc. Dart treats objects as objects and has a Map type when you want a bag of data.\n\n* **Dart has a new runtime library.** We have a really nice full-featured set of collections with all of the higher-order functions you know and love, base classes that can be inherited or mixed in, etc. There's no need for something like underscore in Dart.\n\n    Likewise, Futures and Streams are baked into the language and core libraries, so you don't have to deal with a slew of incompatible ways of doing asynchrony. Futures and Streams are the canonical objects for it, and `async/await` is the syntax to work with them.\n\n**Then there's just a long list of syntax and semantic niceties.** Dart really is its own language. If you sit down and write a few thousand lines of code, you'll find a lot of fun little features that make your code neater, cleaner, and easier to maintain. I really enjoy spending my day writing Dart code.\n", "aSentId": 58579, "answer": "Sorry guys, you lost. I have mixed feelings about this. There is the \"I told you so\" attitude but I am also a bit disappointed that you could not provide an alternative to JS that was obviously better.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58572, "question": "What does Dart offer compared to TypeScript now that they are going toe to toe in JS transpiler space?", "aSentId": 58581, "answer": "I think TypeScript is ok for web-only development but they made a couple of choices differently from Dart that IMHO could hurt them long-term:\n\n1. TypeScript is a strict super-set of JavaScript. This means:\n  - For any feature the language adds to JS it may be different from how JS will implement the same feature in the future, which will require breaking language changes. The result could be that the TypeScript team has to be conservative about evolving the language, or they have to take risks at the expense of having to introduce breaking language changes.\n  - TypeScript has to retain all existing JavaScript semantics, even the ugly ones.\n2. TypeScript does not have a native VM. Dart has a native VM that's much faster than any JS VM (in fact, it's faster than any scripting language I know of). This makes Dart more suitable as a high-performance embedded scripting engine, and for non-browser environments (server, mobile, etc).\n3. A Dart program's structure and symbols are analyzable statically, which allows tools to provide better feedback to the developer (warnings, linters) and it allows great compiler optimization features, with tree-shaking and global minification being my favorites, but also compile-time inlining. While Dart was _designed_ to support these optimizations, TypeScript would have to _break_ JS semantics to support this, which may not be practically possible at this stage.\n\nThat said, TypeScript made some reasonable choices for what they wanted to achieve. Their strategy allowed them to get great JS interop for free on day 1, and build a relatively fast compiler. The Dart team has some work to do to match TypeScript in this area (and believe me they're working hard on it). Luckily, this is not a huge problem, as pub is lush with high-quality libraries for building all kinds of web apps.\n\nIn other words, TypeScript project was a sprint, and it did improve the lives of web developers who need to stay close to JS. The Dart project is a marathon and we're just beginning to reap the advantages of the platform.\n", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58572, "question": "What does Dart offer compared to TypeScript now that they are going toe to toe in JS transpiler space?", "aSentId": 58583, "answer": "TypeScript is like an enhanced javascript. Dart is like \"if we were to implement a language for the web to replace javascript knowing everything we know now what would it look like\". Dart is its own language with a lot of modern programming features.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58585, "question": "\nFor me, I think Futures and all the async stuff Dart has that got me sold ...\n(I also think the name and logo are nicer if that counts)  ", "aSentId": 58586, "answer": "Aye, indeed it does, sadly. Check out wakelang.com, we've got a pretty sweet name and logo! But for most useful things you'll have to check in on us in a couple months :)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58587, "question": "Aye, indeed it does, sadly. Check out wakelang.com, we've got a pretty sweet name and logo! But for most useful things you'll have to check in on us in a couple months :)", "aSentId": 58588, "answer": "&gt; wakelang.com\n\nYes that does look very cool, thanks for sharing", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58562, "question": "Did you read the blog? They are discontinuing the Dart VM: \"We have decided not to integrate the Dart VM into Chrome.\", and spend the next few paragraphs claiming how much Dart is used for critical services, so the rest of Dart \"won't go away, for sure!\". They wouldn't need to do that, if there weren't valid concerns about the this.\n\nThere is no potential for Dart. If you want to use the web's native language, it will be only JavaScript now. If you want to use a language which doesn't suck, you have plenty of alternatives already, which are much better than Dart.", "aSentId": 58590, "answer": "&gt; Did you read the blog? They are discontinuing the Dart VM\n\nDid you? Because that's not what they said.\n\n\"Many of our developers use Dart for both client and server apps, reducing costs by sharing code. We remain committed to optimizing and improving the Dart VM for developer tools, servers, and mobile apps.\"", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58591, "question": "&gt; Did you read the blog? They are discontinuing the Dart VM\n\nDid you? Because that's not what they said.\n\n\"Many of our developers use Dart for both client and server apps, reducing costs by sharing code. We remain committed to optimizing and improving the Dart VM for developer tools, servers, and mobile apps.\"", "aSentId": 58592, "answer": "Which is ... kind of BS?\n\n - Mobile, except it doesn't work on most mobile platforms.\n - Servers, except it would be much cheaper and faster to just compile to class files instead of building your own VM.\n - Developer tools, ... yeah sure. Now that's the killer feature!\n\nSorry, but all of this sounds more like an expensive bored-Lars-Bak-retention-programme.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58593, "question": "Which is ... kind of BS?\n\n - Mobile, except it doesn't work on most mobile platforms.\n - Servers, except it would be much cheaper and faster to just compile to class files instead of building your own VM.\n - Developer tools, ... yeah sure. Now that's the killer feature!\n\nSorry, but all of this sounds more like an expensive bored-Lars-Bak-retention-programme.", "aSentId": 58594, "answer": "&gt; Mobile, except it doesn't work on most mobile platforms.\n\nThe VM supports ARM and MIPS. Android is covered. They are also working on an interpreter for iOS and WP (JIT isn't allowed there).\n\n&gt; Servers, except it would be much cheaper and faster to just compile to class files instead of building your own VM.\n\nSo, every language should be a JVM language and no one should ever bother writing a VM again?\n\n&gt; Developer tools, ... yeah sure. Now that's the killer feature!\n\nThe analysis server, the package manager, the formatter, the documentation generator, and the to-JS compiler (and the new to-JS compiler) are all written in Dart.\n\nOf course there are also things like debugging and profiling. Having a VM is really nice.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58595, "question": "&gt; Mobile, except it doesn't work on most mobile platforms.\n\nThe VM supports ARM and MIPS. Android is covered. They are also working on an interpreter for iOS and WP (JIT isn't allowed there).\n\n&gt; Servers, except it would be much cheaper and faster to just compile to class files instead of building your own VM.\n\nSo, every language should be a JVM language and no one should ever bother writing a VM again?\n\n&gt; Developer tools, ... yeah sure. Now that's the killer feature!\n\nThe analysis server, the package manager, the formatter, the documentation generator, and the to-JS compiler (and the new to-JS compiler) are all written in Dart.\n\nOf course there are also things like debugging and profiling. Having a VM is really nice.", "aSentId": 58596, "answer": "&gt;The analysis server, the package manager, the formatter, the documentation generator, and the to-JS compiler (and the new to-JS compiler) are all written in Dart.\n\nOkay and that's fine, but if it's not used for anything other than developing dev tools why does it need dev tools? \"We're still writing dev tools for Dart in it!\" is not a good reason for keeping around Dart.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58595, "question": "&gt; Mobile, except it doesn't work on most mobile platforms.\n\nThe VM supports ARM and MIPS. Android is covered. They are also working on an interpreter for iOS and WP (JIT isn't allowed there).\n\n&gt; Servers, except it would be much cheaper and faster to just compile to class files instead of building your own VM.\n\nSo, every language should be a JVM language and no one should ever bother writing a VM again?\n\n&gt; Developer tools, ... yeah sure. Now that's the killer feature!\n\nThe analysis server, the package manager, the formatter, the documentation generator, and the to-JS compiler (and the new to-JS compiler) are all written in Dart.\n\nOf course there are also things like debugging and profiling. Having a VM is really nice.", "aSentId": 58598, "answer": "&gt; The VM supports ARM and MIPS. Android is covered. They are also working on an interpreter for iOS and WP (JIT isn't allowed there).\n\nYes, which is exactly what I said: **doesn't work on most mobile platforms**\n\n&gt; So, every language should be a JVM language and no one should ever bother writing a VM again?\n\nIt's not even close to competitors like the JVM (or even the CLR), and without hundreds of man years thrown solely at the runime, the JIT, the GC, the monitoring, the introspection etc. it will never get there. I'm not seeing Google hiring enough people to ever close that gap.\n\nLet's be realistic, of course it is great fun to hack on a VM on company time, but Google can't even get their shit together on Android. Unless Google's approach is \"we can't build one decent runtime, and instead of fixing it, let's build another runtime which is terrible in different ways\" I don't see it changing.\n\n&gt; The analysis server, the package manager, the formatter, the documentation generator, and the to-JS compiler (and the new to-JS compiler) are all written in Dart.\n\n&gt; Of course there are also things like debugging and profiling. Having a VM is really nice.\n\nReally, explain to me, why should people bother? There are solutions out there which do everything Dart is supposed to do, just much better.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58599, "question": "&gt; The VM supports ARM and MIPS. Android is covered. They are also working on an interpreter for iOS and WP (JIT isn't allowed there).\n\nYes, which is exactly what I said: **doesn't work on most mobile platforms**\n\n&gt; So, every language should be a JVM language and no one should ever bother writing a VM again?\n\nIt's not even close to competitors like the JVM (or even the CLR), and without hundreds of man years thrown solely at the runime, the JIT, the GC, the monitoring, the introspection etc. it will never get there. I'm not seeing Google hiring enough people to ever close that gap.\n\nLet's be realistic, of course it is great fun to hack on a VM on company time, but Google can't even get their shit together on Android. Unless Google's approach is \"we can't build one decent runtime, and instead of fixing it, let's build another runtime which is terrible in different ways\" I don't see it changing.\n\n&gt; The analysis server, the package manager, the formatter, the documentation generator, and the to-JS compiler (and the new to-JS compiler) are all written in Dart.\n\n&gt; Of course there are also things like debugging and profiling. Having a VM is really nice.\n\nReally, explain to me, why should people bother? There are solutions out there which do everything Dart is supposed to do, just much better.", "aSentId": 58600, "answer": "&gt; There are solutions out there which do everything Dart is supposed to do, just much better.\n\nSuch as?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58558, "question": "I'm excited about the potential of Dart. Is there anyone using it day to day that would like to share their experiences?", "aSentId": 58602, "answer": "I'm writing a web based database query tool in Dart for work. Dart overall is my favorite programming language to use. It has all you would need from Java or C#, but has a feel that is similar to writing Python. \n\nThe library story around Dart is lacking though due to the small community. There are still benefits to this though. For example, I use the [oracledart](https://github.com/aam/oracledart) package to access my database. I ran into a few problems with using it, but I just asked the maintainer what was up and he helped me get things sorted out. Even the Dart team at Google is good about helping me out quickly.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58604, "question": "Local MirageOS development with Xen and Virtualbox", "aSentId": 58605, "answer": "informative.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58604, "question": "Local MirageOS development with Xen and Virtualbox", "aSentId": 58607, "answer": "In a nutshell, can anyone tell us what's cool about MirageOS?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58608, "question": "In a nutshell, can anyone tell us what's cool about MirageOS?", "aSentId": 58609, "answer": "More efficient and minimal application based VMs.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58604, "question": "Local MirageOS development with Xen and Virtualbox", "aSentId": 58611, "answer": "I got briefly confused at why anyone would spend that much effort on a TI-84+ OS.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58613, "question": "A significant amount of programming is done by superstition", "aSentId": 58614, "answer": "A significant amount of mostly-irrelevant boilerplate gets copy-pasted with a few bits changed to make it work for the new use case. And as the author states, \"the result works when you try it.\" I have a limited amount of time I can spend doing programming tasks. I would rather spend that time working on the hard, new, interesting problem than reading through LSB documentation cover to cover just to write a 10 line init script.\n\nObviously, in some cases this gets blown way out of proportion. You have developers that scour stackoverflow to find a snippet of code they can modify to solve the hard, new, interesting problem that is their main focus. That is definitely going to lead to a lower quality program because A the programmer doesn't fully understand what it does, and B it won't fit elegantly into the rest of the program.\n\nBut for a 10 line init script where I can copy-paste from an existing one and change the executable name? Reading the LSB specification cover to cover followed by the distro-specific documentation on init scripts just to accomplish that task is about the least efficient use of my time I can imagine. I'm going to copy and paste, test it to see if it works, and ship it. And I'm not going to feel bad about it.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58613, "question": "A significant amount of programming is done by superstition", "aSentId": 58616, "answer": "I think the entry undermines its use of the word \"superstition\" a little bit:\n\n&gt; All of this is of course magnified when you're working on secondary artifacts for your program like Makefiles, install scripts, and yes, init scripts. These aren't the important focus of your work (that's the program code itself), they're just a necessary overhead to get everything to go, something you usually bang out more or less at the end of the project and probably without spending the time to do deep research on how to do them exactly right. You grab a starting point from somewhere, cut out the bits that you know don't apply to you, modify the bits you need, test it to see if it works, and then you ship it.\n\nBasically, there are programmers who are superstitious at *everything they do*.  These are the folks who [program by coincidence](https://pragprog.com/the-pragmatic-programmer/extracts/coincidence) pretty much everywhere.  There's the folks who are actually competent at some things, but are sometimes forced to do something utterly outside those.\n\nAnd somewhere in the general vicinity, there's the folks who refuse to become competent at a tool they use all the time because the tool just requires way too much commitment for what it actually does.  (Gradle, I'm looking at you.  Grrrr.)", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58613, "question": "A significant amount of programming is done by superstition", "aSentId": 58618, "answer": "Ah... cargo cult science.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58620, "question": "Google to implement Pointer Events in Chrome", "aSentId": 58621, "answer": "Will this finally enable javascript developers to detect mouse button state without bugs? Last I checked, chrome discarded a second mouse-button-up-event when the mouse was dragged off screen with two buttons pressed and then released which made it very hard to properly implement games or painting applications without using flash. Even Internet Explorer supported an (admittedly non standard) way to correctly check for mouse button state.\n\nEDIT:\nHooray, a standardized [buttons property](http://www.w3.org/TR/pointerevents/#chorded-button-interactions)! All my problems will be solved!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58620, "question": "Google to implement Pointer Events in Chrome", "aSentId": 58623, "answer": "Nice to see them doing the right thing.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58624, "question": "Nice to see them doing the right thing.", "aSentId": 58625, "answer": "As I understand things, the problem now is getting Apple on board...", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58628, "question": "My 2015 Programming Languages Tool Belt", "aSentId": 58629, "answer": "Wow, somebody has been on Proggit and HN too long. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58628, "question": "My 2015 Programming Languages Tool Belt", "aSentId": 58631, "answer": "No Java 8? It has this cool new feature called \"lambda\".", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58628, "question": "My 2015 Programming Languages Tool Belt", "aSentId": 58633, "answer": "No C++14?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58636, "question": "Has anyone got a good resource just like this one for data structures / algorithms?", "aSentId": 58637, "answer": "A note on why this is unique, and what resources i'm looking for: \n\n* this course has no audio lectures (it's OK your resource has one)\n* provides a working environment for you in a simple download\n* automatically checks for correct code (automated unit testing)\n* very concise and minimalistic teaching style\n* **most importantly** self - paced, don't have to wait for opening / closing dates ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58639, "question": "Static Code Analysis", "aSentId": 58640, "answer": "[Frama-C](http://frama-c.com/) ?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58641, "question": "[Frama-C](http://frama-c.com/) ?", "aSentId": 58642, "answer": "An underappreciated powerhouse, if you're trying to do C in a high-reliability context.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58643, "question": "An underappreciated powerhouse, if you're trying to do C in a high-reliability context.", "aSentId": 58644, "answer": "But writing the annotations for a big program is such a pain!", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58639, "question": "Static Code Analysis", "aSentId": 58646, "answer": "One thing that's new to the mix since the article was published is clang-analyze - I'd be curious to know how this compares.\n\nI don't have any experience of it myself (just Coverity).", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58647, "question": "One thing that's new to the mix since the article was published is clang-analyze - I'd be curious to know how this compares.\n\nI don't have any experience of it myself (just Coverity).", "aSentId": 58648, "answer": "I've used both.  Coverity produces better reports, but clang is still very good.\n\nI typically run SW through Coverty, clang-analyze, and cppcheck.  They all find different things. :(", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58649, "question": "I've used both.  Coverity produces better reports, but clang is still very good.\n\nI typically run SW through Coverty, clang-analyze, and cppcheck.  They all find different things. :(", "aSentId": 58650, "answer": "&gt; They all find different things. :(\n\nWhy the disappointment? Less duplication, more bugs caught.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58651, "question": "&gt; They all find different things. :(\n\nWhy the disappointment? Less duplication, more bugs caught.", "aSentId": 58652, "answer": "It means that there are probably a lot more bugs that none of them find.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58647, "question": "One thing that's new to the mix since the article was published is clang-analyze - I'd be curious to know how this compares.\n\nI don't have any experience of it myself (just Coverity).", "aSentId": 58654, "answer": "clang-analyze is still pretty young, but shows a lot of promise.  There are a lot of checkers that still need to be written.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58647, "question": "One thing that's new to the mix since the article was published is clang-analyze - I'd be curious to know how this compares.\n\nI don't have any experience of it myself (just Coverity).", "aSentId": 58656, "answer": "When I tried it about half a year back, it was more on the level of the kind of warnings a compiler can give you, not the much more in-depth warnings a static analysis tool would give you. Most of the stuff it warned me about, clang/gcc would warn me about anyway.\n\nPerhaps it has improved in the meantime, but I really wouldn't expect it to be on the level of actual static analysis tools like coverity, klocwork, PVS et al. Getting there takes a lot of time, experimentation, tuning, statistics and testing against huge real codebases.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58639, "question": "Static Code Analysis", "aSentId": 58658, "answer": "I know Ruby has [Rubocop](https://github.com/bbatsov/rubocop), and Javascript has [JSHint](http://jshint.com/).\n\nAnything out there for Python or PHP any other common languages?\n\nAlso, there's [Code Climate](https://codeclimate.com/) which is free for open source.  I use it on [this Ruby gem](https://codeclimate.com/github/RSMP/sound) so you can see the kinds of things it complains about", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58661, "question": "The Inventor of C++, Bjarne Stroustrup Works at Morgan Stanley", "aSentId": 58662, "answer": "so?", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58661, "question": "The Inventor of C++, Bjarne Stroustrup Works at Morgan Stanley", "aSentId": 58664, "answer": "This just in: Steve Jobs is dead.", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58667, "question": "Git: Working in Detached HEAD", "aSentId": 58668, "answer": "This seemed like a bad idea as I started reading and continued to seem like a bad idea while I was reading. I have finished reading and it continues to seem like a bad idea.\n\nSurely when you do this you get a bunch of different actual branches mixed together when they're all rebased onto the main branch? What's the point of having branches if you're just going to pretend it's all one bit linear history?\n\nTo be fair, the repo I currently work on (in hg) is a giant mess of merges, but that's because we do all our work on release branches. ", "corpus": "reddit"},{"docID": "t5_2fwo", "qSentId": 58669, "question": "This seemed like a bad idea as I started reading and continued to seem like a bad idea while I was reading. I have finished reading and it continues to seem like a bad idea.\n\nSurely when you do this you get a bunch of different actual branches mixed together when they're all rebased onto the main branch? What's the point of having branches if you're just going to pretend it's all one bit linear history?\n\nTo be fair, the repo I currently work on (in hg) is a giant mess of merges, but that's because we do all our work on release branches. ", "aSentId": 58670, "answer": "There are about 15 people working on different tasks simultaneously. After patch passes code review and Jenkins it's cherry-picked on top of master. Branches are necessary to review commits and commit chains separately.\nAnd we're not pretending the history is linear - the history IS linear after patches are adopted.", "corpus": "reddit"}]