[{"docID": "t5_2r3gv","qSentId": 46307,"question": "Surpassing Human-Level Performance on ImageNet Classification","aSentId": 46308,"answer": "OK, it's time for a harder task. Looking at the \"errors\", it seems like further improvement would be optimizing for peculiarities of the test data rather than genuine advances in object recognition. It's exciting that we've come this far so fast.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46309,"question": "OK, it's time for a harder task. Looking at the \"errors\", it seems like further improvement would be optimizing for peculiarities of the test data rather than genuine advances in object recognition. It's exciting that we've come this far so fast.","aSentId": 46310,"answer": "yup, a lot of these labels make no sense. after all \"a picture is word a thousand words\".\n\nHow can you take a scene with dozens of objects and call one of them \"ground truth\"?\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46311,"question": "yup, a lot of these labels make no sense. after all \"a picture is word a thousand words\".\n\nHow can you take a scene with dozens of objects and call one of them \"ground truth\"?\n","aSentId": 46312,"answer": "That's a great question -- coming from the perspective of human language processing I find the question of object recognition to be sort of odd. It varies so much based on the task. Then you get into captions for images (so sentence generation and scene comprehension) and it's obvious that there is no right answer for a lot of things. When would a model decide on \"dog\" or \"poodle\" or \"animal\"? Presumably it should when people should, or as the task requires.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46311,"question": "yup, a lot of these labels make no sense. after all \"a picture is word a thousand words\".\n\nHow can you take a scene with dozens of objects and call one of them \"ground truth\"?\n","aSentId": 46314,"answer": "such a great point. this is the part of machine learning research I like the most. there are several cases where we say the computer is being arbitrary but so are we. seems like some part of cognition involves just making a decision to categorize something in a particular way, even if it's suboptimal because there is no right way.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46309,"question": "OK, it's time for a harder task. Looking at the \"errors\", it seems like further improvement would be optimizing for peculiarities of the test data rather than genuine advances in object recognition. It's exciting that we've come this far so fast.","aSentId": 46316,"answer": "Differentiate ship types, butterflies, churches, plants or something else with enough Pictures linked in Wikidata.\n\nShould be hard enough.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46307,"question": "Surpassing Human-Level Performance on ImageNet Classification","aSentId": 46318,"answer": "This is great. Their improvements are nonspecific to vision so we should also get improvements in speech and NLP using these same tricks. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46307,"question": "Surpassing Human-Level Performance on ImageNet Classification","aSentId": 46320,"answer": "Niiccee... \n\nI wonder though if their gains are in a very specific type of classification, or broadly across all classes. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46322,"question": "I'd like to see them compare against LReLU.","aSentId": 46323,"answer": "And 2 piece maxout.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46322,"question": "I'd like to see them compare against LReLU.","aSentId": 46325,"answer": "come again?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46328,"question": "This seems like a special case of the approach that learns the transfer functions as general piecewise-linear functions:\n\nhttp://arxiv.org/abs/1412.6830","aSentId": 46329,"answer": "I've been looking for something like this, thank you very much.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46332,"question": "Python package for the Particle Swarm Optimization Algorithm (PSO)","aSentId": 46333,"answer": "Is it parallel? ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46334,"question": "Is it parallel? ","aSentId": 46335,"answer": "I do not think so, but the source code is available on GitHub and you could integrate it with multiprocessing. If I get around to doing it myself soon I'll post a link here ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46337,"question": "Stochastic rounding in training neural networks using integer math","aSentId": 46338,"answer": "Has been done before, probably better:\nhttp://arxiv.org/abs/1412.7024","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46339,"question": "Has been done before, probably better:\nhttp://arxiv.org/abs/1412.7024","aSentId": 46340,"answer": "If I'm not mistaken, http://arxiv.org/abs/1412.7024 doesn't talk about stochastic rounding, does it?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46342,"question": "Definitions of \"Parametric\" vs. \"Non-parametric\" in ML and statistics - Let's get it straight","aSentId": 46343,"answer": "Nonparmateric has two separate meanings: (I) regression that has an infinite (or really big) number of basis functions and (II) a distribution-free method.  The problem is there is no distinguishing these two without some context. \n\n  For example, 'Gaussian Process' regression can be considered a method of nonparametric regression as the mean function is the linear combination of an infinite list of basis functions.  However, from the name, it is clear we have specified a Gaussian process measure for our procedure, clearly not nonparametric in the distribution sense of the word.\n\nSimilarly, a bootstrapped, or jackknifed, linear model would be a distribution free method of inference on the parameters.  But it does not involve a infinite combination of basis functions in regression, thus would not be considered nonparametric regression.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46344,"question": "Nonparmateric has two separate meanings: (I) regression that has an infinite (or really big) number of basis functions and (II) a distribution-free method.  The problem is there is no distinguishing these two without some context. \n\n  For example, 'Gaussian Process' regression can be considered a method of nonparametric regression as the mean function is the linear combination of an infinite list of basis functions.  However, from the name, it is clear we have specified a Gaussian process measure for our procedure, clearly not nonparametric in the distribution sense of the word.\n\nSimilarly, a bootstrapped, or jackknifed, linear model would be a distribution free method of inference on the parameters.  But it does not involve a infinite combination of basis functions in regression, thus would not be considered nonparametric regression.","aSentId": 46345,"answer": "Thanks for the comment. So, you'd agree putting Perceptrons and linear SVM into the parametric category whilst RBF SVM would be more parametric according to (I)? This is nice, exactly what I meant, because RBF SVM would be parametric according to (II). \n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46347,"question": "A Machine Learning Journey in F# with Richard Minerich on The F# Show podcast","aSentId": 46348,"answer": "/r/punchablefaces","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46350,"question": "Pattern Discovery in Elastic Search?","aSentId": 46351,"answer": "We do some stuff like this but mostly by writing custom scorers ourselves. Not heard of any packages that do it for you","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46352,"question": "We do some stuff like this but mostly by writing custom scorers ourselves. Not heard of any packages that do it for you","aSentId": 46353,"answer": "Is writing those scorers a straightforward process? As in, can someone who had never done such a thing before build a usable scorer for doing pattern discovery?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46354,"question": "Is writing those scorers a straightforward process? As in, can someone who had never done such a thing before build a usable scorer for doing pattern discovery?","aSentId": 46355,"answer": "They are easy to write but you'd have to implement some kind of pattern recognition yourself. Or using an existing java machine learning library (like weka or mahout)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46356,"question": "They are easy to write but you'd have to implement some kind of pattern recognition yourself. Or using an existing java machine learning library (like weka or mahout)","aSentId": 46357,"answer": "Yeah, that's probably the approach. from what I was able to understand whilst researching a bit on Hadoop. It seems the logial connection is to use Hadoop as the FS for ES and then use something like Mahout to do all those pattern discovery shenannigans and what not.\n\nThanks a lot for your help ;)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46359,"question": "Comparing multiple SVMs for multi-class prediction","aSentId": 46360,"answer": "No problem with that at all to compare predictions themselves if the targets were normalized or set to [-1, 1] for ball involved predictors.\n\nSlight variations on whether you want to use it with svmr or svmc.\n\nNested cross validation. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46359,"question": "Comparing multiple SVMs for multi-class prediction","aSentId": 46362,"answer": "&gt;My question is whether this assumes that identical model parameters are selected for each\n\nYes, thats generally how it is done. \n\n&gt;if you had four one-vs all models, and one was fit using a 4th order polynomial kernel, and the rest 1st order, is it a fair comparison to compare the SVM scores between these models.\n\nProbably yes, because the SVM response is normalized in maximizing the margin. Though I wouldn't be surprised if you got some weirdness if you used disparate models. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46364,"question": "FastML: Torch vs Theano","aSentId": 46365,"answer": "Eh, I've bitched about it before, but Torch was *not* easy to figure out last time I tried.  \n\nTheano is a bit tougher to dive into but things work.\n\nHopefully the documentation has dramatically improved.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46364,"question": "FastML: Torch vs Theano","aSentId": 46367,"answer": "Ummm, what about Caffe? https://github.com/BVLC/caffe \n\nIt has more forks than theano and torch combined.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46368,"question": "Ummm, what about Caffe? https://github.com/BVLC/caffe \n\nIt has more forks than theano and torch combined.","aSentId": 46369,"answer": "Caffe has a pretty different target. More mass market, for people who want to use deep learning for applications. Torch and Theano are more tailored towards people who want to use it for research on DL itself.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46364,"question": "FastML: Torch vs Theano","aSentId": 46371,"answer": "I hated torch when I first used it but I've come to realize that it's actually pretty awesome.  It's also improved a lot in the past 6 months.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46374,"question": "noob trying to use word2vec","aSentId": 46375,"answer": "I don't know how to do this (also should be learning word2vec), but this seems like a brilliant idea! \n\n\n\n\nI might steal it. ;)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46377,"question": "Model selection and training/validation/test sets vs cross-validation.","aSentId": 46378,"answer": "I don't know what his argument was but I agree with you. There should be nothing wrong with using k-fold x-validation instead of the train/validation and still hold out a test set if necessary. If anything you're basing your model selection on a lower variance estimate of the generalization error.\n\nThe downside is it takes longer.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46379,"question": "I don't know what his argument was but I agree with you. There should be nothing wrong with using k-fold x-validation instead of the train/validation and still hold out a test set if necessary. If anything you're basing your model selection on a lower variance estimate of the generalization error.\n\nThe downside is it takes longer.","aSentId": 46380,"answer": "I think his main argument was against using cross-validation only compared to using the train/validation/test sets. It make sense that CV alone would be more biased. From a practical standpoint he didn't make it clear if a single run of train/validation/test sets was sufficient. I think that he was assuming a large dataset, and therefore the sample wouldn't be an issue.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46377,"question": "Model selection and training/validation/test sets vs cross-validation.","aSentId": 46382,"answer": "\"For instance, why not perform k-fold cross-validation on the training/validation set. Then once the model is selected, use the test set to estimate the final error.\"\n\nSure, I think that this is a fine method and it's probably fairly common.  \n\nIt really depends on the amount of data available.  If there's a lot of data, then there probably isn't much harm in doing a single train/validation split, on the other hand if there's a tiny amount of data, leave-one-out cross validation is worth it.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46377,"question": "Model selection and training/validation/test sets vs cross-validation.","aSentId": 46384,"answer": "&gt; For instance, why not perform k-fold cross-validation on the training/validation set. Then once the model is selected, use the test set to estimate the final error. \n\nAs far as I am concerned this is the common practice nowadays. However, I've also seen that the final results are reported as a CV error only.\nThe best approach - in my opinion - is to do it like you suggested:\n\n1) split dataset into training/test set  \n2) model training and selection based on CV on the training set  \n3) report error on test set\n\nHowever, if you have fundamentally different models, e.g., RandomForest, RBF SVM, and nearest neighbors, another approach could be to do steps 1-3 on each of those and then select based on a comparison of the test set errors.\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46389,"question": "Neural Network input question.","aSentId": 46390,"answer": "What you probably want is one-hot vectors. So the input is a 20-dimensional vector with value 1 in the n-th position for the n-th object, and zeros everywhere else. \n\nIf the input consists of multiple objects, then you can activate multiple positions as well. But that depends on the specific problem - if there is some kind of relationship between the objects (time, structure, etc), then it's likely better to keep them separate.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46391,"question": "What you probably want is one-hot vectors. So the input is a 20-dimensional vector with value 1 in the n-th position for the n-th object, and zeros everywhere else. \n\nIf the input consists of multiple objects, then you can activate multiple positions as well. But that depends on the specific problem - if there is some kind of relationship between the objects (time, structure, etc), then it's likely better to keep them separate.","aSentId": 46392,"answer": "It's a space  relationship between about 10 objects. That might slow training down. An extra 200 length seems large","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46393,"question": "It's a space  relationship between about 10 objects. That might slow training down. An extra 200 length seems large","aSentId": 46394,"answer": "You could try activating the relevant positions in a single vector. It loses any kind of positional information, but might still work well enough, depending on the task.\n\nAlso, 200-dimensional vector is not that big really. In NLP we deal with thousands of possible words as input.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46396,"question": "Looking for a special NN for prediction","aSentId": 46397,"answer": "you mean coast, not cost.\n\nSo it depends on your coast function (I'll just show myself out).","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46396,"question": "Looking for a special NN for prediction","aSentId": 46399,"answer": "You've not got a lot of data, and you are trying to extrapolate. You are *far* better off assuming some underlying model, and performing regression. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46401,"question": "Calculus of variations for machine learning?","aSentId": 46402,"answer": "Not sure calculus of variation is widely applied in ML. But classical books on this subject are: Gelfand - calculus of variation and my favorite: Gunter - course on variational analysis (available in Russian, not sure other languages)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46401,"question": "Calculus of variations for machine learning?","aSentId": 46404,"answer": "If you know both calc of variations and machine learning theory, it shouldn't be hard.\n\nHowever, calc of variations is usually used for deriving differential equations. Not sure how you'd apply it to machine learning. Got any ideas?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46406,"question": "Any good project ideas related to applications of ML to biology?","aSentId": 46407,"answer": "There are countless ones. Do you want something specific to systems biology? If so, in which sub-field?  Networks? Interaction and graphs? Prediction/classification? \n\n/Bioinformatics - ML and protein function classification. \n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46406,"question": "Any good project ideas related to applications of ML to biology?","aSentId": 46409,"answer": "Check out past and current DREAM challenges. \n\nThese will get you close to real world data sets (ie highly dimensional, noisy genetic data). Make sure you use a hold out set if the competition doesn't provide one.  This will expose you to some of the issues that plague biology (ie never do feature selection outside of cross validation or you can get results that look great but don't work in practice). ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46412,"question": "Algorithm to recover the original word from an abbreviation?","aSentId": 46413,"answer": "just use standard spelling mistake detection/correction algorithm. even if it doesn't work it will give you some sort of baseline. It will work better if you can restrict the possible outputs. \n\nhttp://norvig.com/spell-correct.html\n\nThe above implementation is especially attractive because it is data driven.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46415,"question": "Hinton and Learning Rates in Deep Networks","aSentId": 46416,"answer": "\"What Hinton suggests is that we divide learning rate with number of batches and by the number of items in the batch.\"\n\nYou definitely shouldn't divide the learning rate by the total number of batches.  This would lead to a very small learning rate for large datasets, which doesn't make sense.  \n\nThe question of how you set the batch size and the learning rate is largely an empirical question.  A reasonable heuristic is to divide the learning rate by the batch size.  I've also seen people divide the learning rate by the square root of the batch size.  \n\nConsider a case where each batch contains duplicates of the same instance.  Then the update is the same for all batch sizes if one divides the learning rate by the batch size.  However, in reality, the instances will be distinct.  I suppose the intuition for dividing by the square root of the batch size is that some of the gradients from different instances will cancel, so we can afford to use a somewhat larger per-instance learning rate if we have a larger batch size.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46417,"question": "\"What Hinton suggests is that we divide learning rate with number of batches and by the number of items in the batch.\"\n\nYou definitely shouldn't divide the learning rate by the total number of batches.  This would lead to a very small learning rate for large datasets, which doesn't make sense.  \n\nThe question of how you set the batch size and the learning rate is largely an empirical question.  A reasonable heuristic is to divide the learning rate by the batch size.  I've also seen people divide the learning rate by the square root of the batch size.  \n\nConsider a case where each batch contains duplicates of the same instance.  Then the update is the same for all batch sizes if one divides the learning rate by the batch size.  However, in reality, the instances will be distinct.  I suppose the intuition for dividing by the square root of the batch size is that some of the gradients from different instances will cancel, so we can afford to use a somewhat larger per-instance learning rate if we have a larger batch size.  ","aSentId": 46418,"answer": "Dividing by batch size is equivalent to using mean error instead of the sum of the errors. This can be helpful if you're using varying batch sizes.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46420,"question": "Tips and Tricks for Encoding Categorical Features in Classification Tasks (IPython nb)","aSentId": 46421,"answer": "Perhaps you could give a little text explanation here of what your notebook going into detail about? IPython notebooks are quite verbose, so a summary would be nice.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46422,"question": "Perhaps you could give a little text explanation here of what your notebook going into detail about? IPython notebooks are quite verbose, so a summary would be nice.","aSentId": 46423,"answer": "Sure, thanks for the note!\n\nIn this notebook, I show some approaches in pandas (and alternatively scikit-learn) for dealing with categorical features for classification tasks.\n\nIt is about how to encode ordinal features (categorical features that imply order, e.g, M, L, XL if you think of \"sizes\").\n\nAnd also, I show how to represent nominal features (categorical features that don't imply order), e.g., colors \"orange\", \"red\", etc.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46424,"question": "Sure, thanks for the note!\n\nIn this notebook, I show some approaches in pandas (and alternatively scikit-learn) for dealing with categorical features for classification tasks.\n\nIt is about how to encode ordinal features (categorical features that imply order, e.g, M, L, XL if you think of \"sizes\").\n\nAnd also, I show how to represent nominal features (categorical features that don't imply order), e.g., colors \"orange\", \"red\", etc.","aSentId": 46425,"answer": "Great. I think that helps the post a lot.\n\nContent looks good. I've recently been thinking about how to deal with very high cardinality categorical features. Any ideas on this front? Maybe we could discuss privately if you're interested and we could put together a couple of ideas.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46426,"question": "Great. I think that helps the post a lot.\n\nContent looks good. I've recently been thinking about how to deal with very high cardinality categorical features. Any ideas on this front? Maybe we could discuss privately if you're interested and we could put together a couple of ideas.","aSentId": 46427,"answer": "I only know this scenario from text classification, where I had up to ~50,000-100,000 features (bag of words model). Typically, you'd store those as sparse matrices (see the `sparse` parameter that I intentionally turned off (`False`) in this notebook for representation purposes). Except for the RandomForest classifier (requires a dense matrix) you can use those when you are working with scikit-learn. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46429,"question": "Machine learning in online dating","aSentId": 46430,"answer": "I'd been thinking about this problem recently. Tinder in particular doesn't seem to be doing much with their data and appears to just use a very simple score for how attractive someone is based on how often they're liked. I'd like to see someone use a Netflix-style algorithm (low-rank SVD) to infer latent variables of users from the graph of likes, messages sent, time spent looking at profiles, etc. Instead of elaborate feature extraction, just let who's attracted to whom tell you what traits and tastes a user has.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46431,"question": "I'd been thinking about this problem recently. Tinder in particular doesn't seem to be doing much with their data and appears to just use a very simple score for how attractive someone is based on how often they're liked. I'd like to see someone use a Netflix-style algorithm (low-rank SVD) to infer latent variables of users from the graph of likes, messages sent, time spent looking at profiles, etc. Instead of elaborate feature extraction, just let who's attracted to whom tell you what traits and tastes a user has.","aSentId": 46432,"answer": "eHarmony has been using machine learning on their problem longer than Tinder has existed.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46433,"question": "eHarmony has been using machine learning on their problem longer than Tinder has existed.","aSentId": 46434,"answer": "Tinder is owned by the same company that owns Match and OkCupid. They should be doing more ML than they are.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46435,"question": "Tinder is owned by the same company that owns Match and OkCupid. They should be doing more ML than they are.","aSentId": 46436,"answer": "I'm going to be an intern at OkCupid this summer. I'm hoping to get a project incorporating machine learning into some aspect of the site (ideally matching), but I don't know what the available projects are yet.\n\nThey've also had explained to me that most users who have successful matches and delete their accounts bring in more new users than those who keep using it to no end, so it is beneficial for them to have successful matches.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46437,"question": "I'm going to be an intern at OkCupid this summer. I'm hoping to get a project incorporating machine learning into some aspect of the site (ideally matching), but I don't know what the available projects are yet.\n\nThey've also had explained to me that most users who have successful matches and delete their accounts bring in more new users than those who keep using it to no end, so it is beneficial for them to have successful matches.","aSentId": 46438,"answer": "That sounds like a great internship. Maybe you can ask them about the SVD approach to the graph. Maybe it doesn't work, maybe they're doing it, maybe they haven't considered it or gotten around to it. It seems to me the latent information will complement anything that comes from explicit features.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46435,"question": "Tinder is owned by the same company that owns Match and OkCupid. They should be doing more ML than they are.","aSentId": 46440,"answer": "But should they? I haven't looked into Match/OkCupid in terms of how they generate revenue, I only know of Tinders recent methodology. With that being said, the more successful the matching is, the less amount of users that will use the product. The lower the users the lower amount of revenue. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46441,"question": "But should they? I haven't looked into Match/OkCupid in terms of how they generate revenue, I only know of Tinders recent methodology. With that being said, the more successful the matching is, the less amount of users that will use the product. The lower the users the lower amount of revenue. ","aSentId": 46442,"answer": "You mean because people who find a good enough match get serious and delete their accounts? In my experience people mostly close or abandon their accounts because they get tired of dealing with bad matches.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46446,"question": "What do you do, and how did you get there?","aSentId": 46447,"answer": "I'll start. \n\nI work for a large internet company, soon to move to a smaller internet company. I work on people data. \n\nHow I got here: I did a bunch of shared tasks at SIGIR and WWW and that helped me get to gradschool. In my MS, I took a lot of ML courses and did an ML thesis. Did NLP for a finance company, and now I do much less ML on much larger systems. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46446,"question": "What do you do, and how did you get there?","aSentId": 46449,"answer": "My area of research is Machine Learning applied to Quantitative Finance problems a.k.a. Computational Finance. For a living, I am a Quantitative Analyst. I mostly work with actuaries, mathematicians, and other computer scientists to build models of investment and retirement products for risk management, and trading purposes. The job is really awesome, and I find that there are many overlaps between Machine Learning and Quantitative Finance [1]. I am also currently studying a Masters degree in Computer Science and am part of a really good AI research group.\n\nHow I got here? I studied an undergraduate degree in Computer Science then moved into a management consulting position at one of the big 4 (to get exposure to finance). While I was working there I pursued my Honours degree part-time. During my honours I focused on Artificial Intelligence and Statistics and started a blog about how it can be used to solve real problems in finance. Through that blog I got the attention of a partner who hired me as a Quant. I recently moved from the big 4 to an international Asset Management and Insurance company.\n\nIn response to your question - 'what it takes to get a machine learning job', I would say that you need to put yourself out there and understand that Machine Learning is NOT the point. Most companies use Machine Learning to add value and generate a competitive advantage. If you can understand how to do that using Machine Learning people will start paying attention and you will be holding the cards. So my advice - pick a field and don't hold back.\n\n[1] For example Adjoint Automatic Differentiation which is used for fast derivatives pricing, is very similar to the Back-propagation algorithm used to train Neural Networks. Actually they are basically the same thing :)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46450,"question": "My area of research is Machine Learning applied to Quantitative Finance problems a.k.a. Computational Finance. For a living, I am a Quantitative Analyst. I mostly work with actuaries, mathematicians, and other computer scientists to build models of investment and retirement products for risk management, and trading purposes. The job is really awesome, and I find that there are many overlaps between Machine Learning and Quantitative Finance [1]. I am also currently studying a Masters degree in Computer Science and am part of a really good AI research group.\n\nHow I got here? I studied an undergraduate degree in Computer Science then moved into a management consulting position at one of the big 4 (to get exposure to finance). While I was working there I pursued my Honours degree part-time. During my honours I focused on Artificial Intelligence and Statistics and started a blog about how it can be used to solve real problems in finance. Through that blog I got the attention of a partner who hired me as a Quant. I recently moved from the big 4 to an international Asset Management and Insurance company.\n\nIn response to your question - 'what it takes to get a machine learning job', I would say that you need to put yourself out there and understand that Machine Learning is NOT the point. Most companies use Machine Learning to add value and generate a competitive advantage. If you can understand how to do that using Machine Learning people will start paying attention and you will be holding the cards. So my advice - pick a field and don't hold back.\n\n[1] For example Adjoint Automatic Differentiation which is used for fast derivatives pricing, is very similar to the Back-propagation algorithm used to train Neural Networks. Actually they are basically the same thing :)","aSentId": 46451,"answer": "What's the distribution of work like there? Do you have a focus on anything (e.g. alpha models, risk models, derivatives pricing)? What do most researchers work on?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46452,"question": "What's the distribution of work like there? Do you have a focus on anything (e.g. alpha models, risk models, derivatives pricing)? What do most researchers work on?","aSentId": 46453,"answer": "The portfolio of work is very large. In the company I work for there are front-office quants who usually deal with quantitative trading strategies and there are middle-office quants who deal more with risk management and hedging (which is often where derivatives pricing comes into play). I would say most researchers are in the middle office; however the biggest applications of machine learning (at this point in time) appear to be the front office. Many firms are looking at ways in which machine learning methods can be used to construct trading strategies.\n\nTo be more specific about my work, we are a team of middle-office quants responsible for building models which calculate the values of exotic embedded derivatives in investment and retirement products sold by our company e.g. guaranteed funds and smoothed funds. Because the derivatives are not your vanilla exchange traded derivatives we usually build the models from scratch and use discounted cash flows and Monte Carlo methods to arrive at a fair value of the liability (the risk side of the derivative). We then come up with hedging strategies to mitigate those risks. \n\nDoing this kind of work has helped me immensely when it comes to my Masters research in Machine Learning because it deals a lot with stochastic models, derivatives (the math kind), statistics, tonnes of data, and simulation techniques. For my Masters research I am trying to show that traditional optimization algorithms cannot solve asset allocation and portfolio optimization problems in certain scenarios which have to do with stochasticity, constraints, and risk factors.\n\nPersonally, I am more interested in the front-office quant work so having a good grounding is common quant topics and machine learning is a good approach. I myself and a Computer Scientist so I have a lot to learn :)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46455,"question": "Genetics as a Social Network - A Data Scientist's Perspective","aSentId": 46456,"answer": "I was under the impression that biostatistics was an established field. Is it only a recent phenomena?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46457,"question": "I was under the impression that biostatistics was an established field. Is it only a recent phenomena?","aSentId": 46458,"answer": "It is an established field. The author is a sophomore at MIT so he might be liable to overestimate his expertise a bit.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46460,"question": "If you could only use one programming language to do your job, what would it be and why?","aSentId": 46461,"answer": "I'm a data scientist whose job mainly consists of prototyping ML approaches on relatively big (250G - 2TB) datasets. \n\nPython is my goto (given that I'm allowed to have a Hadoop/Hive/Spark data backend). It has great library support and when I want to go fast on big data I deploy code on clusters of computers. Everything is about scale. Debugging is pretty great, and we code locally while testing remotely using PyCharm. \n\nIt also works great across multiple operating systems and is easy to teach to new team members. We started as an R shop, but the switch to Python has allowed us to keep our necessary built in stats packages while giving us a language that is capable of doing some more sophisticated things. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46462,"question": "I'm a data scientist whose job mainly consists of prototyping ML approaches on relatively big (250G - 2TB) datasets. \n\nPython is my goto (given that I'm allowed to have a Hadoop/Hive/Spark data backend). It has great library support and when I want to go fast on big data I deploy code on clusters of computers. Everything is about scale. Debugging is pretty great, and we code locally while testing remotely using PyCharm. \n\nIt also works great across multiple operating systems and is easy to teach to new team members. We started as an R shop, but the switch to Python has allowed us to keep our necessary built in stats packages while giving us a language that is capable of doing some more sophisticated things. ","aSentId": 46463,"answer": "Not to hijack the thread, but could you tell me just a little bit more regarding your workflow, when it comes to training models on datasets that are in the 100's of GB's? As in, do you load the entire dataset into Python, and then run algorithms, or do you sample?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46464,"question": "Not to hijack the thread, but could you tell me just a little bit more regarding your workflow, when it comes to training models on datasets that are in the 100's of GB's? As in, do you load the entire dataset into Python, and then run algorithms, or do you sample?","aSentId": 46465,"answer": "Training is always the hardest part. I try a few different solutions:\n\n\nBig Memory: I'll grab an AWS machine with ~250 gigs of RAM. For smaller datasets the problem is done. For larger, not as much. If speed isn't too much of an issue, SSD swap can help a bit but isn't ideal. \n\n\nSampling: sometimes sampling is enough. I'll plot training size vs test performance and see if we can find a point that levels off. Often, for things I do, 100k observations provide almost as much test set performance as 1mil... Sometimes though it doesn't :)\n\n\nDimensionality reduction: PCA or SVD in the database then I bring data in. Or I sample the data, run a random forest and use feature importance to trim the dataset down and retrain on the smaller set. \n\n\nWarm starts: Train with as much data as you can, grab new data and modify the models you already trained on. \n\n\nMahout/Spark/Streaming: I haven't really had to do this yet, but if for some reason the above wouldn't work, I guess I'd start looking into implementing or using models that can work entirely in distributed form. \n\n\nOverall, I always focus on test set performance and do lots of cross validation and holdout based training no matter what techniques I'm trying. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46466,"question": "Training is always the hardest part. I try a few different solutions:\n\n\nBig Memory: I'll grab an AWS machine with ~250 gigs of RAM. For smaller datasets the problem is done. For larger, not as much. If speed isn't too much of an issue, SSD swap can help a bit but isn't ideal. \n\n\nSampling: sometimes sampling is enough. I'll plot training size vs test performance and see if we can find a point that levels off. Often, for things I do, 100k observations provide almost as much test set performance as 1mil... Sometimes though it doesn't :)\n\n\nDimensionality reduction: PCA or SVD in the database then I bring data in. Or I sample the data, run a random forest and use feature importance to trim the dataset down and retrain on the smaller set. \n\n\nWarm starts: Train with as much data as you can, grab new data and modify the models you already trained on. \n\n\nMahout/Spark/Streaming: I haven't really had to do this yet, but if for some reason the above wouldn't work, I guess I'd start looking into implementing or using models that can work entirely in distributed form. \n\n\nOverall, I always focus on test set performance and do lots of cross validation and holdout based training no matter what techniques I'm trying. ","aSentId": 46467,"answer": "&gt; PCA or SVD in the database then I bring data in.\n\nWhat database supports this?  I am intrigued!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46468,"question": "&gt; PCA or SVD in the database then I bring data in.\n\nWhat database supports this?  I am intrigued!","aSentId": 46469,"answer": "I guess by \"database\" I'm referring to spark on S3.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46470,"question": "I guess by \"database\" I'm referring to spark on S3.","aSentId": 46471,"answer": "hahahaha awesome!  How easy is it to set up? Do you have to have your data on S3 in a specific format?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46472,"question": "hahahaha awesome!  How easy is it to set up? Do you have to have your data on S3 in a specific format?","aSentId": 46473,"answer": "Spark has some decent ec2 scripts to make the process easier. I just use tons of simple delimited text files on S3. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46464,"question": "Not to hijack the thread, but could you tell me just a little bit more regarding your workflow, when it comes to training models on datasets that are in the 100's of GB's? As in, do you load the entire dataset into Python, and then run algorithms, or do you sample?","aSentId": 46475,"answer": "Chiming with /u/mansmidi, yes Python can be used to train data more than 1TB. You could Hadoop framework though. Individual node clusters in Hadoop can access data from S3 and process using Python. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46476,"question": "Chiming with /u/mansmidi, yes Python can be used to train data more than 1TB. You could Hadoop framework though. Individual node clusters in Hadoop can access data from S3 and process using Python. ","aSentId": 46477,"answer": "Definitely this is a great approach as well. For algorithms that can be trained in parallel this is ideal.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46462,"question": "I'm a data scientist whose job mainly consists of prototyping ML approaches on relatively big (250G - 2TB) datasets. \n\nPython is my goto (given that I'm allowed to have a Hadoop/Hive/Spark data backend). It has great library support and when I want to go fast on big data I deploy code on clusters of computers. Everything is about scale. Debugging is pretty great, and we code locally while testing remotely using PyCharm. \n\nIt also works great across multiple operating systems and is easy to teach to new team members. We started as an R shop, but the switch to Python has allowed us to keep our necessary built in stats packages while giving us a language that is capable of doing some more sophisticated things. ","aSentId": 46479,"answer": "How do you code locally while testing remotely using PyCharm?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46480,"question": "How do you code locally while testing remotely using PyCharm?","aSentId": 46481,"answer": "The paid version of PyCharm allows remote execution/debugging. It's super easy to setup: you just enter your host name and ssh credentials to a remote Python setup and PyCharm takes care of the rest. It syncs your project in the background.\n\nMost things I develop on my laptop but run on AWS.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46482,"question": "The paid version of PyCharm allows remote execution/debugging. It's super easy to setup: you just enter your host name and ssh credentials to a remote Python setup and PyCharm takes care of the rest. It syncs your project in the background.\n\nMost things I develop on my laptop but run on AWS.","aSentId": 46483,"answer": "Does the autocomplete function as if developing on a local machine?\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46484,"question": "Does the autocomplete function as if developing on a local machine?\n","aSentId": 46485,"answer": "Yes. The files exist both remote and local and are synced so autocomplete, etc work like it's local. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46486,"question": "Yes. The files exist both remote and local and are synced so autocomplete, etc work like it's local. ","aSentId": 46487,"answer": "Everything about pycharm is beautiful. Thanks for sharing this","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46460,"question": "If you could only use one programming language to do your job, what would it be and why?","aSentId": 46489,"answer": "I use Clojure: it gives me the expressibility of Lisp with the ability to use the entire Java ecosystem when needed. Add in [Incanter](http://incanter.org/) and you have a great environment for both experimentation and production.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46490,"question": "I use Clojure: it gives me the expressibility of Lisp with the ability to use the entire Java ecosystem when needed. Add in [Incanter](http://incanter.org/) and you have a great environment for both experimentation and production.","aSentId": 46491,"answer": "How is Incanter, and how are you impacted by the entire project being supported by one guy?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46492,"question": "How is Incanter, and how are you impacted by the entire project being supported by one guy?","aSentId": 46493,"answer": "Incanter is nice, though I haven't used it in anger yet: I make heavy use of R and its packages (kernlab) for experimentation, and have not had to productize the code yet. When I do, it will be on the JVM.\n\nThe fact that fact that it is supported by one guy doesn't bother me too much. There are multiple contributors to the library, and we have the knowledge in house to maintain it (and push changes upstream) if necessary.\n\nI'm sorry I can't give you a better answer.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46494,"question": "Incanter is nice, though I haven't used it in anger yet: I make heavy use of R and its packages (kernlab) for experimentation, and have not had to productize the code yet. When I do, it will be on the JVM.\n\nThe fact that fact that it is supported by one guy doesn't bother me too much. There are multiple contributors to the library, and we have the knowledge in house to maintain it (and push changes upstream) if necessary.\n\nI'm sorry I can't give you a better answer.","aSentId": 46495,"answer": "No problem!  Thanks for the answer.  I really like functional languages in general and often finding myself writing code in Java using a ton of recursion and writing functions I haven't defined yet (like in SICP).","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46496,"question": "No problem!  Thanks for the answer.  I really like functional languages in general and often finding myself writing code in Java using a ton of recursion and writing functions I haven't defined yet (like in SICP).","aSentId": 46497,"answer": "If you haven't tried Clojure yet, I highly recommend it. At work I do most of my development (including GlassFish hosted services) in Clojure, but my coworkers work in Java. As well as being able to use Incanter, Clojure also supports [logic programming](https://github.com/clojure/core.logic/wiki/A-Core.logic-Primer) out of the box. You can have a single program that mixes object-oriented programming, functional programming, and logic programming. Having access to multiple paradigms under a single Lispy umbrella.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46498,"question": "If you haven't tried Clojure yet, I highly recommend it. At work I do most of my development (including GlassFish hosted services) in Clojure, but my coworkers work in Java. As well as being able to use Incanter, Clojure also supports [logic programming](https://github.com/clojure/core.logic/wiki/A-Core.logic-Primer) out of the box. You can have a single program that mixes object-oriented programming, functional programming, and logic programming. Having access to multiple paradigms under a single Lispy umbrella.","aSentId": 46499,"answer": "Damn.  Why Clojure over other Lisps?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46500,"question": "Damn.  Why Clojure over other Lisps?","aSentId": 46501,"answer": "Two words: Java Interoperation. \n\nABCL runs on then JVM too, but the interop isn't as nice. Clojure's compiler generates good code as well. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46460,"question": "If you could only use one programming language to do your job, what would it be and why?","aSentId": 46503,"answer": "**R**\n\nAs a statistician (that concentrated on machine learning and is working as a data scientist), I might be biased.  But others have their own biases.  I repeatedly have to inform people that R is actually a full-fledged programming language, not just a statistical computing environment, and that yes, I do actually know how to program.  It seems the default question is almost always:\n\n&gt; Can you scrape data from the web?  How?\n\n&gt; I don't need that functionality frequently, but I did it for my thesis.  There's a package for that.  It's pretty easy.\n\n&gt; But R is so slow!\n\n&gt; Not if you know how to use vectorization.  My experience is that if you want something fast you're going to have to use C/C++/Fortran anyway.\n\n&gt; Huh.\n\nI've just started to dig into Python's data science tools and am finding the combination of Numpy, Scipy, Scikit-learn, and matplotlib to feel very adhoc.  I started looking around for how Python handles missing values and the discussion is basically, \"How do we do what R does?\"  It's probably an exaggeration, but I feel like many of my Python colleagues spend a good chunk of their time rewriting R code.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46504,"question": "**R**\n\nAs a statistician (that concentrated on machine learning and is working as a data scientist), I might be biased.  But others have their own biases.  I repeatedly have to inform people that R is actually a full-fledged programming language, not just a statistical computing environment, and that yes, I do actually know how to program.  It seems the default question is almost always:\n\n&gt; Can you scrape data from the web?  How?\n\n&gt; I don't need that functionality frequently, but I did it for my thesis.  There's a package for that.  It's pretty easy.\n\n&gt; But R is so slow!\n\n&gt; Not if you know how to use vectorization.  My experience is that if you want something fast you're going to have to use C/C++/Fortran anyway.\n\n&gt; Huh.\n\nI've just started to dig into Python's data science tools and am finding the combination of Numpy, Scipy, Scikit-learn, and matplotlib to feel very adhoc.  I started looking around for how Python handles missing values and the discussion is basically, \"How do we do what R does?\"  It's probably an exaggeration, but I feel like many of my Python colleagues spend a good chunk of their time rewriting R code.","aSentId": 46505,"answer": "&gt;rewriting R code\n\nI was asked to chip in early into some of these stats packages in Python. It really felt like it was really just reinventing the wheel instead of adding something of value. I thought my time would be better spent learning more about R and its ecosystem. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46504,"question": "**R**\n\nAs a statistician (that concentrated on machine learning and is working as a data scientist), I might be biased.  But others have their own biases.  I repeatedly have to inform people that R is actually a full-fledged programming language, not just a statistical computing environment, and that yes, I do actually know how to program.  It seems the default question is almost always:\n\n&gt; Can you scrape data from the web?  How?\n\n&gt; I don't need that functionality frequently, but I did it for my thesis.  There's a package for that.  It's pretty easy.\n\n&gt; But R is so slow!\n\n&gt; Not if you know how to use vectorization.  My experience is that if you want something fast you're going to have to use C/C++/Fortran anyway.\n\n&gt; Huh.\n\nI've just started to dig into Python's data science tools and am finding the combination of Numpy, Scipy, Scikit-learn, and matplotlib to feel very adhoc.  I started looking around for how Python handles missing values and the discussion is basically, \"How do we do what R does?\"  It's probably an exaggeration, but I feel like many of my Python colleagues spend a good chunk of their time rewriting R code.","aSentId": 46507,"answer": "Pandas makes python bearable, but I agree with this sentiment. Handling missing values before pandas was horrific.\n\nFirst off, I'll state up front that I code in python as much as R and like it very much. But I don't know that python has been very innovative in this sphere, but has brought together parts of Matlab and R into one language and integrated them into greater parts. This is what I see as its value added proposition.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46509,"question": "If I had to choose one language, Go with out a doubt.\n\nI use a lot of python for library support and because of legacy code at my current job but we end up falling back to a compiled language (cython, c, go) fairly often for speed and so could never commit to python 100%, especially for novel method development.\n\nGo offers similar productivity/philosophy to python (it is designed as a compiled, statically typed language that feels like a scripting language) but performance is much better, especially when it comes to multicore stuff as you can actually do things like use mutexes or write atomic data structures to maximize performance or use channels for communication which is great for quick parallelism. It makes pythons multiprocessing based approach look like a bad joke.\n\nBeing able to write for loops that actually perform well is also a giant boost in productivity, even if you end up replacing them with BLAS calls later.\n\nThe tooling is also great with automatic formatting, a great profiler, race detector and test suite and easy autocomplete in any editor via daemons included with the language (i use the go sublime package for sublime text).\n\nThe standard library is actually much nicer (and oddly better organized and more 'pythonic' in many ways) then pythons having benefited by being written recently from the ground up by some of the best coders at Google. Ie the http stuff is cleanly organized in one package and easy to use.\n\nThe ml/math specific library support isn't very far along yet so you end up building lots of stuff form the ground up or calling out to existing libraries via cgo. \n\nI'm optimistic that the numerical community will grow quickly, based on the other merits of the language. [I wrote my own random forest library which is gaining some traction for example.](https://github.com/ryanbressler/CloudForest) \n\nSome things like lack of operator overloading and generics may hold it back for a while as it makes it harder to implement a \"write the equation like it looks in the paper\" level numerical library. You end up making calls to functions instead but this can be much higher performance as it allows you to be clearer about memory allocations (ie add this vector into this other vector instead of allocating a new vector for the result).","aSentId": 46510,"answer": "I just got a book on Go a week ago mainly to see how its concurrency works. Is the speed much faster than Python?\n\nAlso, how do you make up for the nunber of libraries in Python? ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46511,"question": "I just got a book on Go a week ago mainly to see how its concurrency works. Is the speed much faster than Python?\n\nAlso, how do you make up for the nunber of libraries in Python? ","aSentId": 46512,"answer": "Go is compiled and statically typed and the speed (with good practices that avoid the garbage collector etc) can be closer to C then that of python. (Ie my RF implementation is faster on some data sets then implementations in C/Cython and Fortran). The only way python can be in the same ballpark is in situations where all the work is done by vectorization or other calls to compiled C/Fortran code like BLAS.\n\nYou either write your own libraries or call out to c libraries. It isn't a great language if you just want to try a bunch of algorithms easily (python/R/matlab are great for that) but if you're implementing novel/production stuff that can't really be done in a scripting language i find go much more productive then C etc.\n\nI think eventually I'll move towards writing libraries in go that can be called from python/R/matlab but the go compiler doesn't currentely support running in situations where it doesn't controll the main function so you have to jump through hoops.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46513,"question": "Go is compiled and statically typed and the speed (with good practices that avoid the garbage collector etc) can be closer to C then that of python. (Ie my RF implementation is faster on some data sets then implementations in C/Cython and Fortran). The only way python can be in the same ballpark is in situations where all the work is done by vectorization or other calls to compiled C/Fortran code like BLAS.\n\nYou either write your own libraries or call out to c libraries. It isn't a great language if you just want to try a bunch of algorithms easily (python/R/matlab are great for that) but if you're implementing novel/production stuff that can't really be done in a scripting language i find go much more productive then C etc.\n\nI think eventually I'll move towards writing libraries in go that can be called from python/R/matlab but the go compiler doesn't currentely support running in situations where it doesn't controll the main function so you have to jump through hoops.","aSentId": 46514,"answer": "Very interesting. The RF seems a perfect place to try and use Go's concurrency features. It will be interesting to see where Go catches on since it is compiled but the syntax is still very simple and Python like. It is also interesting to see this comment since on Hacker News there were comments about Go not really being good for ML.\n\nI do wonder how many companies now are considering it in their production environment. It does have some distinct advantages, but change is hard.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46515,"question": "Very interesting. The RF seems a perfect place to try and use Go's concurrency features. It will be interesting to see where Go catches on since it is compiled but the syntax is still very simple and Python like. It is also interesting to see this comment since on Hacker News there were comments about Go not really being good for ML.\n\nI do wonder how many companies now are considering it in their production environment. It does have some distinct advantages, but change is hard.","aSentId": 46516,"answer": "I ended up not using channels (go's headline concurrency feature) much in the RF implementation as mutexes allow finer grained control the fact that slices/arrays are read safe in go allows you to hold only one copy of the data in memory and access it from multiple go routines. Sciki learn now does this too (via cython) but at the time i started cloudforest it copied the data for each thread.\n\nI did use a channel for a while to pass trees to a go routine that was writing them to disc as they were grown without needing to store them in memory for more efficiency but that is a mutex now as well.\n\nIt would be interesting to do a multi threaded gradient boosting implementation (which can't be parallelized on a tree level) that used channels to parallelize split searching within individual trees.\n\nI think go's niche currently is when you need a performant production web service that does some lightish analysis (there are people using cloudforest in production with a forest kept in memory). \n\nIt doesn't have great support for deep neural networks (which are really hip right now but just get offloaded to cuda anyways) and the [matrix math and optimization stuff](https://github.com/gonum) is still pretty young.\n\n\n\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46513,"question": "Go is compiled and statically typed and the speed (with good practices that avoid the garbage collector etc) can be closer to C then that of python. (Ie my RF implementation is faster on some data sets then implementations in C/Cython and Fortran). The only way python can be in the same ballpark is in situations where all the work is done by vectorization or other calls to compiled C/Fortran code like BLAS.\n\nYou either write your own libraries or call out to c libraries. It isn't a great language if you just want to try a bunch of algorithms easily (python/R/matlab are great for that) but if you're implementing novel/production stuff that can't really be done in a scripting language i find go much more productive then C etc.\n\nI think eventually I'll move towards writing libraries in go that can be called from python/R/matlab but the go compiler doesn't currentely support running in situations where it doesn't controll the main function so you have to jump through hoops.","aSentId": 46518,"answer": "The fact that Go assumes it is the center of the world is why I think it is not going to ever be more than a glue language like Python et al. (Twisted did that for concurrency in Python and it drove me and most other people up the wall and searching for alternatives.) This is not a bad thing, but it tends to make people rewriting libraries in Go appear to be on a fool's errand IMHO.  Rust has a stronger story here and I think it will end up sucking a lot of the library writers and toolsmiths away from Go in the long run.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46519,"question": "The fact that Go assumes it is the center of the world is why I think it is not going to ever be more than a glue language like Python et al. (Twisted did that for concurrency in Python and it drove me and most other people up the wall and searching for alternatives.) This is not a bad thing, but it tends to make people rewriting libraries in Go appear to be on a fool's errand IMHO.  Rust has a stronger story here and I think it will end up sucking a lot of the library writers and toolsmiths away from Go in the long run.","aSentId": 46520,"answer": "They are working on shared object support for go which would address this issue. But they are also talking about moving to a compacting garbage collector in a way that could make handing arrays to blas or other external code more complicated (since the gc might decided to move them unless you specify them as external).\n\nI like Rust's story as you put it but the simplicity and productivity of go is what keeps me using it. I'm hopeful the numerical community grows enough to avoid it becoming a webserver specific language which is where things like the gc revision are pushing it.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46521,"question": "They are working on shared object support for go which would address this issue. But they are also talking about moving to a compacting garbage collector in a way that could make handing arrays to blas or other external code more complicated (since the gc might decided to move them unless you specify them as external).\n\nI like Rust's story as you put it but the simplicity and productivity of go is what keeps me using it. I'm hopeful the numerical community grows enough to avoid it becoming a webserver specific language which is where things like the gc revision are pushing it.","aSentId": 46522,"answer": "I would love to see this happen (have a lot of things I would prefer to do in go over C) but I am not going to hold my breath waiting.  I agree that some sort of way for others to call in to go is necessary, but if they decide to move to a compacting gc I will probably write off the language and go back to Python/Cython/C for these tasks until Rust is a bit more mature.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46523,"question": "I would love to see this happen (have a lot of things I would prefer to do in go over C) but I am not going to hold my breath waiting.  I agree that some sort of way for others to call in to go is necessary, but if they decide to move to a compacting gc I will probably write off the language and go back to Python/Cython/C for these tasks until Rust is a bit more mature.","aSentId": 46524,"answer": "I need to check out Rust in more depth at some point.\n\nI feel like what is needed is an ahead-of-time-compiled, statically typed, slightly low level (ie an array of floats is just that) language with the ease of use of go and great numerical support.\n\nJulia is an interesting project too but, to me, suffers from trying to be a better matlab/dynamically typed scripting language where as I want an easier/more productive C/fortran. Ie threading in Julia is still primarily based on the multiprocessing model and other things are a bit off.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46527,"question": "I'm still a graduate student, but I use things that make sense for the task. I like Python a lot but if I need to do something \"right\" it gets done in Java. As far as statistical packages go, though, MATLAB is fantastically fast and I enjoy using it for everything other than text processing (which is unfortunately my focus).","aSentId": 46528,"answer": "Matlab is actually slow, the fast part of Matlab is LAPACK which is an open source linear algebra library integrated with Matlab. What's fast about Matlab is the coding/development. It's a tradeoff between x hours of coding and y hours of executing code, so if y is small you want to make x as short as possible, and Matlab is perfect.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46531,"question": "If I had to pick only one, it would be **C#** in a heartbeat: \n\n1. Easy networking libraries for making http requests\n2. Easy drawing libraries for whipping up a quick bitmap visualization on the fly\n3. Possibly the best ever multi-threading support I've ever seen (thread pooling, actual threads, PLINQ, etc)\n4. I love strong typing and virtually no classes, generics and functions are just so damn powerful on their own that wrapping them feels dirty.\n5. Visual Studio's debugger is miles ahead of any other language's debugger, and once you discover that, it can transform how you code entirely!\n6. Say what you will, but C# is actually pretty damn fast - and coupled with how easy it is to distribute code to many machines, I'd say it takes the cake over C any day for development speed.\n7. I don't use packages for my job (adaptive systems dev), so SciKit and SciPy, and every other machine learning package is basically useless to me - making the python bandwagon a lot easier to avoid - but that being said NuGet (C# package manager) isn't too bad.\n8. Built in profiling and lots of third party profilers mean I can optimize with basically no effort.\n9. Built in class diagram generator that works amazingly well.\n\nSo yep, definitely C# here.","aSentId": 46532,"answer": "So we use c# a lot for glue applications. Message passing, moving data, infrastructure etc. So from the software engineering side, yea its pretty nice... if you're only in windows. (though yes the VS debugger is awesome).\n\nBut for actual ML work and building models... no way. Also .NET is a pain in the ass with handling images. If you have to say... take a raw byte array and build a bitmap so you can convert to a 32 or 64 bit image with some lossless compression scheme. Yea .net can get lost. I don't know what the specific cause is but there's probably a very good reason openCV doesnt exist for .NET yet. If they figure out how then maybe I could be happy with c# for more purposes but as is it's a huge headache in image processing.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46533,"question": "So we use c# a lot for glue applications. Message passing, moving data, infrastructure etc. So from the software engineering side, yea its pretty nice... if you're only in windows. (though yes the VS debugger is awesome).\n\nBut for actual ML work and building models... no way. Also .NET is a pain in the ass with handling images. If you have to say... take a raw byte array and build a bitmap so you can convert to a 32 or 64 bit image with some lossless compression scheme. Yea .net can get lost. I don't know what the specific cause is but there's probably a very good reason openCV doesnt exist for .NET yet. If they figure out how then maybe I could be happy with c# for more purposes but as is it's a huge headache in image processing.","aSentId": 46534,"answer": "EmguCV is the best .net wrapper for opencv. It uses the native open cv library, so the performance is on par with Java and python. \n\nImaging in general is a snap in .net. The high level framework classes are very good, and simplify most day to day imaging tasks. If you want to flip bits just load your image as a bitmap and call the LockBits method to get a pointer to the raw bitmap in memory.\n\nIf you are interested in an image processing library, AForge.net and Accord.net cover just about everything *except* for deep convolutional networks. Both libraries are a pleasure to use. They are very well factored and because they are written around a set of all defined interfaces, they are very easy to extend. \n\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46535,"question": "EmguCV is the best .net wrapper for opencv. It uses the native open cv library, so the performance is on par with Java and python. \n\nImaging in general is a snap in .net. The high level framework classes are very good, and simplify most day to day imaging tasks. If you want to flip bits just load your image as a bitmap and call the LockBits method to get a pointer to the raw bitmap in memory.\n\nIf you are interested in an image processing library, AForge.net and Accord.net cover just about everything *except* for deep convolutional networks. Both libraries are a pleasure to use. They are very well factored and because they are written around a set of all defined interfaces, they are very easy to extend. \n\n","aSentId": 46536,"answer": "So do you have a preference of the 3? If not ill just try out emgu as most of the team is used to openCV from c++.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46537,"question": "So do you have a preference of the 3? If not ill just try out emgu as most of the team is used to openCV from c++.","aSentId": 46538,"answer": "I would go for whatever the team is most comfortable with. Learning to use new libraries can be a distraction for the team when you have a great idea that you want to get it into production as fast as possible.\n\nUsing familiar tools also helps the team gel and really sets you up for success. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46533,"question": "So we use c# a lot for glue applications. Message passing, moving data, infrastructure etc. So from the software engineering side, yea its pretty nice... if you're only in windows. (though yes the VS debugger is awesome).\n\nBut for actual ML work and building models... no way. Also .NET is a pain in the ass with handling images. If you have to say... take a raw byte array and build a bitmap so you can convert to a 32 or 64 bit image with some lossless compression scheme. Yea .net can get lost. I don't know what the specific cause is but there's probably a very good reason openCV doesnt exist for .NET yet. If they figure out how then maybe I could be happy with c# for more purposes but as is it's a huge headache in image processing.","aSentId": 46540,"answer": "C# runs on Linux, too. Currently through Mono, but Microsoft has recently announced there will be their own official distribution of .NET runtime for Linux.\n\nAnother nice property .NET has is transparent interoperability between languages it supports. So you could use C#, F# or even C++ for the stuff that needs to be fast, and then glue it together with IronPython or the like without messing with SWIG. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46531,"question": "If I had to pick only one, it would be **C#** in a heartbeat: \n\n1. Easy networking libraries for making http requests\n2. Easy drawing libraries for whipping up a quick bitmap visualization on the fly\n3. Possibly the best ever multi-threading support I've ever seen (thread pooling, actual threads, PLINQ, etc)\n4. I love strong typing and virtually no classes, generics and functions are just so damn powerful on their own that wrapping them feels dirty.\n5. Visual Studio's debugger is miles ahead of any other language's debugger, and once you discover that, it can transform how you code entirely!\n6. Say what you will, but C# is actually pretty damn fast - and coupled with how easy it is to distribute code to many machines, I'd say it takes the cake over C any day for development speed.\n7. I don't use packages for my job (adaptive systems dev), so SciKit and SciPy, and every other machine learning package is basically useless to me - making the python bandwagon a lot easier to avoid - but that being said NuGet (C# package manager) isn't too bad.\n8. Built in profiling and lots of third party profilers mean I can optimize with basically no effort.\n9. Built in class diagram generator that works amazingly well.\n\nSo yep, definitely C# here.","aSentId": 46542,"answer": "Check out Accord.net for machine learning. It has a decent SVM implementation and a pretty good framework to construct deep belief networks. \n\nThe kernel SVM class in accord is an order of magnitude slower than libsvm (called through pinvoke), but is fine for the kind of thing I do.\n\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46544,"question": "Python 2.x. Between the tons of libraries available for it and the ease and speed of prototyping new methods or solutions, it's pretty much no contest for me. Even if it is a good bit slower in some cases, the time I save in writing the code always seems to make up for it. \n\nShould note that my job involves a lot of one-off type work, serious production models that will see repeated use will often get reimplemented in C/C++ if the python is too slow. ","aSentId": 46545,"answer": "As someone using R who is just teaching himself Python, do you recommend that I skip learning about 3.x and stick with 2.x instead? The book I am using to learn uses 3.x. I believe (and I could be totally wrong) that they are almost the same, so I am not too worried about syntax, but I want to know which one I should start creating projects in. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 46546,"question": "As someone using R who is just teaching himself Python, do you recommend that I skip learning about 3.x and stick with 2.x instead? The book I am using to learn uses 3.x. I believe (and I could be totally wrong) that they are almost the same, so I am not too worried about syntax, but I want to know which one I should start creating projects in. ","aSentId": 46547,"answer": "My own opinion (for whatever it's worth) would be 2.x; there's not enough that python 3 adds to the language to make up for the numerous libraries that either haven't been ported or have 3.x support that lags 2.x.  A lot seem to develop in 2, then port to 3, so the python 3 version is often behind and buggier.  \n\nMy opinion; there may be very good reasons that I should be jumping to python 3, and I just don't know what they are.","corpus": "reddit"}]