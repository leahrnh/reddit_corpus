[{"docID": "t5_2qhmr", "qSentId": 19712, "question": "I'm learning C++ as a first language. Where can I find a set of practice problems targeted towards beginners?", "aSentId": 19713, "answer": "Don't get too discouraged by people telling you that C++ isn't for beginners. I learned it as a newbie and I actually found it more comfortable than when I attempted to learn python.\n\nYes, C++ has some complex features, but you don't need to reach for them and use them if you don't want to. You don't have to write templates as a beginner. You don't need to know how templated classes and overload resolution works to use std::vector. You don't have to do manual memory management, in fact it's discouraged. Use the STL containers and smart pointers.\n\nI'm not sure why people want to recommend dynamically typed languages to beginners. Python was far more difficult to get out of the tiny toy 50 lines of code project phase for me.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19714, "question": "Don't get too discouraged by people telling you that C++ isn't for beginners. I learned it as a newbie and I actually found it more comfortable than when I attempted to learn python.\n\nYes, C++ has some complex features, but you don't need to reach for them and use them if you don't want to. You don't have to write templates as a beginner. You don't need to know how templated classes and overload resolution works to use std::vector. You don't have to do manual memory management, in fact it's discouraged. Use the STL containers and smart pointers.\n\nI'm not sure why people want to recommend dynamically typed languages to beginners. Python was far more difficult to get out of the tiny toy 50 lines of code project phase for me.", "aSentId": 19715, "answer": "I agree with what you are saying about python and languages alike, and not using all the functionality of c++, but exactly for this reason start with c. The switch from c to c++ is easy, and is similar to a switch from \"c++ w/o c++ functionality\" to c++", "corpus": "reddit"}{"docID": "t5_2qhmr", "qSentId": 19712, "question": "I'm learning C++ as a first language. Where can I find a set of practice problems targeted towards beginners?", "aSentId": 19717, "answer": "/r/dailyprogrammer", "corpus": "reddit"}{"docID": "t5_2qhmr", "qSentId": 19712, "question": "I'm learning C++ as a first language. Where can I find a set of practice problems targeted towards beginners?", "aSentId": 19719, "answer": "Don't let people scare you away from C++ as a first language.  It's pretty typical as a first language in CS programs because you can directly learn basic algorithms, object oriented programming, pointers, and the compile/link process.  It's certainly a harder language, but it's definitely not insurmountable.  ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19720, "question": "Don't let people scare you away from C++ as a first language.  It's pretty typical as a first language in CS programs because you can directly learn basic algorithms, object oriented programming, pointers, and the compile/link process.  It's certainly a harder language, but it's definitely not insurmountable.  ", "aSentId": 19721, "answer": "ya I agree it was the fist language I learned when I was 17. Its the reason I am studying electrical engineering. If I can learn it you defiantly can. I just bought a book and read it but I have always been into reading textbooks. I also found you tube to be amazing for help. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19722, "question": "ya I agree it was the fist language I learned when I was 17. Its the reason I am studying electrical engineering. If I can learn it you defiantly can. I just bought a book and read it but I have always been into reading textbooks. I also found you tube to be amazing for help. ", "aSentId": 19723, "answer": "I am envious of the resources people have today compared to when I was a kid even just 10 years ago. I tried a few times to learn programming from books, and I was so discouraged it was well into college before I tried it again. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19725, "question": "You are... Very brave. ", "aSentId": 19726, "answer": "Not brave, smart!  Well, ok, maybe a little brave. \n\nC++ was also my first language and I attribute a lot of my early success in college to already knowing fundamental C++ constructs.  ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19725, "question": "You are... Very brave. ", "aSentId": 19728, "answer": "ha, my university teaches it the day you get in the door and has 3 subjects dedicated to it. It has been a long and painful process.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19730, "question": "Make something you're interested in. Look into RAII, smart pointers, gdb. Are you on Windows or Linux? [this](https://github.com/Dekken/maiken) might come in handy, shameless plug, I made it.", "aSentId": 19731, "answer": " Could you explain what it is a little bit, please?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19732, "question": " Could you explain what it is a little bit, please?", "aSentId": 19733, "answer": "Sure, it's a convenience app for compiling/linking C++ executables.\n\nSupports multiple platforms/multiple languages/multiple compilers.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19734, "question": "Sure, it's a convenience app for compiling/linking C++ executables.\n\nSupports multiple platforms/multiple languages/multiple compilers.", "aSentId": 19735, "answer": "Cool! Is it alright if I use your code in my own project? I see your code is under the gnu license. That means I'm allowed to modify it and use it right? ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19730, "question": "Make something you're interested in. Look into RAII, smart pointers, gdb. Are you on Windows or Linux? [this](https://github.com/Dekken/maiken) might come in handy, shameless plug, I made it.", "aSentId": 19737, "answer": "Why does your readme link to another readme oO", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19746, "question": "Learn C first, at least to the point where you can mentally translate it into pseudocode assembly.  Learn about memory layout, memory management, pointer arithmetic.\n\nThen, switch to C++, and incorporate its non-C features into your \"vocabulary\" one by one. That way you'll understand how they work and why they were introduced in the first place. Otherwise it will probably be overwhelming and demotivating.\n\nIt's not the easiest route, but it definitely is rewarding.", "aSentId": 19747, "answer": "I strongly agree with this, but it's obviously not a popular opinion here, because everyone is indoctrinated by huge companies like microsoft that c++ is TEH B3ST. I'm not saying c++ is very bad but it's pretty bad, it is however useful in some cases but definitely not as a beginner.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19749, "question": "Most of what useful stuff you'll write in C++ is basically C, so I'll leave you with implementing your own singly- and doubly-linked list data structures and figure out how to sort them. Also, learn how implement a tree data structure. Write code to traverse it depth first order and also in breadth first order. If you don't run into segfaults often or have memory leaks (it's also worth learning what those are), then it means you have a good handle on most of the stuff that trip people up. That is pointers and memory management. Everything else is based around making that stuff easier to do or think about.", "aSentId": 19750, "answer": "&gt; Most of what useful stuff you'll write in C++ is basically C [...]\n\nThat was maybe true 20 years ago. Nowadays C++ and C are so different that writing basically C in C++ will set you up for frustration and failure. The same is true the other way around of course, too. Pick C, or pick C++, but don't blend them together into some kind of monster.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19751, "question": "&gt; Most of what useful stuff you'll write in C++ is basically C [...]\n\nThat was maybe true 20 years ago. Nowadays C++ and C are so different that writing basically C in C++ will set you up for frustration and failure. The same is true the other way around of course, too. Pick C, or pick C++, but don't blend them together into some kind of monster.", "aSentId": 19752, "answer": "By useful stuff, I meant more of the inane regular stuff that does actual work in the implementation. I'm not trying to advocate mixing malloc() in with the \"new\" operator or using C-strings interchangeably with std::string. That'd be horrible! I'm talking about the pure language, like how the control structures are largely the same; the whole notion of pointers and how they are usually used inside structs; the notion of constness; etc. The advantage to C++ is that C forces you to be more explicit when writing implementation code even when you don't care to be.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19756, "question": "I recruit for IT. Learn a language that will get you paid, like C#, Java, or Ruby. ", "aSentId": 19757, "answer": "I work in high tech. Learning C++ well is a huge asset for anyone interested in embedded development, robotics, gaming, simulation, graphics, scientific computing... And the pay is pretty nice, too.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19758, "question": "I work in high tech. Learning C++ well is a huge asset for anyone interested in embedded development, robotics, gaming, simulation, graphics, scientific computing... And the pay is pretty nice, too.", "aSentId": 19759, "answer": "In all honesty though it probably is on its way out as an industry standard. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19760, "question": "In all honesty though it probably is on its way out as an industry standard. ", "aSentId": 19761, "answer": "It's been on it's way out as an industry standard for decades now, yet it's still an industry standard...", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19760, "question": "In all honesty though it probably is on its way out as an industry standard. ", "aSentId": 19763, "answer": "Honestly, the only language I see eventually (i.e. in ten years maybe) taking C++ down, is rust. Every language that has been toted as a C++-killer has been missing features that make C++ so great for its domain.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19766, "question": "Don't do this if you're. beginner. C++ has a very low ratio of time spent to skill acquired. The more time you waste learning to get to a particular skill level, you're pushing that next job or shipped project out in time. \n\nAlso, one of the marks of a skilled programmer is to only know enough C++ to get the job done, for any given problem. Any more than that and you're wasting mental bandwidth. Even the creator of C++ probably wouldn't recommend this. c++ is not hard because it is sophisticated and learning it will give you massive skills afterwards. It's hard because it's poorly designed and convoluted and certain features are best avoided unless you really know what you're doing. And that can only come with experience doing OOP. What you want to do instead is maximize the rate at which you acquire OOP experience, and C++ will not help you with that. Much time will be wasted on debugging issues that have no bearing on that\n\nI am telling you right now, that learning C++ is a waste of time unless you really need it. Taking heed of this advice is a lot faster than you spending months coming to the same conclusion on your own.", "aSentId": 19767, "answer": "This is asinine, half-baked dribble. You say that there is very little reward for learning C++, yet you claim it is not hard (the reason being its sophistication). Then you revert back to saying that it is indeed hard because certain features requires actual thought. What are you doing giving advice to novices? I am by no means an expert, but you are clearly talking absolute fucking shit.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19769, "question": "If the halting problem says no program can determine if a program terminates, how can I know while(true){skip} doesn't terminate?", "aSentId": 19770, "answer": "The halting problem says no program can determine if an *arbitrary* program terminates. There are, of course, programs where it's easy to determine whether they halt.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19771, "question": "The halting problem says no program can determine if an *arbitrary* program terminates. There are, of course, programs where it's easy to determine whether they halt.", "aSentId": 19772, "answer": "&gt; There are, of course, programs where it's easy to determine whether they halt.\n\nIn particular, it's possible to have conservative termination checkers: Ones that, reliably, can tell you whether a program terminates, not terminates, or, and this is the trick, saying \"I don't know, am afraid to try, I don't want to loop myself\" otherwise. The better the checker, the smaller the third category.\n\nThe about easiest of those schemes is to check whether recursive calls are structurally decreasing, that is, whether numbers all get smaller on all paths, lists shorter, etc. Already covers a surprisingly large number of cases, and can be done in O(AST size).\n\nDependently typed languages generally have such a thing because you generally want to have assurance that computations on the type level will terminate. Non-termination is a rather annoying compile failure. If you want to prove something to terminate, translating it into a structurally decreasing thing is also a very valid proof strategy.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19771, "question": "The halting problem says no program can determine if an *arbitrary* program terminates. There are, of course, programs where it's easy to determine whether they halt.", "aSentId": 19774, "answer": "Similarly, sorting an arbitrary list is O(n log n), despite the fact that I can sort \"1, 2, 3, 4, 5\" in linear time by observing that it's already sorted. Whenever we make a statement about the difficulty of a general problem, that doesn't usually stop us from finding an easier instance.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19771, "question": "The halting problem says no program can determine if an *arbitrary* program terminates. There are, of course, programs where it's easy to determine whether they halt.", "aSentId": 19776, "answer": "Everything in theoretical CS is about the general solutions. As it is in any science. Many seem to forget that. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19769, "question": "If the halting problem says no program can determine if a program terminates, how can I know while(true){skip} doesn't terminate?", "aSentId": 19778, "answer": "Of course I can write a program to determine if *a* program terminates. The halting problem says I can't write a program to determine if *any* program terminates.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19769, "question": "If the halting problem says no program can determine if a program terminates, how can I know while(true){skip} doesn't terminate?", "aSentId": 19780, "answer": "You can write a 'halts' program that always gives correct answers if you allow \"I_Don't_Know\" as an answer:\n\n    halts(\"while(true){skip}\") = False;\n    halts(\"while(false){skip}\") = True;\n    halts(anything_else) = I_Don't_Know;\n\nit's just that the algorithm will not be able to know in all cases.\n\nYou can make a better 'halts' algorithm than mine with just 3 cases, but you will never write a complete one.\n\nI hope that helps?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19781, "question": "You can write a 'halts' program that always gives correct answers if you allow \"I_Don't_Know\" as an answer:\n\n    halts(\"while(true){skip}\") = False;\n    halts(\"while(false){skip}\") = True;\n    halts(anything_else) = I_Don't_Know;\n\nit's just that the algorithm will not be able to know in all cases.\n\nYou can make a better 'halts' algorithm than mine with just 3 cases, but you will never write a complete one.\n\nI hope that helps?", "aSentId": 19782, "answer": "Why does the second one return false?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19783, "question": "Why does the second one return false?", "aSentId": 19784, "answer": "My mistake! I had them the wrong way around, I've fixed it now. Thank you!", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19785, "question": "My mistake! I had them the wrong way around, I've fixed it now. Thank you!", "aSentId": 19786, "answer": "Ha! I didn't even notice the first one was wrong. In fact, of initially misread the headline as saying it halts, and justified it by thinking 'skip' was OP's pseudo code for skipping the rest of a loop.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19788, "question": "Don't worry, this is a common misconception. The first week I was hired out of school, a senior programmer made this same mistake in lunchtime conversation.\n\nTo illustrate with an analogy: the \"you're not infinitely strong\" problem states that there isn't a person so strong that they can lift everything. You've asked basically \"yeah, but I can lift this paper clip. How can I lift this paper clip if I can't lift everything?\" In this analogy, \"while(true){skip}\" is a metaphorical paper clip. Lifting it doesn't prove that you can lift everything.\n\nBack to the matter at hand, the halting problem claims, categorically, that there *exists* programs that will confound analysis, that no program you write will be able to analyze them. ", "aSentId": 19789, "answer": "And for some objects, like a vending machine, you never know for sure whether you actually are unable to lift it ever, or whether you aren't buff enough yet. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19790, "question": "And for some objects, like a vending machine, you never know for sure whether you actually are unable to lift it ever, or whether you aren't buff enough yet. ", "aSentId": 19791, "answer": "LOL, so true! \n\nBob: \"I bet you $100 you can't lift that vending machine\"\n\nAlice: \"You're on! In fact, I'll even give you $20 back if you decide I win\"\n\n[ Alice and Bob both hand $100 to Carol ]\n\n[ Alice groans, trying to lift ]\n\nBob: \"See, you can't.\"\n\nAlice: \"Stop interrupting. You never said how long I had to lift it. Wooo halting problem!\" \n\n[ Alice strains, trying to lift again ]\n\nAlice: \"But you know, if you concede, you'll have $20 more than if you don't\" ;-)\n\nBob: [ glares ] \"Fine. I concede. Just give me my $20\"", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19788, "question": "Don't worry, this is a common misconception. The first week I was hired out of school, a senior programmer made this same mistake in lunchtime conversation.\n\nTo illustrate with an analogy: the \"you're not infinitely strong\" problem states that there isn't a person so strong that they can lift everything. You've asked basically \"yeah, but I can lift this paper clip. How can I lift this paper clip if I can't lift everything?\" In this analogy, \"while(true){skip}\" is a metaphorical paper clip. Lifting it doesn't prove that you can lift everything.\n\nBack to the matter at hand, the halting problem claims, categorically, that there *exists* programs that will confound analysis, that no program you write will be able to analyze them. ", "aSentId": 19793, "answer": "Sorry, but the undecidability of the halting problem does not mean that there exists programs for which it is impossible to decide whether they terminate or not. That is not what the problem says. It states that there is no single program that can decide whether any arbitrary program halts on a given input.\n\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19794, "question": "Sorry, but the undecidability of the halting problem does not mean that there exists programs for which it is impossible to decide whether they terminate or not. That is not what the problem says. It states that there is no single program that can decide whether any arbitrary program halts on a given input.\n\n", "aSentId": 19795, "answer": "First, what I meant to say was \"means that for every analysis program A, there exists some subject program S such that A(S) either computes the wrong answer or A(S) itself does not halt\".  \n\nSecond, I'm pretty sure that's equivalent to your restating as \"It states that there is no single program that can decide whether any arbitrary program halts on a given input\"", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19796, "question": "First, what I meant to say was \"means that for every analysis program A, there exists some subject program S such that A(S) either computes the wrong answer or A(S) itself does not halt\".  \n\nSecond, I'm pretty sure that's equivalent to your restating as \"It states that there is no single program that can decide whether any arbitrary program halts on a given input\"", "aSentId": 19797, "answer": "&gt;\"means that for every analysis program A, there exists some subject program S such that A(S) either computes the wrong answer or A(S) itself does not halt\". \n\nYes, this formulation is correct, but it is not what your wrote in your first post (see below).\n\nThis only means that A doesn't compute the correct answer for S. There might be a program B which can correctly determine whether S halts or not (but B then has its own program S' for which it cannot do this). The undecidabiltity of the halting problem does not rule out this possibility. It only says that every analysis program has at least one program for which it cannot determine whether it halts or not. This doesn't mean that there is no other analysis program that could correctly analyse it.\n\n&gt; Second, I'm pretty sure that's equivalent to your restating as \"It states that there is no single program that can decide whether any arbitrary program halts on a given input\"\n\nNo, sorry, it is not. You wrote \"no program you write will be able to analyze them\" in your first post. So what you meant was \"there exists a program S such that there exists no program A that can determine whether S halts on a given input\".\n\nThis is not what the halting problem is talking about. The undecidability of the halting problem means that \"there exists no program A that can decide for every program S if S halts on a given input\".", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19798, "question": "&gt;\"means that for every analysis program A, there exists some subject program S such that A(S) either computes the wrong answer or A(S) itself does not halt\". \n\nYes, this formulation is correct, but it is not what your wrote in your first post (see below).\n\nThis only means that A doesn't compute the correct answer for S. There might be a program B which can correctly determine whether S halts or not (but B then has its own program S' for which it cannot do this). The undecidabiltity of the halting problem does not rule out this possibility. It only says that every analysis program has at least one program for which it cannot determine whether it halts or not. This doesn't mean that there is no other analysis program that could correctly analyse it.\n\n&gt; Second, I'm pretty sure that's equivalent to your restating as \"It states that there is no single program that can decide whether any arbitrary program halts on a given input\"\n\nNo, sorry, it is not. You wrote \"no program you write will be able to analyze them\" in your first post. So what you meant was \"there exists a program S such that there exists no program A that can determine whether S halts on a given input\".\n\nThis is not what the halting problem is talking about. The undecidability of the halting problem means that \"there exists no program A that can decide for every program S if S halts on a given input\".", "aSentId": 19799, "answer": "You seem to imply that for every program S\\_i that A can't analyze, that there exists some program B\\_i that *can* analyze it, or at least we can't rule out the possibility that such a program B\\_i does not exist. Am I understand you correctly?\n\nAnd this set of programs B\\_i would also have to be countably infinite, right?\n\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19800, "question": "You seem to imply that for every program S\\_i that A can't analyze, that there exists some program B\\_i that *can* analyze it, or at least we can't rule out the possibility that such a program B\\_i does not exist. Am I understand you correctly?\n\nAnd this set of programs B\\_i would also have to be countably infinite, right?\n\n", "aSentId": 19801, "answer": "More or less. What I'm saying is that it doesn't follow directly from the undecidability of the halting problem that there are problems for which there is no program that can determine whether it halts or not.\n\nFrom what I know, we cannot rule out that such B_i exist, yes. At least the halting problem doesn't say anything about program like this. I actually do not know any result that rules out or shows the existence of such programs. I also know no program P for which it was shown that there exists no specific program that can decide whether P terminates or not. On the contrary, given a *specific* program P it is usually possible to write a *specific* and tailor-made analysis program that decides whether P halts or not. The example in the title is a trivial example.\n\nBut I can assure you that the basic halting problem doesn't make any statement about the existence of such programs.\n\nI don't understand your question about the size of the set. For one program S_i and one program A that cannot analyse S_i, it woul be sufficient to show that there exists one single program B that can decide S_i (so the set would only consist of this one program) to show that the termination of S_i is not generally undecidable.\n\n//edit: Made some formulations clearer.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19802, "question": "More or less. What I'm saying is that it doesn't follow directly from the undecidability of the halting problem that there are problems for which there is no program that can determine whether it halts or not.\n\nFrom what I know, we cannot rule out that such B_i exist, yes. At least the halting problem doesn't say anything about program like this. I actually do not know any result that rules out or shows the existence of such programs. I also know no program P for which it was shown that there exists no specific program that can decide whether P terminates or not. On the contrary, given a *specific* program P it is usually possible to write a *specific* and tailor-made analysis program that decides whether P halts or not. The example in the title is a trivial example.\n\nBut I can assure you that the basic halting problem doesn't make any statement about the existence of such programs.\n\nI don't understand your question about the size of the set. For one program S_i and one program A that cannot analyse S_i, it woul be sufficient to show that there exists one single program B that can decide S_i (so the set would only consist of this one program) to show that the termination of S_i is not generally undecidable.\n\n//edit: Made some formulations clearer.", "aSentId": 19803, "answer": "&gt;I don't understand your question about the size of the set. For one program S_i and one program A that cannot analyse S_i, it woul be sufficient to show that there exists one single program B that can decide S_i (so the set would only consist of this one program) to show that the termination of S_i is not generally undecidable.\n\nWell, if the set was finite, then there would exist a program U that simulated execution of A and all B\\_i, reporting the result of the first analysis that halts. Then this program U would solve the halting problem. Since that's impossible, I think we have to conclude that this set B\\_i must not be finite. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19806, "question": "There is a lot of work on termination analysis. The halting problem says you can't write a program that will determine if *any* program will terminate. However, programs that only use a restricted set of operations can easily be said to terminate or not. If your program depends on user or other external input, for example, there's no way to possibly know if it will terminate. There are several things that can be done to ensure a program terminates, though.", "aSentId": 19807, "answer": "&gt;  If your program depends on user or other external input, for example, there's no way to possibly know if it will terminate.\n\nOh yes there is: `more` accepts keyboard input, but given that it can't scroll back and files can't be infinitely long, it's going to terminate. As long as it gets enough input, that is, but you can solve even that by a time out.\n\nThat's usually not what you, though, you want the program to possibly not terminate (like `less`) but be *productive*, that is, do a limited amount of work for every single input datum. Something something codata.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19808, "question": "&gt;  If your program depends on user or other external input, for example, there's no way to possibly know if it will terminate.\n\nOh yes there is: `more` accepts keyboard input, but given that it can't scroll back and files can't be infinitely long, it's going to terminate. As long as it gets enough input, that is, but you can solve even that by a time out.\n\nThat's usually not what you, though, you want the program to possibly not terminate (like `less`) but be *productive*, that is, do a limited amount of work for every single input datum. Something something codata.", "aSentId": 19809, "answer": "Well... `more` can read from a pipe as well as a file, so the input can be as long as the pipe hasn't send EOF, which means that it can continue until that pipe is done, whether or not you are sending data.\n\n     $ yes | more", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19808, "question": "&gt;  If your program depends on user or other external input, for example, there's no way to possibly know if it will terminate.\n\nOh yes there is: `more` accepts keyboard input, but given that it can't scroll back and files can't be infinitely long, it's going to terminate. As long as it gets enough input, that is, but you can solve even that by a time out.\n\nThat's usually not what you, though, you want the program to possibly not terminate (like `less`) but be *productive*, that is, do a limited amount of work for every single input datum. Something something codata.", "aSentId": 19811, "answer": "&gt; but given that it can't scroll back and files can't be infinitely long\n\nEven ignoring actual infinitely long files like /dev/fd/0 or /dev/random: It can avoid scrolling forward. If more runs without me scrolling down, then it doesn't terminate.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19812, "question": "&gt; but given that it can't scroll back and files can't be infinitely long\n\nEven ignoring actual infinitely long files like /dev/fd/0 or /dev/random: It can avoid scrolling forward. If more runs without me scrolling down, then it doesn't terminate.", "aSentId": 19813, "answer": "That's what \"as long as it gets enough input\" handles but yes, the example is bad because of infinite files.\n\nSomething like a program which checks the checksum of a single ISBN would work. It has a set maximum length, if you pass the ISBN via command line you still have \"external input\", and if it's read from stdin you have the same condition: \"as long as it gets enough input\". That is, effectively, is ever *told* to quit, and you can prove that if/when such a command comes, it indeed will.\n\nWhich is why I mentioned that what we usually care about is productivity, even if other approaches are possible.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19806, "question": "There is a lot of work on termination analysis. The halting problem says you can't write a program that will determine if *any* program will terminate. However, programs that only use a restricted set of operations can easily be said to terminate or not. If your program depends on user or other external input, for example, there's no way to possibly know if it will terminate. There are several things that can be done to ensure a program terminates, though.", "aSentId": 19815, "answer": "Dependence on user input isn't really part of the formal description of the halting problem, which originally describes Turing machines. Turing machines don't really have the notion of a stream of input over time, but you can obviously simulate that by seeding it with timestamped inputs.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19817, "question": "Maybe someone here can explain this one to me.  It's always seemed to me that this was solvable given infinite time and memory.\n\nRun the test program but for each instruction snapshot and save the state of memory and registers.\n\nIf the same state is achieved again it's looping and will never stop.\n\nYou're not done checking until the program has achieved every permutation of bits in memory or has looped back around it some state it had before.\n\nWhat am I missing?\n\nBasically - a computer has a finite (but huge) set of states.  So this seems solvable.", "aSentId": 19818, "answer": "The halting problem for finite state machines is decidable, so yes you can solve it for a physical computer. Programming languages are typically defined in terms of an idealised machine, though.\n\nIn other words, you can take a program and use this trick to see if it works on *a particular* computer, but you don't necessarily know if your result applies to a bigger machine.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19819, "question": "The halting problem for finite state machines is decidable, so yes you can solve it for a physical computer. Programming languages are typically defined in terms of an idealised machine, though.\n\nIn other words, you can take a program and use this trick to see if it works on *a particular* computer, but you don't necessarily know if your result applies to a bigger machine.", "aSentId": 19820, "answer": "The original formulation of the halting problems refers to Turing machines, and Turing machines by definition have unbounded memory (tape).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19817, "question": "Maybe someone here can explain this one to me.  It's always seemed to me that this was solvable given infinite time and memory.\n\nRun the test program but for each instruction snapshot and save the state of memory and registers.\n\nIf the same state is achieved again it's looping and will never stop.\n\nYou're not done checking until the program has achieved every permutation of bits in memory or has looped back around it some state it had before.\n\nWhat am I missing?\n\nBasically - a computer has a finite (but huge) set of states.  So this seems solvable.", "aSentId": 19822, "answer": "CS is only about computers in so much as the model allows. Theoretical computational devices like the Turning machine ignore those limits and ask with the most powerful computational device, if an algorithm X is possible to solve all problems Y across all inputs. And other similar general problems.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19817, "question": "Maybe someone here can explain this one to me.  It's always seemed to me that this was solvable given infinite time and memory.\n\nRun the test program but for each instruction snapshot and save the state of memory and registers.\n\nIf the same state is achieved again it's looping and will never stop.\n\nYou're not done checking until the program has achieved every permutation of bits in memory or has looped back around it some state it had before.\n\nWhat am I missing?\n\nBasically - a computer has a finite (but huge) set of states.  So this seems solvable.", "aSentId": 19824, "answer": "As other have pointed out we are talking about a theoretical computer, when referring to the halting problem.  However, you could also imagine that instead of your programing running on a single machine, which will fail within a couple of decades, you could imagine it running in virtual machine one something like Amazon cloud which is constantly being upgraded with more memory and more disk space.  In this case the computer will always have finite memory but it is unbounded in time, so the plan of enumerating all states doesn't work in this case.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19827, "question": "While this answer is true, it's completely useless, the set of states is just too big. ;)\n\nYou need another angle of attack to solve the issue, and I will redirect you to the abundant literature on termination checking for that.", "aSentId": 19828, "answer": "I understand its impractically slow, but that's VERY different than \"not solvable at all\" which is what the theory around the halting problem states.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19829, "question": "I understand its impractically slow, but that's VERY different than \"not solvable at all\" which is what the theory around the halting problem states.", "aSentId": 19830, "answer": "Actually it's not always impractical. Some model checkers like [Spin](http://spinroot.com) do it to detect livelocks in (models of) parallel programs. It'll find infinite loops in sequential code too, but programmers can usually catch those without any help.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19832, "question": "I think this neatly sums up the difference between compsci as math and compsci as engineering.", "aSentId": 19833, "answer": "Um, what do you mean?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19834, "question": "Um, what do you mean?", "aSentId": 19835, "answer": "I think he means the maths says something can't be done, while the engineering says well we can make it work in the ways we need it to.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19836, "question": "I think he means the maths says something can't be done, while the engineering says well we can make it work in the ways we need it to.", "aSentId": 19837, "answer": "It can't be done, period. If you relax the requirement that the program needs to determine halting of any arbitrary program so that only needs to examine specific programs, then the math says it can be done as well. It's also a different problem entirely then.\n\nIt's not like math says something can't be done but we make it work in real life anyways. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19838, "question": "It can't be done, period. If you relax the requirement that the program needs to determine halting of any arbitrary program so that only needs to examine specific programs, then the math says it can be done as well. It's also a different problem entirely then.\n\nIt's not like math says something can't be done but we make it work in real life anyways. ", "aSentId": 19839, "answer": "Yeah, I think he means the engineering just looks for the good enough answer.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19840, "question": "Yeah, I think he means the engineering just looks for the good enough answer.", "aSentId": 19841, "answer": "One of the best termination checkers is in Coq, and I doubt OP would call the authors \"engineers\", they're pretty hardcore maths people. Monads are child's play to what those people do.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19842, "question": "One of the best termination checkers is in Coq, and I doubt OP would call the authors \"engineers\", they're pretty hardcore maths people. Monads are child's play to what those people do.", "aSentId": 19843, "answer": "Coq/Gallina is not Turing-complete though, on purpose, precisely because they want to be able to do termination analysis. You can't write programs that don't provably terminate.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19840, "question": "Yeah, I think he means the engineering just looks for the good enough answer.", "aSentId": 19845, "answer": "That's exactly what I meant, thank you. If I'm a developer working on an IDE and my boss says add some infinite loop detection for common cases, I'm not going to reply with \"no because turing\".\n\nLots of hate here though...holy shit.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19846, "question": "That's exactly what I meant, thank you. If I'm a developer working on an IDE and my boss says add some infinite loop detection for common cases, I'm not going to reply with \"no because turing\".\n\nLots of hate here though...holy shit.", "aSentId": 19847, "answer": "And you'll get the exact same thing from hardcore computer scientists. Its not like the **entire** field of program analysis just put up their hands and said \"everything we ever do is undecidable\". No, they made rigorous formulations for answering these questions by allowing for some errors.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19836, "question": "I think he means the maths says something can't be done, while the engineering says well we can make it work in the ways we need it to.", "aSentId": 19849, "answer": "That's what I thought. In this case, the \"engineering\" isnt solving what the math says is impossible.  Theres a longer comment that covers anything else I would say so ill just leave it at that.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19850, "question": "That's what I thought. In this case, the \"engineering\" isnt solving what the math says is impossible.  Theres a longer comment that covers anything else I would say so ill just leave it at that.", "aSentId": 19851, "answer": "Yeah, I think he means the engineering just looks for the good enough answer.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19852, "question": "Yeah, I think he means the engineering just looks for the good enough answer.", "aSentId": 19853, "answer": "Yeah I think so too. Which can be a frustrating thing about working in industry, but that's another discussion.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19832, "question": "I think this neatly sums up the difference between compsci as math and compsci as engineering.", "aSentId": 19855, "answer": "It actually neatly sums up the way people who just wanted a job as an engineer glossed over all the math in compsci &amp; don't understand it as a consequence.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19856, "question": "It actually neatly sums up the way people who just wanted a job as an engineer glossed over all the math in compsci &amp; don't understand it as a consequence.", "aSentId": 19857, "answer": "You neatly sum up the ridiculous elitism in the field.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19858, "question": "You neatly sum up the ridiculous elitism in the field.", "aSentId": 19859, "answer": "I don't think it's \"elitism\" to expect someone who went through a science program to understand what is probably the most basic foundational theory in our current understanding of that science.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19860, "question": "I don't think it's \"elitism\" to expect someone who went through a science program to understand what is probably the most basic foundational theory in our current understanding of that science.", "aSentId": 19861, "answer": "Where did OP say he has completed a degree in CS?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19862, "question": "Where did OP say he has completed a degree in CS?", "aSentId": 19863, "answer": "Haha I'm actually in third year of a CS degree so that doesn't look too good, I thought of the question in a lecture and didn't want to think through it myself.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19864, "question": "Haha I'm actually in third year of a CS degree so that doesn't look too good, I thought of the question in a lecture and didn't want to think through it myself.", "aSentId": 19865, "answer": "Pro tip for the future: google first, then ask.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19867, "question": "Are there any introductory algorithm resources (other than CLRS) that have LOTS of practice problems and solutions?", "aSentId": 19868, "answer": "Try codeforces.com.  The practice rooms are great.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19867, "question": "Are there any introductory algorithm resources (other than CLRS) that have LOTS of practice problems and solutions?", "aSentId": 19870, "answer": "I fucking LOVE Kleinberg and Tardos' *Algorithm Design*. More pedagogical than CLRS for sure.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19872, "question": "Really cool pattern generating algorithm. Have you ever seen anything like it?", "aSentId": 19873, "answer": "Don't know why I was saying that it doesn't color the origin. Anyway if you introduce vector arithmetic then it will invert a point if it is a multiple of the point you entered. Given a sequence of inputs a point will be black if it is a multiple of an odd number of the inputs.\n\nBasically you start with  \n&gt; array[n][m] := false  \n\nand when you input x, y you will have  \n&gt; array[n][m] := array[n][m] xor ((x divides n) and (y divides m))", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19872, "question": "Really cool pattern generating algorithm. Have you ever seen anything like it?", "aSentId": 19875, "answer": "With The points that are colored black are those of the form  \n&gt; (n\\*x, m\\*y)\n\nThe set of black points can be inductively defined by  \n&gt; (x, y) is black.  \n&gt; If (v, w) is black then (v + x, w) and (v, w + y) are black.  \n\n(And nothing else is black).\n\nAlso the set of black points is a union of lines. Every line has slope y/x and every line contains a point in the form (n\\*x, m\\*y). You can also define these lines inductively but I'm getting lazy and I don't want to do it. Anyway what you see drawn on the screen are these lines.\n\nIf you're familiar with linear algebra you'll know that the set of black points are those which are a linear combination of (x, 0) and (0, y) but excluding the origin (0, 0) which always seems to be white for whatever reason. (In what I have written x,y,n,m,v,w have all ranged over the positive integers.) Or put another way the vector space is that with the basis {(x, 0), (0, y)}.\n\nYou can probably come up with a vector space of lines if you properly define the arithmetic operators for lines.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19876, "question": "With The points that are colored black are those of the form  \n&gt; (n\\*x, m\\*y)\n\nThe set of black points can be inductively defined by  \n&gt; (x, y) is black.  \n&gt; If (v, w) is black then (v + x, w) and (v, w + y) are black.  \n\n(And nothing else is black).\n\nAlso the set of black points is a union of lines. Every line has slope y/x and every line contains a point in the form (n\\*x, m\\*y). You can also define these lines inductively but I'm getting lazy and I don't want to do it. Anyway what you see drawn on the screen are these lines.\n\nIf you're familiar with linear algebra you'll know that the set of black points are those which are a linear combination of (x, 0) and (0, y) but excluding the origin (0, 0) which always seems to be white for whatever reason. (In what I have written x,y,n,m,v,w have all ranged over the positive integers.) Or put another way the vector space is that with the basis {(x, 0), (0, y)}.\n\nYou can probably come up with a vector space of lines if you properly define the arithmetic operators for lines.", "aSentId": 19877, "answer": "I think there's a problem with your definition, because it is a requirement that x != 0 &amp; y != 0.\n\nChanging topic, yes, it's a combination over GF(2) of periodic patterns. The period in each direction is at most the least common multiple of 'x' or 'y'. I think it gets interesting when you choose a box as the fundamental tile and try to find a basis for this tile. I think the basis will involve the factors of the dimensions of the tile. So for example, a tile of size (5,7) will only have the basis elements {(1,1),(1,7),(5,1),(5,7)}, so 2^4 possible tiles; in general I expect the basis dimension for a tile of size (X,Y) to be m*n, where m is the number of factors of x and n the number of factors of y, yielding 2^mn possible (X,Y)-tiles, disregarding symmetries. Which interestingly is usually much lower than 2^xy , which may give rise to the nice symmetries.\n\nI believe there's a way to construct interesting tiles: take a few numbers relatively prime (e.g. x,y,z) to create a tile of size xyz by adding t=(1,x)+(x,1)+...+(z,1). Then choose elements not of the form (1,\\*) or (\\*,1).\n\nExample: (mirrored)\n\n * 1,2\n * 1,3\n * 1,5\n\nthen choose from {(2,2),(2,3),(2,5),(3,3),(3,5),(5,5)}. I really like (2,2)!\n\nFood for thought -- here's an interesting problem: given an arbitrary tile of dimension (X,Y), find the closest constructible tile (i.e. the \"projection\" on the given basis).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19876, "question": "With The points that are colored black are those of the form  \n&gt; (n\\*x, m\\*y)\n\nThe set of black points can be inductively defined by  \n&gt; (x, y) is black.  \n&gt; If (v, w) is black then (v + x, w) and (v, w + y) are black.  \n\n(And nothing else is black).\n\nAlso the set of black points is a union of lines. Every line has slope y/x and every line contains a point in the form (n\\*x, m\\*y). You can also define these lines inductively but I'm getting lazy and I don't want to do it. Anyway what you see drawn on the screen are these lines.\n\nIf you're familiar with linear algebra you'll know that the set of black points are those which are a linear combination of (x, 0) and (0, y) but excluding the origin (0, 0) which always seems to be white for whatever reason. (In what I have written x,y,n,m,v,w have all ranged over the positive integers.) Or put another way the vector space is that with the basis {(x, 0), (0, y)}.\n\nYou can probably come up with a vector space of lines if you properly define the arithmetic operators for lines.", "aSentId": 19879, "answer": "Well, when you draw several patterns, they each operate on the output of the previous.  So it does become more complicated than that :-)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19880, "question": "Well, when you draw several patterns, they each operate on the output of the previous.  So it does become more complicated than that :-)", "aSentId": 19881, "answer": "Actually it's just a bitmap. You start with all the bits 0 say and you make a new map using the input and then you take the XOR of both bitmaps. Conveniently XOR is associative so...", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19882, "question": "Actually it's just a bitmap. You start with all the bits 0 say and you make a new map using the input and then you take the XOR of both bitmaps. Conveniently XOR is associative so...", "aSentId": 19883, "answer": "it's also commutative, which is necessary to say \"order doesn't matter\"", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19886, "question": "From Set Theory to Type Theory", "aSentId": 19887, "answer": "This is way out of my league. Any suggestions on a good link/class to get an introduction to this stuff?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19888, "question": "This is way out of my league. Any suggestions on a good link/class to get an introduction to this stuff?", "aSentId": 19889, "answer": "Just from glancing at it it just seems more like his observations and opinions on some related subjects, not a real introduction. I mean he starts using acronyms apologetically and without introduction.\n\nTo understand what he wrote (and keep in mind I only looked it over once) you probably need to be familiar with the predicate calculus as well as set theory (in particular knowing how you can model things like pairs, natural numbers, etc. using set theory).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19891, "question": "Struggling in discrete structures. Any good text books I should read?", "aSentId": 19892, "answer": "I found Discrete Mathematics with Applications by Susanna Epp to be a gentle introduction to the topic.  ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19891, "question": "Struggling in discrete structures. Any good text books I should read?", "aSentId": 19894, "answer": "Read the textbook your prof assigned and ask specific questions here. Can you say more about which aspects of the course are troublesome? ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19891, "question": "Struggling in discrete structures. Any good text books I should read?", "aSentId": 19896, "answer": "Discrete structures was the first class I ever took that tended to confuse the shit out of me. What particular section of the course are you struggling with?\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19899, "question": "I have seen a very nice analogy of time of a clock cycle. Can anyone here find it?", "aSentId": 19900, "answer": "Yeh Im trying to remember that site too. I saved the actual table into a notepad but never thought to take the link. Heres the table anyway. The formatting is lost in reddit.... but you get the idea.\n\n1 CPU cycle\n0.3 ns\n1 s\nLevel 1 cache access\n0.9 ns\n3 s\nLevel 2 cache access\n2.8 ns\n9 s\nLevel 3 cache access\n12.9 ns\n43 s\nMain memory access\n120 ns\n6 min\nSolid-state disk I/O\n50-150 \u03bcs\n2-6 days\nRotational disk I/O\n1-10 ms\n1-12 months\nInternet: SF to NYC\n40 ms\n4 years\nInternet: SF to UK\n81 ms\n8 years\nInternet: SF to Australia\n183 ms\n19 years\nOS virtualization reboot\n4 s\n423 years\nSCSI command time-out\n30 s\n3000 years\nHardware virtualization reboot\n40 s\n4000 years\nPhysical system reboot\n5 m\n32 millenia\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19901, "question": "Yeh Im trying to remember that site too. I saved the actual table into a notepad but never thought to take the link. Heres the table anyway. The formatting is lost in reddit.... but you get the idea.\n\n1 CPU cycle\n0.3 ns\n1 s\nLevel 1 cache access\n0.9 ns\n3 s\nLevel 2 cache access\n2.8 ns\n9 s\nLevel 3 cache access\n12.9 ns\n43 s\nMain memory access\n120 ns\n6 min\nSolid-state disk I/O\n50-150 \u03bcs\n2-6 days\nRotational disk I/O\n1-10 ms\n1-12 months\nInternet: SF to NYC\n40 ms\n4 years\nInternet: SF to UK\n81 ms\n8 years\nInternet: SF to Australia\n183 ms\n19 years\nOS virtualization reboot\n4 s\n423 years\nSCSI command time-out\n30 s\n3000 years\nHardware virtualization reboot\n40 s\n4000 years\nPhysical system reboot\n5 m\n32 millenia\n", "aSentId": 19902, "answer": "Stick four spaces in front of your table to get monospace formatting:\n\n    1 CPU cycle                    0.3 ns    1 s \n    Level 1 cache access           0.9 ns    3 s \n    Level 2 cache access           2.8 ns    9 s \n    Level 3 cache access           12.9 ns   43 s \n    Main memory access             120 ns    6 min \n    Solid-state disk I/O           50-150 \u03bcs 2-6 days \n    Rotational disk I/O            1-10 ms   1-12 months \n    Internet: SF to NYC            40 ms     4 years \n    Internet: SF to UK             81 ms     8 years \n    Internet: SF to Australia      183 ms    19 years \n    OS virtualization reboot       4 s       423 years \n    SCSI command time-out          30 s      3000 years \n    Hardware virtualization reboot 40 s      4000 years \n    Physical system reboot         5 m       32 millenia ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19903, "question": "Stick four spaces in front of your table to get monospace formatting:\n\n    1 CPU cycle                    0.3 ns    1 s \n    Level 1 cache access           0.9 ns    3 s \n    Level 2 cache access           2.8 ns    9 s \n    Level 3 cache access           12.9 ns   43 s \n    Main memory access             120 ns    6 min \n    Solid-state disk I/O           50-150 \u03bcs 2-6 days \n    Rotational disk I/O            1-10 ms   1-12 months \n    Internet: SF to NYC            40 ms     4 years \n    Internet: SF to UK             81 ms     8 years \n    Internet: SF to Australia      183 ms    19 years \n    OS virtualization reboot       4 s       423 years \n    SCSI command time-out          30 s      3000 years \n    Hardware virtualization reboot 40 s      4000 years \n    Physical system reboot         5 m       32 millenia ", "aSentId": 19904, "answer": "Now that it is a handy little tip. Thanks :)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19903, "question": "Stick four spaces in front of your table to get monospace formatting:\n\n    1 CPU cycle                    0.3 ns    1 s \n    Level 1 cache access           0.9 ns    3 s \n    Level 2 cache access           2.8 ns    9 s \n    Level 3 cache access           12.9 ns   43 s \n    Main memory access             120 ns    6 min \n    Solid-state disk I/O           50-150 \u03bcs 2-6 days \n    Rotational disk I/O            1-10 ms   1-12 months \n    Internet: SF to NYC            40 ms     4 years \n    Internet: SF to UK             81 ms     8 years \n    Internet: SF to Australia      183 ms    19 years \n    OS virtualization reboot       4 s       423 years \n    SCSI command time-out          30 s      3000 years \n    Hardware virtualization reboot 40 s      4000 years \n    Physical system reboot         5 m       32 millenia ", "aSentId": 19906, "answer": "    mono\n    spacing\n    formatted\n    reply\n    ???", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19901, "question": "Yeh Im trying to remember that site too. I saved the actual table into a notepad but never thought to take the link. Heres the table anyway. The formatting is lost in reddit.... but you get the idea.\n\n1 CPU cycle\n0.3 ns\n1 s\nLevel 1 cache access\n0.9 ns\n3 s\nLevel 2 cache access\n2.8 ns\n9 s\nLevel 3 cache access\n12.9 ns\n43 s\nMain memory access\n120 ns\n6 min\nSolid-state disk I/O\n50-150 \u03bcs\n2-6 days\nRotational disk I/O\n1-10 ms\n1-12 months\nInternet: SF to NYC\n40 ms\n4 years\nInternet: SF to UK\n81 ms\n8 years\nInternet: SF to Australia\n183 ms\n19 years\nOS virtualization reboot\n4 s\n423 years\nSCSI command time-out\n30 s\n3000 years\nHardware virtualization reboot\n40 s\n4000 years\nPhysical system reboot\n5 m\n32 millenia\n", "aSentId": 19908, "answer": "Yeah it's this one! I'll look for it now that I have more of the data.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19910, "question": "help provinging the a logic equations are equal", "aSentId": 19911, "answer": "That notation is so bad it almost makes me wanna barf. True that conjunction distributes over disjunction but disjunction distributes over conjunction. In fact they are entirely symmetric operators that form a complemented lattice.\n\nFast way to check this is, of course, to construct a truth table, but fastest way is to check all the possible combinations with a computer program. Slow but effective way is to rewrite it using the laws of Boolean Algebra. It's worth noting that in the propositional calculus, the equality relation is an operator on the algebra. It's important because you have, for example, that disjunction distributes over equality. So you can rewrite your equation as  \n&gt; (ab'e'f + a'b'ef = false) + acde' + a'cd'e + b'c'f + b'df .\n\nContinuing in this vein you can easily arrive at your answer.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19912, "question": "That notation is so bad it almost makes me wanna barf. True that conjunction distributes over disjunction but disjunction distributes over conjunction. In fact they are entirely symmetric operators that form a complemented lattice.\n\nFast way to check this is, of course, to construct a truth table, but fastest way is to check all the possible combinations with a computer program. Slow but effective way is to rewrite it using the laws of Boolean Algebra. It's worth noting that in the propositional calculus, the equality relation is an operator on the algebra. It's important because you have, for example, that disjunction distributes over equality. So you can rewrite your equation as  \n&gt; (ab'e'f + a'b'ef = false) + acde' + a'cd'e + b'c'f + b'df .\n\nContinuing in this vein you can easily arrive at your answer.", "aSentId": 19913, "answer": "This is exactly how the question that i received is written (except for the not's). I havent done this sort of thing for a long time, and am having trouble even making a truth table. do you have any examples of how to do so ? ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19914, "question": "This is exactly how the question that i received is written (except for the not's). I havent done this sort of thing for a long time, and am having trouble even making a truth table. do you have any examples of how to do so ? ", "aSentId": 19915, "answer": "I know that the notation is not your own doing, I was not accusing you. You are a victim of it though.\n\nLook let's start by rewriting each of the disjuncts, to make things more manageable:\n&gt; p = a \u2227 \u00acb \u2227 \u00ace \u2227 f  \n&gt; q = \u00aca \u2227 \u00acb \u2227 e \u2227 f  \n&gt; r = a \u2227 c \u2227 d \u2227 \u00ace  \n&gt; s = \u00aca \u2227 c \u2227 \u00acd \u2227 e  \n&gt; t = \u00acb \u2227 \u00acc \u2227 f  \n&gt; u = \u00acb \u2227 d \u2227 f\n\nSo now our equation becomes  \n&gt; (p \u2228 q) \u2228 (r \u2228 s \u2228 t \u2228 u) = r \u2228 s \u2228 t \u2228 u  \n&gt; = { (x \u2228 y = y) = (x =&gt; y) with x := p \u2228 q and y := r \u2228 s \u2228 t \u2228 u }  \n&gt; p \u2228 q =&gt; r \u2228 s \u2228 t \u2228 u  \n&gt; = { (x \u2228 y =&gt; z) = (x =&gt; z) \u2227 (y =&gt; z) with x := p and y := q and z := r \u2228 s \u2228 t \u2228 u }  \n&gt; (p =&gt; r \u2228 s \u2228 t \u2228 u) \u2227 (q =&gt; r \u2228 s \u2228 t \u2228 u)\n\nNow for the left conjunct we observe  \n&gt; p =&gt; r \u2228 s \u2228 t \u2228 u  \n&gt; = { def. of p }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; r \u2228 s \u2228 t \u2228 u  \n&gt; = { def. of r }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; (a \u2227 c \u2227 d \u2227 \u00ace) \u2228 s \u2228 t \u2228 u  \n&gt; = { antecedent }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; (c \u2227 d) \u2228 s \u2228 t \u2228 u  \n&gt; = { def. of s }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; (c \u2227 d) \u2228 (\u00aca \u2227 c \u2227 \u00acd \u2227 e) \u2228 t \u2228 u  \n&gt; = { antecedent }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; (c \u2227 d) \u2228 t \u2228 u  \n&gt; = { def. of t }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; (c \u2227 d) \u2228 (\u00acb \u2227 \u00acc \u2227 f) \u2228 u  \n&gt; = { antecedent }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; (c \u2227 d) \u2228 \u00acc \u2228 u  \n&gt; = { negated absorption }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; d \u2228 \u00acc \u2228 u  \n&gt; = { def. of u }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; d \u2228 \u00acc \u2228 (\u00acb \u2227 d \u2227 f)  \n&gt; = { antecedent }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; d \u2228 \u00acc \u2228 d  \n&gt; = { \u2228 symmetric, idempotent }  \n&gt; a \u2227 \u00acb \u2227 \u00ace \u2227 f =&gt; d \u2228 \u00acc  \n\nAnd actually we can stop there because we can make the left conjunct (and therefore the whole expression) false by choosing a,b,e,f,d,c := true,false,false,true,false,true .", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19910, "question": "help provinging the a logic equations are equal", "aSentId": 19917, "answer": "The fuck happened with the title? ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19910, "question": "help provinging the a logic equations are equal", "aSentId": 19919, "answer": "Regarding the notational concern that PvtPoopyPants mentioned: Using juxtaposition (\"multiplication\") for \u2227 isn't really a problem, but using + for \u2228 is confusing.  That's in part because in a boolean ring the addition operation is *exclusive* or (symmetric difference), not disjunction.  In particular, \u2228 doesn't have inverses, so doesn't satisfy the group axioms, but mathematicians usually expect an operator written \"+\" to do so.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19921, "question": "Given a sentence, could a system create a similar sentence?", "aSentId": 19922, "answer": "Yes, WordAi is an example", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19925, "question": "How does computer science compare to computer engineering?", "aSentId": 19926, "answer": "Computer Science: \"In low-level languages like C\"\n\nComputer Engineering: \"In high-level languages like C\"", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19927, "question": "Computer Science: \"In low-level languages like C\"\n\nComputer Engineering: \"In high-level languages like C\"", "aSentId": 19928, "answer": "Best description ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19927, "question": "Computer Science: \"In low-level languages like C\"\n\nComputer Engineering: \"In high-level languages like C\"", "aSentId": 19930, "answer": "Wow, you nailed it! Best answer I ever read on this topic. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19925, "question": "How does computer science compare to computer engineering?", "aSentId": 19932, "answer": "Depends on the university, different programs have different focuses. \n\nI am almost done with a computer engineering degree and it seemed like just a lower focus on programming except for low level languages. You would take circuits, architecture (cs normally does  abit too but you would do more), signal processing, and micro controller programming,  while having less depth in cs topics such as ai, or algorithms (neither of those classes had its own course, they were both merged with other related topics).\n\nedit: also, i would look into how easy it is to transfer between one and the other during school, i figured out halfway through my degree i really hate circuits and signal processing and I really love programming high level languages and algorithm classes. But the system at my school makes it hard to switch between the 2 so i stuck with computer engineering. This dragged down my gpa and makes school a pain a lot of the time. You may figure out what you want more during college and it would be nice if you could switch at that point.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19925, "question": "How does computer science compare to computer engineering?", "aSentId": 19934, "answer": "Check out the course descriptions of the programs you're looking at.\n\nCS revolves around: Can we compute it, and how efficiently can we compute it.\n\nI'm not very familiar with computer engineering, but I've heard it's the hardware side of electronics.  So a sort of computer oriented electrical engineering?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19935, "question": "Check out the course descriptions of the programs you're looking at.\n\nCS revolves around: Can we compute it, and how efficiently can we compute it.\n\nI'm not very familiar with computer engineering, but I've heard it's the hardware side of electronics.  So a sort of computer oriented electrical engineering?", "aSentId": 19936, "answer": "That is generally correct. A computer engineering degree is generally 50% CS and 50% ElecEng.  Much like Mechatronics which is about 50% MechEng and 50% ElecEng.  The exact percentages will differ slightly though depending on the university.  The 50% ElecEng content also generally focuses on digital electronics, as apposed to RF and analogue electronics which you will also do in a full ElecEng degree.\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19935, "question": "Check out the course descriptions of the programs you're looking at.\n\nCS revolves around: Can we compute it, and how efficiently can we compute it.\n\nI'm not very familiar with computer engineering, but I've heard it's the hardware side of electronics.  So a sort of computer oriented electrical engineering?", "aSentId": 19938, "answer": "Software engineering includes a lot of CS, but focuses a lot more on  \"standards\" and \"testing\" as do most \"engineering\" disciplines.\n\nIt's there to make sure software doesn't fail. Make future development possible, and easier, as well as better and safer. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19935, "question": "Check out the course descriptions of the programs you're looking at.\n\nCS revolves around: Can we compute it, and how efficiently can we compute it.\n\nI'm not very familiar with computer engineering, but I've heard it's the hardware side of electronics.  So a sort of computer oriented electrical engineering?", "aSentId": 19940, "answer": "I'd define it as: Computer Science is all about software, Electronics Engineering is all about circuits and hardware, Computer Engineering is the middle-ground between them. \n\nSource: I'm a Computer Engineer ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19944, "question": "I hate to say this in a compsci setting, but:\n\nCEs can CS a lot easier than CSs can CE, so if you can manage the engineering courses, you get a more professionally flexible degree.\n\nAlso, many applications of computer programming aren't simply related to databases, apps or web technology, a lot of them are applied engineering in highly technical fields like signal processing, sensor systems, flight systems/avionics, communications technology, music/image/video processing, medical bioengineering technologies, space systems and so on.  \n\nEven though it's not impossible for someone with a purely CS background to get hired to work on, say, a navigation system, someone with an engineering background is a much better candidate to work with computer programming in advanced/applied science contexts than someone who took soft electives alongside their CS requirements.\n\nSo there are 2 layers to being a programmer: the programming/design itself and the underlying application's nature.  If you want to work on web apps, you don't need a CE.  If you want to work at NASA, you may be better off getting a CE instead of a CS.", "aSentId": 19945, "answer": "Not sure if I agree with you here, CS is engineering. \rI work at a satellite communications firm and we hire either EE or CS, as CE majors generally don't know enough about dsp/signal processing to be useful, and not enough algorithms/OS courses to be useful in the software side. Of course well take the really smart ones but generally they are disadvantaged and can't leverage what they have learned once here.\r\rI imagine NASA would be the same, but I don't really know any thing about their needs/hiring practices.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19946, "question": "Not sure if I agree with you here, CS is engineering. \rI work at a satellite communications firm and we hire either EE or CS, as CE majors generally don't know enough about dsp/signal processing to be useful, and not enough algorithms/OS courses to be useful in the software side. Of course well take the really smart ones but generally they are disadvantaged and can't leverage what they have learned once here.\r\rI imagine NASA would be the same, but I don't really know any thing about their needs/hiring practices.", "aSentId": 19947, "answer": "Computer science is not engineering. Just because you hire CS majors as engineers doesn't mean it is an engineering field.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19948, "question": "Computer science is not engineering. Just because you hire CS majors as engineers doesn't mean it is an engineering field.", "aSentId": 19949, "answer": "It's more engineering than science.. Closer to applied math. Depends on how your school teaches it I guess.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19950, "question": "It's more engineering than science.. Closer to applied math. Depends on how your school teaches it I guess.", "aSentId": 19951, "answer": "Computer science is a proper field with professionals. It is irrelevant how a school teaches it. It has been a field decades before it was common to give out degrees in it.\n\nI don't mean to be condescending but it's obvious you're not a computer scientist or know anybody who is. Your opinion is rather uninformed. It's true that many people study computer science to go on to be software engineers but that doesn't change what the *field* (not the degree) of computer science is.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19952, "question": "Computer science is a proper field with professionals. It is irrelevant how a school teaches it. It has been a field decades before it was common to give out degrees in it.\n\nI don't mean to be condescending but it's obvious you're not a computer scientist or know anybody who is. Your opinion is rather uninformed. It's true that many people study computer science to go on to be software engineers but that doesn't change what the *field* (not the degree) of computer science is.", "aSentId": 19953, "answer": "Okay, that's like saying electrical engineering is just physics, even though they might do engineering once hired.\r\rI've been out of school for four years now, and now evaluate new grads in addition to embedded software engineering :)  It's obvious you are still in school. From your point of view there are no CS/math/physics jobs other than research, am I correct? Don't get hung up on semantics because you have a stick up your butt. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19954, "question": "Okay, that's like saying electrical engineering is just physics, even though they might do engineering once hired.\r\rI've been out of school for four years now, and now evaluate new grads in addition to embedded software engineering :)  It's obvious you are still in school. From your point of view there are no CS/math/physics jobs other than research, am I correct? Don't get hung up on semantics because you have a stick up your butt. ", "aSentId": 19955, "answer": "No it's not because electrical engineering is not physics. Computer science is a misnomer because computer science is not about computers. An electrical engineer does engineering because that field is appropriately named. That's why some people use the name \"computing science\" instead, and why some departments are named that, to avoid confusion.\n\n&gt; From your point of view there are no CS/math/physics jobs other than research, am I correct?\n\nWhat on Earth are you on about. Computer science is a *science* and a computer scientist is a *scientist*. A scientist is someone who does research. If they don't then they're not computer scientists, even if they have a degree in computer science. I didn't say there are no jobs other than research. It is perfectly legitimate to study a science and not go on to practice it. You can a learn a lot that will be useful regardless of that. The same way a physician can do his undergraduate degree in biology. Are you going to say that physicians are biologists or scientists?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19946, "question": "Not sure if I agree with you here, CS is engineering. \rI work at a satellite communications firm and we hire either EE or CS, as CE majors generally don't know enough about dsp/signal processing to be useful, and not enough algorithms/OS courses to be useful in the software side. Of course well take the really smart ones but generally they are disadvantaged and can't leverage what they have learned once here.\r\rI imagine NASA would be the same, but I don't really know any thing about their needs/hiring practices.", "aSentId": 19957, "answer": "I guess I can agree with this.  CE can be mainly computer hardware and students can skip out on the aspect of engineering that is more applied physics.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19944, "question": "I hate to say this in a compsci setting, but:\n\nCEs can CS a lot easier than CSs can CE, so if you can manage the engineering courses, you get a more professionally flexible degree.\n\nAlso, many applications of computer programming aren't simply related to databases, apps or web technology, a lot of them are applied engineering in highly technical fields like signal processing, sensor systems, flight systems/avionics, communications technology, music/image/video processing, medical bioengineering technologies, space systems and so on.  \n\nEven though it's not impossible for someone with a purely CS background to get hired to work on, say, a navigation system, someone with an engineering background is a much better candidate to work with computer programming in advanced/applied science contexts than someone who took soft electives alongside their CS requirements.\n\nSo there are 2 layers to being a programmer: the programming/design itself and the underlying application's nature.  If you want to work on web apps, you don't need a CE.  If you want to work at NASA, you may be better off getting a CE instead of a CS.", "aSentId": 19959, "answer": "In theory, CE bridges the gap between CS and EE. In practice, graduates come out of college lacking fundamental skills in both.  \nA lot of companies go after CEs trying to kill two birds with one stone, but what you end up with is half-assed hardware AND half-assed software. So far my experience in the aerospace industry has been to let full electrical guys do the hardware, full software guys do the SW, and carefully mediate where they meet. C code written by CE's ends up being much more expensive through code reviews  and bugs. I can't tell you how much time i've spent trying to explain virtual memory, address spaces and cache hierarchies to fresh-hired CEs.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19960, "question": "In theory, CE bridges the gap between CS and EE. In practice, graduates come out of college lacking fundamental skills in both.  \nA lot of companies go after CEs trying to kill two birds with one stone, but what you end up with is half-assed hardware AND half-assed software. So far my experience in the aerospace industry has been to let full electrical guys do the hardware, full software guys do the SW, and carefully mediate where they meet. C code written by CE's ends up being much more expensive through code reviews  and bugs. I can't tell you how much time i've spent trying to explain virtual memory, address spaces and cache hierarchies to fresh-hired CEs.", "aSentId": 19961, "answer": "As I said to the other post, I can see how this can happen if CEs aren't learning real applied-physics type engineering (rather, computer hardware) AND they're not learning real programming/CS skills.  At the school I go to, the CE students take real CS classes up to the junior level, I believe.\n\nIt's possible to get the worst of both worlds, which it sounds like you're getting -- people who are more computer hardware than they are real applied-physics engineering types and not good programmers/software engineers, either.\n\nI was coming from the perspective of best of both worlds.  In all, I think whether you get the best of both worlds or the worst of both worlds depends, in part, what schools you recruit from and how competitive your offers are.\n\nOn the other hand, I see A LOT of CS majors blowing through the program cheating and barely learning the material.  It seems to me that it's much easier to cheat in CS than engineering.  So if you want to go with the worst of both worlds narrative, a bad/shallow-learning CS grad is not better than a bad/shallow-learning CE grad.\n\nOne upside to to being single-focused (either engineering or CS but not both) is that you can be less shallow and more competent in the area of your single focus.  But again, it seems to me that is, again, an issue of what schools you may be recruiting from and how competitive your offers are.\n\n&gt; I can't tell you how much time i've spent trying to explain virtual memory, address spaces and cache hierarchies to fresh-hired CEs.\n\nIt sounds to me like you have some bad schools in your region, because any CE should know that.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19960, "question": "In theory, CE bridges the gap between CS and EE. In practice, graduates come out of college lacking fundamental skills in both.  \nA lot of companies go after CEs trying to kill two birds with one stone, but what you end up with is half-assed hardware AND half-assed software. So far my experience in the aerospace industry has been to let full electrical guys do the hardware, full software guys do the SW, and carefully mediate where they meet. C code written by CE's ends up being much more expensive through code reviews  and bugs. I can't tell you how much time i've spent trying to explain virtual memory, address spaces and cache hierarchies to fresh-hired CEs.", "aSentId": 19963, "answer": "&gt;C code written by CE's ends up being much more expensive through code reviews and bugs.\n\nSounds quite based. From my experience coding skills are way more dependent on the individual than on the degree. Especially because most CS degrees don't teach that much pratical software development anyway.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19966, "question": "One knows how and why it works (CS) the other knows how to use it (CompEng)\n\nThat's pretty much my understanding of it, being a Computer Science master myself, I might be a bit coloured.", "aSentId": 19967, "answer": "I think you have those backwards.\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19968, "question": "I think you have those backwards.\n", "aSentId": 19969, "answer": "Why? My understanding is that CS is \"theory\" and CompEng is \"practice\".\n\nWe don't have \"Computer Science\" in Denmark, where I'm from. Instead it's called \"Datalogi\", also those I've spoken to aren't \"Computer Engineers\", but rather \"Software Engineers\". They too agree on what I stated above, that they are taught how to use stuff, but not the actual internals and theory.\n\nAn example is that they know lower limit of sorting is \u03a9(nlogn), but not how to prove it.\n\nI know, in theory, how to create a lot of complicated algorithms, but I'd have an engineer implement it from some specs.\n\n^((Note: The line between them are really thin)^) ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19970, "question": "Why? My understanding is that CS is \"theory\" and CompEng is \"practice\".\n\nWe don't have \"Computer Science\" in Denmark, where I'm from. Instead it's called \"Datalogi\", also those I've spoken to aren't \"Computer Engineers\", but rather \"Software Engineers\". They too agree on what I stated above, that they are taught how to use stuff, but not the actual internals and theory.\n\nAn example is that they know lower limit of sorting is \u03a9(nlogn), but not how to prove it.\n\nI know, in theory, how to create a lot of complicated algorithms, but I'd have an engineer implement it from some specs.\n\n^((Note: The line between them are really thin)^) ", "aSentId": 19971, "answer": "In the US at least, computer engineering means you learn circuits, signal processing, more physics courses, all the steps of computer architecture from basic logic gates up to programming FPGAs, etc. Close to electrical engineering, but also many of the same courses compsci students can take like data structures, operating systems, AI, security, computer vision, and such.\n\nComputer engineering will teach you how to prove algorithm run times. They won't however have to make proofs on nondeterministic finite automata or turing machines or take a discreet math/logic course.\n\nI think the same way you describe for software engineers vs computer scientists, but a computer engineer is something different. Computer engineering is arguably a more difficult major and I've seen many more ECE grads adapt to CS jobs than CS grads in ECE jobs.\n\nComputer engineer: assembly, C, circuits, differential equations, physics, intel, qualcomm\n\nComputer scientist: C++, python, graphing, clustering, lambda calculus, microsoft, facebook", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19970, "question": "Why? My understanding is that CS is \"theory\" and CompEng is \"practice\".\n\nWe don't have \"Computer Science\" in Denmark, where I'm from. Instead it's called \"Datalogi\", also those I've spoken to aren't \"Computer Engineers\", but rather \"Software Engineers\". They too agree on what I stated above, that they are taught how to use stuff, but not the actual internals and theory.\n\nAn example is that they know lower limit of sorting is \u03a9(nlogn), but not how to prove it.\n\nI know, in theory, how to create a lot of complicated algorithms, but I'd have an engineer implement it from some specs.\n\n^((Note: The line between them are really thin)^) ", "aSentId": 19973, "answer": "Nah, Software Engineering and Computer Engineering is even more different than Computer Science and Computer Engineering.\n\nComputer Engineering = CS + Electrical Engineering", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19975, "question": "science is theory and engineering is implementation.", "aSentId": 19976, "answer": "And that distinction has absolutely no bearing on any education system. Do you think that Electrical Engineering majors spend their time on building circuits or things like Signal Theory and Fourior Analysis? That's just a subset of science. The same thing can be said for any engineering discipline where academic exposure is over 90% just theory. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19978, "question": "Faculty size and growth in the top 20 Computer Science departments.", "aSentId": 19979, "answer": "It looks like some high school student with serious adhd was in charge of doing the graphs.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19981, "question": "Need help with CompSci homework (Java/Eclipse)! Absolutely lost and require explanation!", "aSentId": 19982, "answer": "In future, please write out the problem you're dealing with and an attempt at a solution (or at least some thoughts about what you might do). Also it might be more appropriate to post this in a sub specifically for programming or learning to program. Having said that, I will help with 1a) The idea of this is to a) create an array of size equal to the number of tosses b) loop through the array. At each index, generate a random number between 1 and some value specified by an attribute of the number cube object you passed into the method (Java has built in functions and libraries that can help you with this). That's it. It's maybe 5 to 10 lines of code total. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19981, "question": "Need help with CompSci homework (Java/Eclipse)! Absolutely lost and require explanation!", "aSentId": 19984, "answer": "I could try to help if you send me the problems.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19987, "question": "If anyone wants to discuss Godel, Escher, Bach, we have a read-through of it going in /r/rational. Join us!", "aSentId": 19988, "answer": "Any interest in cross-posting to /r/geb?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19987, "question": "If anyone wants to discuss Godel, Escher, Bach, we have a read-through of it going in /r/rational. Join us!", "aSentId": 19990, "answer": "Hey. Thanks :) ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19987, "question": "If anyone wants to discuss Godel, Escher, Bach, we have a read-through of it going in /r/rational. Join us!", "aSentId": 19992, "answer": "For any aspiring CS students, this book is an all-but-required read.  It gives a great background for many of the theoretical aspects of the field.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19993, "question": "For any aspiring CS students, this book is an all-but-required read.  It gives a great background for many of the theoretical aspects of the field.", "aSentId": 19994, "answer": "It was used as a textbook for one of my favorite classes. Really hard class but very  glad I took it.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19993, "question": "For any aspiring CS students, this book is an all-but-required read.  It gives a great background for many of the theoretical aspects of the field.", "aSentId": 19996, "answer": "As an aspiring CS student, can you elaborate on this? I intended to start the book this weekend and catch up with the read-through, since I'm busy ATM.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 19997, "question": "As an aspiring CS student, can you elaborate on this? I intended to start the book this weekend and catch up with the read-through, since I'm busy ATM.", "aSentId": 19998, "answer": "It covers many of the topics that you'll cover in computer science theory, but in a non-mathematical way.  However, the book is fairly readable, is intended for the layman, and you'll come away glad you read it.  It is not a textbook, but intended to be read for enjoyment (and to stretch your brain).\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20002, "question": "Does anyone know if there's a fairly comprehensive guide on C++?", "aSentId": 20003, "answer": "C++ is too expansive to cover with just a set of notes. I have a book called C++ Primer that I thought was good when I took my C++ class (maybe *actual* programmers disagree) so maybe check out that.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20002, "question": "Does anyone know if there's a fairly comprehensive guide on C++?", "aSentId": 20005, "answer": "Stroustrop's \"The C++ Programming Language\" is about as close as you get, but it is not a light read to read in a week. I recommend reading the latest addition if you are serious about becoming a quality C++ developer. Should keep you busy over summer vacation.  If you finish that,  Andre's template metaprogramming  book is a good ad on, plus Scott Meyer's Effective series.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20008, "question": "Super Simple Python Question", "aSentId": 20009, "answer": "In this case, the 2 represents that you want an answer rounded to the hundredths place. This number could be changed depending on the accuracy needed, but it looks like that's as accurate as it needs to be for this case. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20010, "question": "In this case, the 2 represents that you want an answer rounded to the hundredths place. This number could be changed depending on the accuracy needed, but it looks like that's as accurate as it needs to be for this case. ", "aSentId": 20011, "answer": "Sorry to nit pick but it is technically precision and not accuracy ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20012, "question": "Sorry to nit pick but it is technically precision and not accuracy ", "aSentId": 20013, "answer": "Oh whoops, I very often mix up the two, thanks for pointing that out. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20010, "question": "In this case, the 2 represents that you want an answer rounded to the hundredths place. This number could be changed depending on the accuracy needed, but it looks like that's as accurate as it needs to be for this case. ", "aSentId": 20015, "answer": "Thanks a lot!\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20017, "question": "How to make a Search engine on a database", "aSentId": 20018, "answer": "Azure search looks nice too. The natural language library alone makes me want to use it. \n \nEdit: forgot to mention azure machine learning for the big data stuff.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20020, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 20021, "answer": "I decided to go to the source: *The Art of Computer Programing: Volume 3. Sorting and Searching* by **Donald Knuth** (second edition).\n\nI was hoping to get a fast answer from his 46-page chapter on hashing.  No such luck.  Here's some context that I'm quoting from him:\n\n&gt;  **Analysis of the algorithms**.  It is especially important to know the average behavior of a hashing method, because we are committed to trusting in the laws of probability whenever we hash.  The worst case of these algorithms in almost unthinkably bad, so we need to be reassured that the average behavior is very good.\n\nWhat follows is several pages of hard math [I did a math undergrad, and I'm a PhD student in CS at a top university -- so I'm qualified to say it's \"hard\"].  Nowhere do we see \"O(1)\" in Knuth's work...\n\n...*until the last paragraph*, which must have been added in the second edition:\n\n&gt; In the theoretical realm, more complicated methods have been devised by which is is possible to guarantee O(1) maximum time per access, with O(1) average amortized time per insertion and deletion, regardless of the keys being examined; moreover, the total storage used at any time is bounded by a constant times the number of items currently present, plus another additive constant.  This result, which builds on ideas of Fredman, Komlos, and Szemeredi [JACM 31 (1984), 538-544], is due to Dietzfelbinger, Karlin, Mehlhorn, Meyer auf der Heide, Rohnert, and Tarjan [SICOMP 23 (1994), 738-761].\n\nNow, on a practical note, his chapter has some very useful graphs.  They show the \"average number of probes\" on the y-axis plotted against the \"load factor\" (i.e. proportion of buckets that are filled) for several hashing strategies on the x-axis.  The plot looks kind of like [this](http://algoviz.org/OpenDSA/Books/OpenDSA/html/_images/hashplot.png).  (I couldn't find the exact thing online, but this really does look very similar)  The graphs go up exponentially as the load factor approaches one, but when the load factor is say, 0.5, linear probing requires 2.5 probes on average for an unsuccessful search and 1.5 for a successful search (using Knuth's numbers).\n\n*so*, based on these graphs, I can use my probability intuition and suggest something like this:\n\nIf you have:\n\n * a uniform hash function, meaning, if you uniformly pick x and y then P(h(x) = h(y)) = 1/N where N is the number of buckets\n * a bound on the load factor (e.g. the load factor is always 0.5 or less)\n * and maybe some other assumptions\n\nthen you get O(1) amortized complexity.  There's a major caveat here: this constraint on the load factor, which limits how many things you can put into a table of a given size.  It's important to remember that *the load factor can go down if you delete from the hash table*. There are plenty of people bickering in other comments about how for a table of size M there exists an N such that if you do N insertions it takes more than O(N) time, which isn't what we want.  So to get O(1) amortized complexity you need to ensure that you also delete elements from the hash table often enough to keep your load factor under the given bound.  This way, for *any* N, you can have a stream of N operations of inserts and deletes, and so long as you keep the load factor under your bound by deleting frequently enough, then you get your O(1) amortized complexity guarantee.  I think.  (This part is my speculation, not from Knuth).\n\n**TL;DR - it's very complicated.  Anyone here suggesting a quick answer is probably wrong.  Lookup SICOMP 23 (1994), 738-761.**", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20022, "question": "I decided to go to the source: *The Art of Computer Programing: Volume 3. Sorting and Searching* by **Donald Knuth** (second edition).\n\nI was hoping to get a fast answer from his 46-page chapter on hashing.  No such luck.  Here's some context that I'm quoting from him:\n\n&gt;  **Analysis of the algorithms**.  It is especially important to know the average behavior of a hashing method, because we are committed to trusting in the laws of probability whenever we hash.  The worst case of these algorithms in almost unthinkably bad, so we need to be reassured that the average behavior is very good.\n\nWhat follows is several pages of hard math [I did a math undergrad, and I'm a PhD student in CS at a top university -- so I'm qualified to say it's \"hard\"].  Nowhere do we see \"O(1)\" in Knuth's work...\n\n...*until the last paragraph*, which must have been added in the second edition:\n\n&gt; In the theoretical realm, more complicated methods have been devised by which is is possible to guarantee O(1) maximum time per access, with O(1) average amortized time per insertion and deletion, regardless of the keys being examined; moreover, the total storage used at any time is bounded by a constant times the number of items currently present, plus another additive constant.  This result, which builds on ideas of Fredman, Komlos, and Szemeredi [JACM 31 (1984), 538-544], is due to Dietzfelbinger, Karlin, Mehlhorn, Meyer auf der Heide, Rohnert, and Tarjan [SICOMP 23 (1994), 738-761].\n\nNow, on a practical note, his chapter has some very useful graphs.  They show the \"average number of probes\" on the y-axis plotted against the \"load factor\" (i.e. proportion of buckets that are filled) for several hashing strategies on the x-axis.  The plot looks kind of like [this](http://algoviz.org/OpenDSA/Books/OpenDSA/html/_images/hashplot.png).  (I couldn't find the exact thing online, but this really does look very similar)  The graphs go up exponentially as the load factor approaches one, but when the load factor is say, 0.5, linear probing requires 2.5 probes on average for an unsuccessful search and 1.5 for a successful search (using Knuth's numbers).\n\n*so*, based on these graphs, I can use my probability intuition and suggest something like this:\n\nIf you have:\n\n * a uniform hash function, meaning, if you uniformly pick x and y then P(h(x) = h(y)) = 1/N where N is the number of buckets\n * a bound on the load factor (e.g. the load factor is always 0.5 or less)\n * and maybe some other assumptions\n\nthen you get O(1) amortized complexity.  There's a major caveat here: this constraint on the load factor, which limits how many things you can put into a table of a given size.  It's important to remember that *the load factor can go down if you delete from the hash table*. There are plenty of people bickering in other comments about how for a table of size M there exists an N such that if you do N insertions it takes more than O(N) time, which isn't what we want.  So to get O(1) amortized complexity you need to ensure that you also delete elements from the hash table often enough to keep your load factor under the given bound.  This way, for *any* N, you can have a stream of N operations of inserts and deletes, and so long as you keep the load factor under your bound by deleting frequently enough, then you get your O(1) amortized complexity guarantee.  I think.  (This part is my speculation, not from Knuth).\n\n**TL;DR - it's very complicated.  Anyone here suggesting a quick answer is probably wrong.  Lookup SICOMP 23 (1994), 738-761.**", "aSentId": 20023, "answer": "I was assuming a perfect hash function; the criticisms I'm makin apply even in that case. This is the core of my point:\n\n&gt;then you get O(1) amortized complexity. There's a major caveat here: this constraint on the load factor, which limits how many things you can put into a table of a given size. \n\nBut that's the problem: if you assume the same hash function (possibly with some truncation operation), then this means you're putting a bound on n, which violates standard big-O analysis! Why no just call sorting O(1) then?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20024, "question": "I was assuming a perfect hash function; the criticisms I'm makin apply even in that case. This is the core of my point:\n\n&gt;then you get O(1) amortized complexity. There's a major caveat here: this constraint on the load factor, which limits how many things you can put into a table of a given size. \n\nBut that's the problem: if you assume the same hash function (possibly with some truncation operation), then this means you're putting a bound on n, which violates standard big-O analysis! Why no just call sorting O(1) then?", "aSentId": 20025, "answer": "Also, more importantly than what I have to say is what Knuth points out: *there is theoretical work with a O(1) time bound, but it's complicated and recent*.  It's safe to say that nobody actually implements it this way.  I might go into my comment and remove all the stuff that I added and leave only what Knuth says.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20024, "question": "I was assuming a perfect hash function; the criticisms I'm makin apply even in that case. This is the core of my point:\n\n&gt;then you get O(1) amortized complexity. There's a major caveat here: this constraint on the load factor, which limits how many things you can put into a table of a given size. \n\nBut that's the problem: if you assume the same hash function (possibly with some truncation operation), then this means you're putting a bound on n, which violates standard big-O analysis! Why no just call sorting O(1) then?", "aSentId": 20027, "answer": "You're right, perhaps you misunderstood what I wrote because you thought I was saying what everyone else said.  If you *only allow inserts*, then hashing as we typically do it, is **not** O(1) amortized.  \n\nIf you allow *inserts and deletes* and, always guarantee that you load factor stays under a certain limit then you can get the amortized guarantee.\n\nFor example, I have a hash table of size 100 and my load factor bound is 0.5.\n\nI do 50 inserts\n25 deletes\n25 inserts\n25 deletes\n...\nfor a total of M operations.\n\nNo matter how huge M is, you will witness O(1) amortized performance.  If you took this stream of operations and removed the deletes you're out of luck, it's not O(1) anymore.\n\nPlease read my comment carefully -- the real takeaway is that hashing, as the vast majority of people implement it, is not O(1) amortized.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20028, "question": "You're right, perhaps you misunderstood what I wrote because you thought I was saying what everyone else said.  If you *only allow inserts*, then hashing as we typically do it, is **not** O(1) amortized.  \n\nIf you allow *inserts and deletes* and, always guarantee that you load factor stays under a certain limit then you can get the amortized guarantee.\n\nFor example, I have a hash table of size 100 and my load factor bound is 0.5.\n\nI do 50 inserts\n25 deletes\n25 inserts\n25 deletes\n...\nfor a total of M operations.\n\nNo matter how huge M is, you will witness O(1) amortized performance.  If you took this stream of operations and removed the deletes you're out of luck, it's not O(1) anymore.\n\nPlease read my comment carefully -- the real takeaway is that hashing, as the vast majority of people implement it, is not O(1) amortized.", "aSentId": 20029, "answer": "I still think we're talking past each other, but let me do my best be responsive. \n\nMy claim is that, regardless of assumptions you can make about the load factor, the lookup time is still an increasing function of n, where n is the number of items you wish to keep in the list. \n\nThe hard part is proving why this remains true if any one table can assume the number of inputs remains bounded for that table. And the reason is that, the larger the table you want, the more operations your hash function requires, because a larger hash output requires linearly more operations. This isn't obvious when you use a 128 but hash and can keep truncating it, but what about when you have more than 2^128 elements? If you want a 129 bit hash function, you need to perform additional operations, or you fall back to the c-items-per-bucket case. \n\nThe only way around that would be to use identity as the hash function, but then you can cheat and hide all the complexity in metadata for each key. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20030, "question": "I still think we're talking past each other, but let me do my best be responsive. \n\nMy claim is that, regardless of assumptions you can make about the load factor, the lookup time is still an increasing function of n, where n is the number of items you wish to keep in the list. \n\nThe hard part is proving why this remains true if any one table can assume the number of inputs remains bounded for that table. And the reason is that, the larger the table you want, the more operations your hash function requires, because a larger hash output requires linearly more operations. This isn't obvious when you use a 128 but hash and can keep truncating it, but what about when you have more than 2^128 elements? If you want a 129 bit hash function, you need to perform additional operations, or you fall back to the c-items-per-bucket case. \n\nThe only way around that would be to use identity as the hash function, but then you can cheat and hide all the complexity in metadata for each key. ", "aSentId": 20031, "answer": "I think I agree with your claim, if I understand correctly, you're saying that \"as you add things into your hash table the expected time to insert (i.e. number of probes required) increases\".  This is totally true.  And that's why hashing, as we typically see it *is not O(1) amortized*, because as N increases, the steps it takes to insert N items is a function f(n) that's not a member of O(n) (i.e. f grows faster than linearly).  Are we good so far?\n\nGoing forward, we need to be very careful about what 'n' is.  For a hash table of a fixed size, with the load factor bounded by a fixed constant, we are interested in the number of computational steps required, f(n) to insert n elements.  Note that f(n) is a function that depends on the size of the hash table and the load factor.  If we increase the size of the hash table, we get a whole new f(n); in particular, increasing the size of the hash table doesn't correspond to increasing n.  Rather, inserting more inputs into the hash table corresponds to increasing n.\n\nOkay, now let's consider the bounded case. Create a table with 2^128 elements.  We have some hash function *h* that takes a key and produces 128 bits. Lets say it takes *k* steps to run *h* on a single key.  It doesn't matter if you've inserted 0 elements or 2^128 elements, *h* still takes the same amount of time to run, so it runs in O(1).  *However*, once you've inserted many elements, say 2^128 , your load factor is 1/2, so it will take multiple probes to find the correct point in your hash function.  Using Knuth's data, lets say the expected number of probes for load factor 1/2 is 2.5.  Then the *expected* number of steps to insert n new value is f(n) = 2.5*k*n = O(n).  So, we get O(1) amortized complexity.\n\nIf we create a table with 2^129 elements and hash keys to 129 bit values, then we'll get a new hash function.  It probably takes more steps to compute than the old one did, but still a constant number of steps in the worst case scenario.  We do the exact same work as before: let h' denote the new hash function, k' denote the new number of steps to run this hash function once, and v denote the expected number of probes per insert at the load factor.  Then the expected number of steps to insert n elements is f(n) = k'*n*v = O(n), and thus, O(1) amortized.\n\nDoes this help?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20032, "question": "I think I agree with your claim, if I understand correctly, you're saying that \"as you add things into your hash table the expected time to insert (i.e. number of probes required) increases\".  This is totally true.  And that's why hashing, as we typically see it *is not O(1) amortized*, because as N increases, the steps it takes to insert N items is a function f(n) that's not a member of O(n) (i.e. f grows faster than linearly).  Are we good so far?\n\nGoing forward, we need to be very careful about what 'n' is.  For a hash table of a fixed size, with the load factor bounded by a fixed constant, we are interested in the number of computational steps required, f(n) to insert n elements.  Note that f(n) is a function that depends on the size of the hash table and the load factor.  If we increase the size of the hash table, we get a whole new f(n); in particular, increasing the size of the hash table doesn't correspond to increasing n.  Rather, inserting more inputs into the hash table corresponds to increasing n.\n\nOkay, now let's consider the bounded case. Create a table with 2^128 elements.  We have some hash function *h* that takes a key and produces 128 bits. Lets say it takes *k* steps to run *h* on a single key.  It doesn't matter if you've inserted 0 elements or 2^128 elements, *h* still takes the same amount of time to run, so it runs in O(1).  *However*, once you've inserted many elements, say 2^128 , your load factor is 1/2, so it will take multiple probes to find the correct point in your hash function.  Using Knuth's data, lets say the expected number of probes for load factor 1/2 is 2.5.  Then the *expected* number of steps to insert n new value is f(n) = 2.5*k*n = O(n).  So, we get O(1) amortized complexity.\n\nIf we create a table with 2^129 elements and hash keys to 129 bit values, then we'll get a new hash function.  It probably takes more steps to compute than the old one did, but still a constant number of steps in the worst case scenario.  We do the exact same work as before: let h' denote the new hash function, k' denote the new number of steps to run this hash function once, and v denote the expected number of probes per insert at the load factor.  Then the expected number of steps to insert n elements is f(n) = k'*n*v = O(n), and thus, O(1) amortized.\n\nDoes this help?", "aSentId": 20033, "answer": "Thanks for the write up, but it still comes back to the same question: why wouldn't k need to increase logarithmically with n? And v linearly with the load factor?\n\nEdit: to clarify, I think I agree with your first paragraph, but I also think it contradicts your last paragraph since it implies k and v would not be constant in n. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20034, "question": "Thanks for the write up, but it still comes back to the same question: why wouldn't k need to increase logarithmically with n? And v linearly with the load factor?\n\nEdit: to clarify, I think I agree with your first paragraph, but I also think it contradicts your last paragraph since it implies k and v would not be constant in n. ", "aSentId": 20035, "answer": "The assumption that they make for the hashing algorithm h(x) is that to generate a 64 bit hash is the same time as to generate a 128 bit hash for the same object. This doesn't seem to make sense at first however lets assume we are using modulo to generate the hash and that we have an object O with a constant size (note that I am limiting the object's size not the number of objects).\n\nTo generate a 64 bit hash we take object % 2^64. C Arithmetic operations since O is of constant size.\n\nTo generate a 128 bit hash we take object % 2^128. C Arithmetic operations. \n\nTo generate a 256 bit hash we take object % 2^256. C Arithmetic operations.\n\nNow instead of the actual object we may use memory address or some other convenient (normally distributed) thing available to us. \n\nSo with a fairly simple hashing algorithm h(x) = O % k we have achieved a O(c) hash function.\n\nNote that I've also made the assumption that our word size for the processor is big enough to hold k. Which is a fair assumption.\n\nEDIT: forgot 2^ in a bunch of places\n\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20036, "question": "The assumption that they make for the hashing algorithm h(x) is that to generate a 64 bit hash is the same time as to generate a 128 bit hash for the same object. This doesn't seem to make sense at first however lets assume we are using modulo to generate the hash and that we have an object O with a constant size (note that I am limiting the object's size not the number of objects).\n\nTo generate a 64 bit hash we take object % 2^64. C Arithmetic operations since O is of constant size.\n\nTo generate a 128 bit hash we take object % 2^128. C Arithmetic operations. \n\nTo generate a 256 bit hash we take object % 2^256. C Arithmetic operations.\n\nNow instead of the actual object we may use memory address or some other convenient (normally distributed) thing available to us. \n\nSo with a fairly simple hashing algorithm h(x) = O % k we have achieved a O(c) hash function.\n\nNote that I've also made the assumption that our word size for the processor is big enough to hold k. Which is a fair assumption.\n\nEDIT: forgot 2^ in a bunch of places\n\n", "aSentId": 20037, "answer": "But a 256 bit hash takes linearly more operations to compute than a 64 bit.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20038, "question": "But a 256 bit hash takes linearly more operations to compute than a 64 bit.", "aSentId": 20039, "answer": "As long as the number of bits in the hash is less than word size of the processor it actually takes the same amount of time. It takes exactly one arithmetic operation. \n\nSince the word size of the processor also limits the addressable memory space you will run out of memory at the same time that this operation takes more than 1 arithmetic operation.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20040, "question": "As long as the number of bits in the hash is less than word size of the processor it actually takes the same amount of time. It takes exactly one arithmetic operation. \n\nSince the word size of the processor also limits the addressable memory space you will run out of memory at the same time that this operation takes more than 1 arithmetic operation.", "aSentId": 20041, "answer": "And as long as you get to bound n by a constant, you're not doing big O ...", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20042, "question": "And as long as you get to bound n by a constant, you're not doing big O ...", "aSentId": 20043, "answer": "Correct, however if we increase the word size of the processor to allow for more memory (ie to increase addressable space for our algorithm so we don't get OOM) then we also increase the size of k for constant time operations we can perform. Thus if we want a 128 bit key we have to increase word size so that we can address 2^128 memory positions and now we can do any key operation on 128 bit in one arithmetic operation.\n\nEdit:\nEssentially because the maximum number of items we can store in memory is tightly bound to the maximum size of the hash key to go to infinite N we need an infinitely sized word and infinite memory on the system.\n\nEdit Edit:\nkeep fucking up the 2^ sorry man", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20038, "question": "But a 256 bit hash takes linearly more operations to compute than a 64 bit.", "aSentId": 20045, "answer": "That depends on what your O(n) represents. If O(n) is the number of arithmetic operations of *any size*, then it is O(1) amortized.\n\nIf you consider O(n) to be the number of arithmetic operations of some fixed size (like in real computers), then not even addition or subtraction is O(1). A simple lookup in an array would be O(log n) just because it has to read the index which needs log n bits to cover the entire array. This isn't really a useful distinction as an array lookup will always be practically O(1).\n\nThough there are cases where arbitrary sized arithmetic isn't considered O(1).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20034, "question": "Thanks for the write up, but it still comes back to the same question: why wouldn't k need to increase logarithmically with n? And v linearly with the load factor?\n\nEdit: to clarify, I think I agree with your first paragraph, but I also think it contradicts your last paragraph since it implies k and v would not be constant in n. ", "aSentId": 20047, "answer": "I think we're close.  It's \"what does 'n' represent?\" that's really important now. \n\n*The size of the hash table is a fixed constant!*\n\n*The maximum load factor is a fixed constant!*\n\n*n is not the size of the hash table!*\n\n*n is the number of operations we're doing on the hash table* (e.g. inserts/deletes/etc)\n\nAs n increases, the load factor doesn't necessarily go up -- e.g. if we alternate between inserts and deletes.  k increases logarithmically with the size of the hash table -- but not with n!  v does increase with the load factor (*exponentially* in fact), but if we bound the maximum load factor (e.g. by requiring inserts to be followed by deletes), then the load factor is bounded by a constant and v is O(1).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20048, "question": "I think we're close.  It's \"what does 'n' represent?\" that's really important now. \n\n*The size of the hash table is a fixed constant!*\n\n*The maximum load factor is a fixed constant!*\n\n*n is not the size of the hash table!*\n\n*n is the number of operations we're doing on the hash table* (e.g. inserts/deletes/etc)\n\nAs n increases, the load factor doesn't necessarily go up -- e.g. if we alternate between inserts and deletes.  k increases logarithmically with the size of the hash table -- but not with n!  v does increase with the load factor (*exponentially* in fact), but if we bound the maximum load factor (e.g. by requiring inserts to be followed by deletes), then the load factor is bounded by a constant and v is O(1).", "aSentId": 20049, "answer": "Why would n be the number of operations rather than the elements stored in the table? For a fair comparison to other data structures, you'd need to take n to be the number of elements, not operations. Difficulty is usually expressed as a function of problem size, so it confuses things to use n to mean something else. \n\nIf you instead call the number of operations s, (and items stored, n, as the convention in every other data structure) then s insert/deletes under constant load factor would take k s v.\n\nMy original post can then be rephrased as: you can either use increasing hash functions to keep items always in separate buckets, in which k is log and v constant, or you can use the same hash function for all sizes, in which case k is constant and v is linear in n. (Why would v be exponential under either definition of n?)\n\nEither way, diving out by s doesn't get you a constant. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20050, "question": "Why would n be the number of operations rather than the elements stored in the table? For a fair comparison to other data structures, you'd need to take n to be the number of elements, not operations. Difficulty is usually expressed as a function of problem size, so it confuses things to use n to mean something else. \n\nIf you instead call the number of operations s, (and items stored, n, as the convention in every other data structure) then s insert/deletes under constant load factor would take k s v.\n\nMy original post can then be rephrased as: you can either use increasing hash functions to keep items always in separate buckets, in which k is log and v constant, or you can use the same hash function for all sizes, in which case k is constant and v is linear in n. (Why would v be exponential under either definition of n?)\n\nEither way, diving out by s doesn't get you a constant. ", "aSentId": 20051, "answer": "I think the reason that we make 'n' the number of operations is because we're doing amortized complexity.  This is how I typically see amortized complexity done for other data structures too.  For any given problem you can discuss asymptotic complexity using a variety of parameters, and it makes sense that it's O(1) with respect to one parameter and O(log x) or O(x) with respect to some other parameter x.\n\nI would also point out that almost nobody dynamically resizes hash tables or the hash function.  If you're dynamically resizing a hash table you've probably chosen the wrong data structure since that's such an expensive operation.\n\nThere are many ways to implement a hash table, and this leads to differences in v.  If you implement it with a linked list in each bucket, then I think it's linear in the load factor.  If you use linear/quadratic programing (among others), then you get the exponential relationship.  \n\nLastly, when you divide out by s you get k*v.  k is a constant, based on how I defined it.  v is not a constant, but is bounded by a constant:  if v = O(g(t)) where t is the load factor and g is any function (exponential, linear, etc.) and we bound the load factor by a constant, then t is O(1), so g(t) is O(1), so v is O(1).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20030, "question": "I still think we're talking past each other, but let me do my best be responsive. \n\nMy claim is that, regardless of assumptions you can make about the load factor, the lookup time is still an increasing function of n, where n is the number of items you wish to keep in the list. \n\nThe hard part is proving why this remains true if any one table can assume the number of inputs remains bounded for that table. And the reason is that, the larger the table you want, the more operations your hash function requires, because a larger hash output requires linearly more operations. This isn't obvious when you use a 128 but hash and can keep truncating it, but what about when you have more than 2^128 elements? If you want a 129 bit hash function, you need to perform additional operations, or you fall back to the c-items-per-bucket case. \n\nThe only way around that would be to use identity as the hash function, but then you can cheat and hide all the complexity in metadata for each key. ", "aSentId": 20053, "answer": "Another potential difference (I'm not sure?) in what we're thinking: I don't assume you can dynamically grow your hash table.  If it has 2^128 bins and you insert 2^128 + 1 elements, then I don't assume you can create a new hash bucket or change the hash function.  Instead you need to use a linked list or something else (which forces the complexity over O(1) amortized too).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20020, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 20055, "answer": "In a way you're actually right. Asymptotic analysis is not for real computers but for an abstract model where some operations are assumed to be primitive. Arithmatic operations are one such example. You don't actually report walltime but the number of times primitive operations are called. When you say O(n) you mean number of primitive operations are less than Cn + D for all n, where C and N are arbitrarily large.\n\nFurther, hash table analysis is expected time analysis, not worst case. In expected time analysis, we can always ensure that the expected length of any linked list never goes above, say, 3. You may have a point with the hashing function growing larger, but we it's relationship to n is so small that we can take it as primitive. I'm sure if you look up hash function literature they will provide precise asymptotic bounds for n. But I'm also sure they will be very small. But yes, if you decide that the hash function should not be treated as a primitive, you can proive a better estimate than O(1) for specific hash functions. (but it's not going to be worth it, imo)\n\nSpeaking of arithmatic operations, normally you take c = a + b to be an O(1) operation. But the actual time depends on the implementation. Frequently it's log(a) or log(b). But, keep in mind, you can just assume all addition operations to be O(64) in the worst case, which is just O(1), and will cover all numbers of 64 bits, which is more than the number of atoms in the universe (not really). ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20020, "question": "Why is hashtable lookup O(1) rather than O(log n) or O(n) (please read first)", "aSentId": 20057, "answer": "To be really precise with O-notation, you should always note the computational model you're using (turing machine, pointer machine, counter machine,...).\n\nIn computational models with binary numbers, we assume that operations like integer multiplication and array access are constant time for w-bit integers. Usually* it is assumed that n \u2208 O( 2^w ).\n\n(*) I've never seen this not being the case.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20059, "question": "It's not unreasonable or uncommon to neglect the asymptotic cost of dealing with big numbers. We do this all the time when indexing arrays, for instance. You could argue that traversing from one array cell to the next is not O(1) because incrementing the index becomes more expensive as the number of digits in the index grows. (That darn carry from 999...999 to 1000...000.) Ultimately, we sacrifice some pedantry for the sake of simplicity, whether it's a uniform memory model or an arbitrarily big hash function.", "aSentId": 20060, "answer": "This isn't true in the theory world!  Careful theorists will observe that the time to add X + Y is O(log X + log Y) because they need to count the number of digits!", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20059, "question": "It's not unreasonable or uncommon to neglect the asymptotic cost of dealing with big numbers. We do this all the time when indexing arrays, for instance. You could argue that traversing from one array cell to the next is not O(1) because incrementing the index becomes more expensive as the number of digits in the index grows. (That darn carry from 999...999 to 1000...000.) Ultimately, we sacrifice some pedantry for the sake of simplicity, whether it's a uniform memory model or an arbitrarily big hash function.", "aSentId": 20062, "answer": "But a) the whole point of big-O is that we don't get to place bounds on problem size, and b) for that reason I have a hard time saying that memory access is constant either!\n\nSo (and I admit this is changing the question a bit), what if I'm in contexts where I'm judged by someone by-the-book and I also don't want to say anything I consider to be wrong?\n\n- \"It's constant across real-world ranges.\" ?\n- \"Within standard memory limits, it's constant.\" ?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20063, "question": "But a) the whole point of big-O is that we don't get to place bounds on problem size, and b) for that reason I have a hard time saying that memory access is constant either!\n\nSo (and I admit this is changing the question a bit), what if I'm in contexts where I'm judged by someone by-the-book and I also don't want to say anything I consider to be wrong?\n\n- \"It's constant across real-world ranges.\" ?\n- \"Within standard memory limits, it's constant.\" ?", "aSentId": 20064, "answer": "I would say \"it takes amortized O(1) arithmetic operations and array lookups\".\n\nSimilarly, I never say \"mergesort takes O(n log n) time\", I say \"mergesort takes O(n log n) comparisons\" because you can't compare two arbitrarily large integers in constant time.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20065, "question": "I would say \"it takes amortized O(1) arithmetic operations and array lookups\".\n\nSimilarly, I never say \"mergesort takes O(n log n) time\", I say \"mergesort takes O(n log n) comparisons\" because you can't compare two arbitrarily large integers in constant time.", "aSentId": 20066, "answer": "But (for the reasons I gave in the initial post), a hashtable still requires log(n) operations for lookup since you have to perform more operations to get a bigger hash output.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20067, "question": "But (for the reasons I gave in the initial post), a hashtable still requires log(n) operations for lookup since you have to perform more operations to get a bigger hash output.", "aSentId": 20068, "answer": "I meant to treat computing k mod n as a single arithmetic operation. Sorry if I wasn't clear.\n\nSo looking up an element takes 1 operation (to compute the hash), then 1 array lookup (to lookup the entries with the computed hash).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20069, "question": "I meant to treat computing k mod n as a single arithmetic operation. Sorry if I wasn't clear.\n\nSo looking up an element takes 1 operation (to compute the hash), then 1 array lookup (to lookup the entries with the computed hash).", "aSentId": 20070, "answer": "&gt;I meant to treat computing k mod n as a single arithmetic operation. Sorry if I wasn't clear.\n\nIt doesn't matter; the point stands. Mod doesn't prevent doubling up once there are more elements than the modulus. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20071, "question": "&gt;I meant to treat computing k mod n as a single arithmetic operation. Sorry if I wasn't clear.\n\nIt doesn't matter; the point stands. Mod doesn't prevent doubling up once there are more elements than the modulus. ", "aSentId": 20072, "answer": "&gt; Mod doesn't prevent doubling up once there are more elements than the modulus.\n\nWhat does \"doubling up\" mean?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20073, "question": "&gt; Mod doesn't prevent doubling up once there are more elements than the modulus.\n\nWhat does \"doubling up\" mean?", "aSentId": 20074, "answer": "More than one item per bucket aka hash collision. Is there a reason that was hard to infer from the context?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20071, "question": "&gt;I meant to treat computing k mod n as a single arithmetic operation. Sorry if I wasn't clear.\n\nIt doesn't matter; the point stands. Mod doesn't prevent doubling up once there are more elements than the modulus. ", "aSentId": 20076, "answer": "I'm assuming you resize (increase the modulus n) as the hashtable grows so that the average number of entries with the same hash is O(1). So the analysis goes like this:\n\n- 1 arithmetic operation to compute the hash\n- 1 array lookup to find everything with the given hash, say there are p results\n- for each of the p results, compare them to the lookup key (1 arithmetic operation)\n\nsumming up, you have p + 1 expected arithmetic operations and 1 array lookup, which means O(1) arithmetic operations and array lookups - which part of the analysis is wrong?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20077, "question": "I'm assuming you resize (increase the modulus n) as the hashtable grows so that the average number of entries with the same hash is O(1). So the analysis goes like this:\n\n- 1 arithmetic operation to compute the hash\n- 1 array lookup to find everything with the given hash, say there are p results\n- for each of the p results, compare them to the lookup key (1 arithmetic operation)\n\nsumming up, you have p + 1 expected arithmetic operations and 1 array lookup, which means O(1) arithmetic operations and array lookups - which part of the analysis is wrong?", "aSentId": 20078, "answer": "The part where n becomes *greater* than the modulus -- which would be a lot easier to talk about if you didn't keep referring to the modulus as n :-P\n\nIf you keep using the same hash function, that forces you to put c elements in each bucket, where c depends linearly on n, which takes you back to O(n) time. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20079, "question": "The part where n becomes *greater* than the modulus -- which would be a lot easier to talk about if you didn't keep referring to the modulus as n :-P\n\nIf you keep using the same hash function, that forces you to put c elements in each bucket, where c depends linearly on n, which takes you back to O(n) time. ", "aSentId": 20080, "answer": "I'm not using the same hash function, I increase the number of buckets and hence increase the modulus as more elements are inserted into the hashtable.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20081, "question": "I'm not using the same hash function, I increase the number of buckets and hence increase the modulus as more elements are inserted into the hashtable.", "aSentId": 20082, "answer": "Again, what happens when your modulus is bigger than the space if hash outputs? And if you're not using the hash function, why did you write this:\n\n&gt;1 arithmetic operation to compute the hash", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20083, "question": "Again, what happens when your modulus is bigger than the space if hash outputs? And if you're not using the hash function, why did you write this:\n\n&gt;1 arithmetic operation to compute the hash", "aSentId": 20084, "answer": "&gt; Again, what happens when your modulus is bigger than the space if hash outputs? And if you're not using the hash function, why did you write this:\n\noops, edited to say \"not using the same hash function\". I don't quite understand you - so which of the three steps is inaccurate? Are you saying that p is not O(1)? \n\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20085, "question": "&gt; Again, what happens when your modulus is bigger than the space if hash outputs? And if you're not using the hash function, why did you write this:\n\noops, edited to say \"not using the same hash function\". I don't quite understand you - so which of the three steps is inaccurate? Are you saying that p is not O(1)? \n\n", "aSentId": 20086, "answer": "Again, I'm saying that the number of hash function steps is not constant with respect to n. If you want more buckets (aka larger hash space aka more possible hash outputs), you have to use a hash that requires more operations. Up to a point, you can keep the same function and just truncate it with a mod operation, but eventually that's no longer possible. \n\nIs there a reason that wasn't clear before?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20087, "question": "Again, I'm saying that the number of hash function steps is not constant with respect to n. If you want more buckets (aka larger hash space aka more possible hash outputs), you have to use a hash that requires more operations. Up to a point, you can keep the same function and just truncate it with a mod operation, but eventually that's no longer possible. \n\nIs there a reason that wasn't clear before?", "aSentId": 20088, "answer": "Why not just modulo a larger number when you need more buckets? In any case, we assume the hash function will be a reasonably simple arithmetic expression, whatever precise form it takes. \n\nIf you're saying that taking the item modulo $n$ has lower computational complexity than item modulo $2n$, then just by definition, that's not correct, at least in the context of analyzing the complexity of hashing. Of course in a real machine dividing by ever larger integers takes more time, but we simply agree to not count that when we're analyzing algorithms like hashing or sorting or array lookup. We aren't going to have hash tables for which the number of buckets requires enough bits to bother with. You're right that formally, we aren't supposed to bound $n$. Well, tough. We do. Big-O of hashing ignores it because we just defined it to ignore it. You're welcome to argue that we're wrong to ignore that, but you've found the answer to your question. We ignore it because we decided to, and because ignoring it is more informative of the actual salient issue that we're trying to address than accounting for it is. If it ever became an issue, someone would write a paper saying that \"ordinarily we say that hashing is O(1), but for very large hashes, we need to account for the complexity of the hash function itsellf, and thus conventional hashing is actually O(n). We present our super-clever way of doing it in O(log n)\" or whatever. And the field would be fine with that. \n\nIt's no contradiction for you to say hashing is O(n) and me to say it's O(1), provided we fully define what it is we're counting. I'm merely telling you that the standard way we count is to ignore the word size entirely, and the amortized cost is O(1).\n\nBig-O is a formally defined mathematical construct. However, the choice to use it to model growth of runtime of functions is arbitrary. We use it because it's a rough model that provides an easily understood description of the \"big picture\" aspects of how an algorithm behaves as a function of the size of its inputs. It's important to keep that in mind -- we use it because it works for what we want it to do. For any practical purpose, the number of bits in the modulus in a hash function is fixed. We could define everything differently. We could say that array indexing is O(log n), because it takes log(n) bits to store the index. We don't, purely because we made a pragmatic decision that doing so obscures rather than illuminates the behavior we find to be most useful to characterize.\n\nAll models are wrong; some models are useful. Temperature is a completely fake property in physics. There's no real reason why treating an entire thing as though it were one big particle with a defined energy should be better than accurately modeling all the collisions. But modeling all the collisions is hard, and anyway, it turns out that just averaging everything gives you a much better picture of the macro-level phenomenon. It's a similar thing here. We could account for the fact that the hash function consists of adds and multiplies and divides that all scale as O(n) or O(log n), but we're just back to counting individual particles for no good reason.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20089, "question": "Why not just modulo a larger number when you need more buckets? In any case, we assume the hash function will be a reasonably simple arithmetic expression, whatever precise form it takes. \n\nIf you're saying that taking the item modulo $n$ has lower computational complexity than item modulo $2n$, then just by definition, that's not correct, at least in the context of analyzing the complexity of hashing. Of course in a real machine dividing by ever larger integers takes more time, but we simply agree to not count that when we're analyzing algorithms like hashing or sorting or array lookup. We aren't going to have hash tables for which the number of buckets requires enough bits to bother with. You're right that formally, we aren't supposed to bound $n$. Well, tough. We do. Big-O of hashing ignores it because we just defined it to ignore it. You're welcome to argue that we're wrong to ignore that, but you've found the answer to your question. We ignore it because we decided to, and because ignoring it is more informative of the actual salient issue that we're trying to address than accounting for it is. If it ever became an issue, someone would write a paper saying that \"ordinarily we say that hashing is O(1), but for very large hashes, we need to account for the complexity of the hash function itsellf, and thus conventional hashing is actually O(n). We present our super-clever way of doing it in O(log n)\" or whatever. And the field would be fine with that. \n\nIt's no contradiction for you to say hashing is O(n) and me to say it's O(1), provided we fully define what it is we're counting. I'm merely telling you that the standard way we count is to ignore the word size entirely, and the amortized cost is O(1).\n\nBig-O is a formally defined mathematical construct. However, the choice to use it to model growth of runtime of functions is arbitrary. We use it because it's a rough model that provides an easily understood description of the \"big picture\" aspects of how an algorithm behaves as a function of the size of its inputs. It's important to keep that in mind -- we use it because it works for what we want it to do. For any practical purpose, the number of bits in the modulus in a hash function is fixed. We could define everything differently. We could say that array indexing is O(log n), because it takes log(n) bits to store the index. We don't, purely because we made a pragmatic decision that doing so obscures rather than illuminates the behavior we find to be most useful to characterize.\n\nAll models are wrong; some models are useful. Temperature is a completely fake property in physics. There's no real reason why treating an entire thing as though it were one big particle with a defined energy should be better than accurately modeling all the collisions. But modeling all the collisions is hard, and anyway, it turns out that just averaging everything gives you a much better picture of the macro-level phenomenon. It's a similar thing here. We could account for the fact that the hash function consists of adds and multiplies and divides that all scale as O(n) or O(log n), but we're just back to counting individual particles for no good reason.", "aSentId": 20090, "answer": "&gt;Why not just modulo a larger number when you need more buckets? \n\nBecause as n gets arbitrarily large, the modulus becomes larger than the hash function output. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20087, "question": "Again, I'm saying that the number of hash function steps is not constant with respect to n. If you want more buckets (aka larger hash space aka more possible hash outputs), you have to use a hash that requires more operations. Up to a point, you can keep the same function and just truncate it with a mod operation, but eventually that's no longer possible. \n\nIs there a reason that wasn't clear before?", "aSentId": 20092, "answer": "&gt; If you want more buckets (aka larger hash space aka more possible hash outputs), you have to use a hash that requires more operations.\n\nThe hash function I'm using is this: the hash of an integer is that integer modulo the number of buckets. This is always 1 arithmetic operation.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20093, "question": "&gt; If you want more buckets (aka larger hash space aka more possible hash outputs), you have to use a hash that requires more operations.\n\nThe hash function I'm using is this: the hash of an integer is that integer modulo the number of buckets. This is always 1 arithmetic operation.", "aSentId": 20094, "answer": "Then the items per bucket is a linear function of n and you're right back to the first case in my original post. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20095, "question": "Then the items per bucket is a linear function of n and you're right back to the first case in my original post. ", "aSentId": 20096, "answer": "No, I use case 2 in your original post - I change the number of buckets when more items are inserted.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20097, "question": "No, I use case 2 in your original post - I change the number of buckets when more items are inserted.", "aSentId": 20098, "answer": "Then (as I stated in case 2), you need to spend logarithmically more ops per hash toget the greater bucket count. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20099, "question": "Then (as I stated in case 2), you need to spend logarithmically more ops per hash toget the greater bucket count. ", "aSentId": 20100, "answer": "No you don't! I just don't understand why you think this... maybe if you can explain why you think larger hash functions take more operations, someone could do a better job of explaining why you're wrong.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20099, "question": "Then (as I stated in case 2), you need to spend logarithmically more ops per hash toget the greater bucket count. ", "aSentId": 20102, "answer": "No, my hash function always takes exactly one arithmetic operation to compute", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20103, "question": "No, my hash function always takes exactly one arithmetic operation to compute", "aSentId": 20104, "answer": "If you keep using that hash function, then you're back to the first case, with linear pileup in each bucket, and thus linear lookup (albeit with a very small coefficient).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20105, "question": "If you keep using that hash function, then you're back to the first case, with linear pileup in each bucket, and thus linear lookup (albeit with a very small coefficient).", "aSentId": 20106, "answer": "I've made it clear that the following two points are both true at the same time:\n\n- (1) I use case 2 in your analysis, ie, as the hashtable grows larger, there are more buckets and the range of the hash function increases\n- (2) The hash function always takes 1 arithmetic operation to compute\n\nYou seem to not realize that - when I use claim (2) you reply by saying that I must be using case 1 (constant number of buckets), and when I claim (1) you reply that the hash function must then take more operations to compute.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20083, "question": "Again, what happens when your modulus is bigger than the space if hash outputs? And if you're not using the hash function, why did you write this:\n\n&gt;1 arithmetic operation to compute the hash", "aSentId": 20108, "answer": "You need to implement a hash function to keep increasing n and have more buckets or it all goes out the window and it's not O (1) but that's due to shitty implementation in that case", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20067, "question": "But (for the reasons I gave in the initial post), a hashtable still requires log(n) operations for lookup since you have to perform more operations to get a bigger hash output.", "aSentId": 20110, "answer": "No you don't, you perform the same (constant) number of arithmetic operations on bigger numbers.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20111, "question": "No you don't, you perform the same (constant) number of arithmetic operations on bigger numbers.", "aSentId": 20112, "answer": "This is exactly what my original post attempted to address. As n gets arbitrarily large, and the hash output size k doesn't increase, you eventually swamp the hashspace and revert to the case crawling through a bucket with c\\*n elements. That's O(n) and what I explained in case 1. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20063, "question": "But a) the whole point of big-O is that we don't get to place bounds on problem size, and b) for that reason I have a hard time saying that memory access is constant either!\n\nSo (and I admit this is changing the question a bit), what if I'm in contexts where I'm judged by someone by-the-book and I also don't want to say anything I consider to be wrong?\n\n- \"It's constant across real-world ranges.\" ?\n- \"Within standard memory limits, it's constant.\" ?", "aSentId": 20114, "answer": "\"This is O(1) presuming the cost of the hash function does not materially depend on n.\"\n\nYou do get to put practical limits on things. All the time. You have to.  And, more importantly, these sorts of limits are part of the language that people use to communicate ideas.  By hauling off and redefining the constraints for O(1) for yourself, you make it harder to actually communicate.  (Also, it makes you look like a bit of a twat).\n\nIf you're feeling very strongly about it, your best bet is not to try changing people's minds by fiat, but rather to do some research and prove that the effect should be of concern.  It is very well possible that e.g. memory should not be considered O(1) access time. But do a literature study first, and then do some research before spending time trying to convince people e.g. that the technically non constant big-O of ADD(a,b) is terribly important to how they design their sort algorithm.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20115, "question": "\"This is O(1) presuming the cost of the hash function does not materially depend on n.\"\n\nYou do get to put practical limits on things. All the time. You have to.  And, more importantly, these sorts of limits are part of the language that people use to communicate ideas.  By hauling off and redefining the constraints for O(1) for yourself, you make it harder to actually communicate.  (Also, it makes you look like a bit of a twat).\n\nIf you're feeling very strongly about it, your best bet is not to try changing people's minds by fiat, but rather to do some research and prove that the effect should be of concern.  It is very well possible that e.g. memory should not be considered O(1) access time. But do a literature study first, and then do some research before spending time trying to convince people e.g. that the technically non constant big-O of ADD(a,b) is terribly important to how they design their sort algorithm.", "aSentId": 20116, "answer": "What research would I be doing?   You agree that I'm correct about the asymptotic properties, which is all I was disputing, and which is what big-O is referring to.\n\n&gt;\"This is O(1) presuming the cost of the hash function does not materially depend on n.\"\n\n... but it has to, for the reasons I gave above.\n\n&gt; By hauling off and redefining the constraints for O(1) for yourself, you make it harder to actually communicate.\n\nThe O(1) result is what requires redefining the standard meaning of words, and the collective agreement to make this arbitrary exception, IMHO, is what makes communication difficult.  Why do we get to assume a bound on input size in this case but not others?\n\nEdit: since when does calling me a twat for a legitimate question earn you a higher rating?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20117, "question": "What research would I be doing?   You agree that I'm correct about the asymptotic properties, which is all I was disputing, and which is what big-O is referring to.\n\n&gt;\"This is O(1) presuming the cost of the hash function does not materially depend on n.\"\n\n... but it has to, for the reasons I gave above.\n\n&gt; By hauling off and redefining the constraints for O(1) for yourself, you make it harder to actually communicate.\n\nThe O(1) result is what requires redefining the standard meaning of words, and the collective agreement to make this arbitrary exception, IMHO, is what makes communication difficult.  Why do we get to assume a bound on input size in this case but not others?\n\nEdit: since when does calling me a twat for a legitimate question earn you a higher rating?", "aSentId": 20118, "answer": "He wasn't calling you a twat. He was saying that people who try to redefine the constraints accepted in academic discourse for complexity analysis can sound like twats. \n\nI think part of this is the sentiment, \"oh no, not this again.\" The argument you are making is well-known. Nearly every computer science PhD in the country knows exactly what you are describing, and have probably heard this idea floated by undergrad after undergrad who has newly learned about Big-O. But they have settled on using the notion that they do because it's the one which is most useful for applications. Is this a triumph of application over purity? Perhaps. But computer science has never been as disconnected from engineering as pure math.\n\nAt the ultimate limit, everything breaks down. The runtime of a hash table with 5\\^\\^\\^5 elements may as well be undefined, because there aren't enough particles in the universe to store it. By the time we have computers with enough storage for the compute time of the hash function to be a major concern for lookup time, it could easily be that we have 128-bit architectures, and we are back to constant complexity because it takes as much time to add 1+1 as it does to add 2^126 + 2^126 if you are working with long long long ints.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20119, "question": "He wasn't calling you a twat. He was saying that people who try to redefine the constraints accepted in academic discourse for complexity analysis can sound like twats. \n\nI think part of this is the sentiment, \"oh no, not this again.\" The argument you are making is well-known. Nearly every computer science PhD in the country knows exactly what you are describing, and have probably heard this idea floated by undergrad after undergrad who has newly learned about Big-O. But they have settled on using the notion that they do because it's the one which is most useful for applications. Is this a triumph of application over purity? Perhaps. But computer science has never been as disconnected from engineering as pure math.\n\nAt the ultimate limit, everything breaks down. The runtime of a hash table with 5\\^\\^\\^5 elements may as well be undefined, because there aren't enough particles in the universe to store it. By the time we have computers with enough storage for the compute time of the hash function to be a major concern for lookup time, it could easily be that we have 128-bit architectures, and we are back to constant complexity because it takes as much time to add 1+1 as it does to add 2^126 + 2^126 if you are working with long long long ints.", "aSentId": 20120, "answer": "&gt;He wasn't calling you a twat. He was saying that people who try to redefine the constraints accepted in academic discourse for complexity analysis can sound like twats.\n\n... which would be calling me a twat when combined with his and your insistence that I'm doing that :-P\n\nBut then, my point is that I'm *not* redefining anything but applying what I thought were the definitions. \n\n&gt;I think part of this is the sentiment, \"oh no, not this again.\" The argument you are making is well-known. Nearly every computer science PhD in the country knows exactly what you are describing, and have probably heard this idea floated by undergrad after undergrad who has newly learned about Big-O\n\nThen why aren't there pages that address this specific point? Or which ones did I miss?\n\n&gt;At the ultimate limit, everything breaks down. The runtime of a hash table with 5^^^5 elements may as well be undefine\n\nThere's a big difference between the practical limits of universe size  vs bounding n under a definition that requires n be unbounded :-P", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20121, "question": "&gt;He wasn't calling you a twat. He was saying that people who try to redefine the constraints accepted in academic discourse for complexity analysis can sound like twats.\n\n... which would be calling me a twat when combined with his and your insistence that I'm doing that :-P\n\nBut then, my point is that I'm *not* redefining anything but applying what I thought were the definitions. \n\n&gt;I think part of this is the sentiment, \"oh no, not this again.\" The argument you are making is well-known. Nearly every computer science PhD in the country knows exactly what you are describing, and have probably heard this idea floated by undergrad after undergrad who has newly learned about Big-O\n\nThen why aren't there pages that address this specific point? Or which ones did I miss?\n\n&gt;At the ultimate limit, everything breaks down. The runtime of a hash table with 5^^^5 elements may as well be undefine\n\nThere's a big difference between the practical limits of universe size  vs bounding n under a definition that requires n be unbounded :-P", "aSentId": 20122, "answer": "Well, here is a professor discussing this issue along the same lines as you: http://lemire.me/blog/archives/2009/08/18/do-hash-tables-work-in-constant-time/\n\nNotice the meta-assumption that assumptions are crucial to performing any sort of complexity analysis.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20121, "question": "&gt;He wasn't calling you a twat. He was saying that people who try to redefine the constraints accepted in academic discourse for complexity analysis can sound like twats.\n\n... which would be calling me a twat when combined with his and your insistence that I'm doing that :-P\n\nBut then, my point is that I'm *not* redefining anything but applying what I thought were the definitions. \n\n&gt;I think part of this is the sentiment, \"oh no, not this again.\" The argument you are making is well-known. Nearly every computer science PhD in the country knows exactly what you are describing, and have probably heard this idea floated by undergrad after undergrad who has newly learned about Big-O\n\nThen why aren't there pages that address this specific point? Or which ones did I miss?\n\n&gt;At the ultimate limit, everything breaks down. The runtime of a hash table with 5^^^5 elements may as well be undefine\n\nThere's a big difference between the practical limits of universe size  vs bounding n under a definition that requires n be unbounded :-P", "aSentId": 20124, "answer": "&gt; &gt;    He wasn't calling you a twat. He was saying that people who try to redefine the constraints accepted in academic discourse for complexity analysis can sound like twats.\n&gt; \n&gt; ... which would be calling me a twat when combined with his and your insistence that I'm doing that :-P\n\n[/me intervenes before this thread descends further into the missed subtlety of meaning.]\n\nContrary to the insistance of some, sounding like a duck does not make you a duck. It just means that you should be willing for some type systems to treat you like a duck.\n\nTo be less obtuse: you don't have to *be* a twat for someone to think that you're *sounding* like one.\n\nTo return to the analogy, if you quack and someone says to you \"hmm, you sound like a duck\", it doesn't mean that that person is calling you a duck but you should be prepared for their Python program to assume you can swim.\n\n**Edit:**\n\n&gt;Then why aren't there pages that address this specific point? Or which ones did I miss?\n\nAny good text book on algorithmic complexity should discuss \"primitive operations\" early on. A \"primitive operation\" being some operation we define to have constant unit cost. When comparing algorithms X and Y, one usually states which operations one treat as primitive or rely on convention. How does one determine convention? One reads a lot since convention may change over time in response to practicalities in the subject.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20125, "question": "&gt; &gt;    He wasn't calling you a twat. He was saying that people who try to redefine the constraints accepted in academic discourse for complexity analysis can sound like twats.\n&gt; \n&gt; ... which would be calling me a twat when combined with his and your insistence that I'm doing that :-P\n\n[/me intervenes before this thread descends further into the missed subtlety of meaning.]\n\nContrary to the insistance of some, sounding like a duck does not make you a duck. It just means that you should be willing for some type systems to treat you like a duck.\n\nTo be less obtuse: you don't have to *be* a twat for someone to think that you're *sounding* like one.\n\nTo return to the analogy, if you quack and someone says to you \"hmm, you sound like a duck\", it doesn't mean that that person is calling you a duck but you should be prepared for their Python program to assume you can swim.\n\n**Edit:**\n\n&gt;Then why aren't there pages that address this specific point? Or which ones did I miss?\n\nAny good text book on algorithmic complexity should discuss \"primitive operations\" early on. A \"primitive operation\" being some operation we define to have constant unit cost. When comparing algorithms X and Y, one usually states which operations one treat as primitive or rely on convention. How does one determine convention? One reads a lot since convention may change over time in response to practicalities in the subject.", "aSentId": 20126, "answer": "And that still wouldn't adress this point: as I said in the original post, hash functions with more bits in the output require more primitive operations. \n\nIn any case, if you don't think I'm actually being a twat for asking this, then I'd appreciate some help with the mass downvoting. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20127, "question": "And that still wouldn't adress this point: as I said in the original post, hash functions with more bits in the output require more primitive operations. \n\nIn any case, if you don't think I'm actually being a twat for asking this, then I'd appreciate some help with the mass downvoting. ", "aSentId": 20128, "answer": "&gt; And that still wouldn't adress this point: as I said in the original post, hash functions with more bits in the output require more primitive operations. \n\n... unless computing a hash is considered a primitive operation. Which it probably is in the context of hash tables as a data stucture. Otherwise one could say \"I've invented a O(2^n ) hash function so hash tables must be O(2^n ).\" (I'm reducing to the absurd there but I hope you see the point.)\n\n&gt; In any case, if you don't think I'm actually being a twat for asking this, then I'd appreciate some help with the mass downvoting. \n\nI don't think you're being a twat in asking what assumptions are held in different contexts within complexity analysis; questioning implicit assumptions is an important part of learning. But you're coming across as a twat in the comments. You may be genuinely trying to understand but, as a jaded teacher, you do sound a lot like one of *those* undergrads. Is that fair? Possibly not. Is it a cause for downvotes? Probably yes.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20129, "question": "&gt; And that still wouldn't adress this point: as I said in the original post, hash functions with more bits in the output require more primitive operations. \n\n... unless computing a hash is considered a primitive operation. Which it probably is in the context of hash tables as a data stucture. Otherwise one could say \"I've invented a O(2^n ) hash function so hash tables must be O(2^n ).\" (I'm reducing to the absurd there but I hope you see the point.)\n\n&gt; In any case, if you don't think I'm actually being a twat for asking this, then I'd appreciate some help with the mass downvoting. \n\nI don't think you're being a twat in asking what assumptions are held in different contexts within complexity analysis; questioning implicit assumptions is an important part of learning. But you're coming across as a twat in the comments. You may be genuinely trying to understand but, as a jaded teacher, you do sound a lot like one of *those* undergrads. Is that fair? Possibly not. Is it a cause for downvotes? Probably yes.", "aSentId": 20130, "answer": "&gt; Which it probably is in the context of hash tables as a data stucture. Otherwise one could say \"I've invented a O(2n ) hash function so hash tables must be O(2n ).\" (I'm reducing to the absurd there but I hope you see the point.)\n\nWhich is why I addressed that attempted reduction, where it was brought up, by saying that both hash functions must satisfy the same well-mixedness criteria. \n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20131, "question": "&gt; Which it probably is in the context of hash tables as a data stucture. Otherwise one could say \"I've invented a O(2n ) hash function so hash tables must be O(2n ).\" (I'm reducing to the absurd there but I hope you see the point.)\n\nWhich is why I addressed that attempted reduction, where it was brought up, by saying that both hash functions must satisfy the same well-mixedness criteria. \n", "aSentId": 20132, "answer": "Then I counter by proposing a hash function which distributes the *n* bits of input in parallel to the Oracle who responds in constant time by setting in parallel *k* bits of output in the countably infinitely sized register used by my machine's architecture.\n\n**Edit:** the idea being that it's *not useful* to consider the cost of computing a hash function or dealing with its output *in this context* even though it may be both *useful* and *correct* in another. If it will /thread, then you are *correct* if one assumes that there is O(log *n*) cost in dealing with values up to *n* and it is *useful* to talk about this when, for example, deciding whether your hash table is appropriate for a *n*-bit architecture **but** in the context in which hash tables are usually discussed it is simply not useful to consider this.\n\nStubbornly refusing to accept that \"correct\" is contextual and that your context is uncommon is one of the more annoying aspects of your replies.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20133, "question": "Then I counter by proposing a hash function which distributes the *n* bits of input in parallel to the Oracle who responds in constant time by setting in parallel *k* bits of output in the countably infinitely sized register used by my machine's architecture.\n\n**Edit:** the idea being that it's *not useful* to consider the cost of computing a hash function or dealing with its output *in this context* even though it may be both *useful* and *correct* in another. If it will /thread, then you are *correct* if one assumes that there is O(log *n*) cost in dealing with values up to *n* and it is *useful* to talk about this when, for example, deciding whether your hash table is appropriate for a *n*-bit architecture **but** in the context in which hash tables are usually discussed it is simply not useful to consider this.\n\nStubbornly refusing to accept that \"correct\" is contextual and that your context is uncommon is one of the more annoying aspects of your replies.", "aSentId": 20134, "answer": "Stubbornly refusing to accept that one big-O trivium is based on different assumptions from the rest is one of the more annoying aspects of trying to get a straight answer about this :-P ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20063, "question": "But a) the whole point of big-O is that we don't get to place bounds on problem size, and b) for that reason I have a hard time saying that memory access is constant either!\n\nSo (and I admit this is changing the question a bit), what if I'm in contexts where I'm judged by someone by-the-book and I also don't want to say anything I consider to be wrong?\n\n- \"It's constant across real-world ranges.\" ?\n- \"Within standard memory limits, it's constant.\" ?", "aSentId": 20136, "answer": "You don't get to place bounds on problem size, correct. But you DO get to decide what you want to count. For example, when it's stated that sorting is O(n lg n), what's being counted is the number of comparison operations. Any formal time complexity analysis will state clearly what is being counted.\n\nRemember that big-O analysis always takes place in the context of a function. The output of the function is what is being counted.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20138, "question": "The problem is that you think constant-time RAM access or hash computation implies a restriction on our problem size. This simply need not be true. We can make whatever laws we like for our abstract models.\n\nOf course a *real* machine would have issues accessing an unbounded amount of memory. But then a real machine can't adjust its memory to suit an arbitrarily large input either.", "aSentId": 20139, "answer": "But you can't have it both ways -- the O(1) lookup result seems to be saying we can rely on real-world constraints for ignoring the behavior for &gt; 2^64 elements, but we're just talking about an abstract machine when it comes to the number of ops in a hash.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20140, "question": "But you can't have it both ways -- the O(1) lookup result seems to be saying we can rely on real-world constraints for ignoring the behavior for &gt; 2^64 elements, but we're just talking about an abstract machine when it comes to the number of ops in a hash.", "aSentId": 20141, "answer": "The model is simple: A hash function takes an arbitrary input to an output of any desired size in constant time. Given that hashing is constant time (by definition / using an oracle / a wizard did it), we can show those other asymptotic properties of hash tables.\n\nNowhere did I say we are relying on real-world constraints. When we're proving things about big-O for algorithms, we're talking about abstract models, not the real world.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20142, "question": "The model is simple: A hash function takes an arbitrary input to an output of any desired size in constant time. Given that hashing is constant time (by definition / using an oracle / a wizard did it), we can show those other asymptotic properties of hash tables.\n\nNowhere did I say we are relying on real-world constraints. When we're proving things about big-O for algorithms, we're talking about abstract models, not the real world.", "aSentId": 20143, "answer": "&gt;The model is simple: A hash function takes an arbitrary input to an output of any desired size in constant time. Given that hashing is constant time (by definition / using an oracle / a wizard did it), we can show those other asymptotic properties of hash tables.\n\nFine, but then it's \"constant lookup because we say so\", not \"constant lookup because you can rigorously derive that asymptotic behavior\".\n\n&gt; Nowhere did I say we are relying on real-world constraints\n\nSorry --everyone else in the discussion was justifying it that way and calling me an idiot for not playing along, so I just assumed :-P", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20144, "question": "&gt;The model is simple: A hash function takes an arbitrary input to an output of any desired size in constant time. Given that hashing is constant time (by definition / using an oracle / a wizard did it), we can show those other asymptotic properties of hash tables.\n\nFine, but then it's \"constant lookup because we say so\", not \"constant lookup because you can rigorously derive that asymptotic behavior\".\n\n&gt; Nowhere did I say we are relying on real-world constraints\n\nSorry --everyone else in the discussion was justifying it that way and calling me an idiot for not playing along, so I just assumed :-P", "aSentId": 20145, "answer": "Well, here's the thing, if we adopt your approach, we'd have to claim that with _N_ elements, not just hashing, but _comparison_ should take about O(log _N_) time.  Thus, if you want to say that hash tables are O(log N) expected amortized time, then trees would be O((log _N_)^(2)) time.\n\nAnd likewise, for sorting _N_ distinct elements, they must be at least O(log _N_) in size (even if we use pointers, the pointer must be O(log _N_) in size), so swapping takes O(log _N_) time, and thus sorting becomes O(_N_ (log _N_)^(2)).\n\nPretty much ***no one does this.***  Instead we count comparisons, swaps, arithmetic ops, etc.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20146, "question": "Well, here's the thing, if we adopt your approach, we'd have to claim that with _N_ elements, not just hashing, but _comparison_ should take about O(log _N_) time.  Thus, if you want to say that hash tables are O(log N) expected amortized time, then trees would be O((log _N_)^(2)) time.\n\nAnd likewise, for sorting _N_ distinct elements, they must be at least O(log _N_) in size (even if we use pointers, the pointer must be O(log _N_) in size), so swapping takes O(log _N_) time, and thus sorting becomes O(_N_ (log _N_)^(2)).\n\nPretty much ***no one does this.***  Instead we count comparisons, swaps, arithmetic ops, etc.", "aSentId": 20147, "answer": "I can bound the size of the elements (thereby getting constant comparison time) without bounding the *number* of elements.  There's no such distinction for the hashtable, where increasing the number of elements requires computing bigger hashes with (linear) more operations (in the length of the output).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20148, "question": "I can bound the size of the elements (thereby getting constant comparison time) without bounding the *number* of elements.  There's no such distinction for the hashtable, where increasing the number of elements requires computing bigger hashes with (linear) more operations (in the length of the output).", "aSentId": 20149, "answer": "&gt; I can bound the size of the elements\n\nNot unless you're going to disallow people from having _distinct_ elements. If the elements are distinct and there are _N_ of them, they must be at least O(log _N_) in size.\n\n(And if your elements are less than O(log _N_) in size, you don't need a hash table at all. You can just use an array and use the bit representation as the index.  You can also sort in O(_N_) using radix sorting.)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20148, "question": "I can bound the size of the elements (thereby getting constant comparison time) without bounding the *number* of elements.  There's no such distinction for the hashtable, where increasing the number of elements requires computing bigger hashes with (linear) more operations (in the length of the output).", "aSentId": 20151, "answer": "This is not accurate but has been explained by another poster elsewhere. If you would like me to clarify I can.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20144, "question": "&gt;The model is simple: A hash function takes an arbitrary input to an output of any desired size in constant time. Given that hashing is constant time (by definition / using an oracle / a wizard did it), we can show those other asymptotic properties of hash tables.\n\nFine, but then it's \"constant lookup because we say so\", not \"constant lookup because you can rigorously derive that asymptotic behavior\".\n\n&gt; Nowhere did I say we are relying on real-world constraints\n\nSorry --everyone else in the discussion was justifying it that way and calling me an idiot for not playing along, so I just assumed :-P", "aSentId": 20153, "answer": "&gt; Fine, but then it's \"constant lookup because we say so\",\n\nYes.\n\n&gt; not \"constant lookup because you can rigorously derive that asymptotic behavior\".\n\nIt's also rigorously (and trivially) derived from the above assumption.\n\nThe same problem you have with non-constant-time hash functions you're also going to have with indexing and other addressing computations. Under your model you will have more difficulty describing the behavior of other algorithms and data structures besides hash tables. In the end, is the added precision useful? Will it change how you develop algorithms and write code? Or is the precision lost by the uniform model an acceptable trade-off for its ease of use and relevance to the real world? (Note that the real world may justify our design decisions for an abstract model, but has no bearing on rigorous proofs within the model.)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20154, "question": "&gt; Fine, but then it's \"constant lookup because we say so\",\n\nYes.\n\n&gt; not \"constant lookup because you can rigorously derive that asymptotic behavior\".\n\nIt's also rigorously (and trivially) derived from the above assumption.\n\nThe same problem you have with non-constant-time hash functions you're also going to have with indexing and other addressing computations. Under your model you will have more difficulty describing the behavior of other algorithms and data structures besides hash tables. In the end, is the added precision useful? Will it change how you develop algorithms and write code? Or is the precision lost by the uniform model an acceptable trade-off for its ease of use and relevance to the real world? (Note that the real world may justify our design decisions for an abstract model, but has no bearing on rigorous proofs within the model.)", "aSentId": 20155, "answer": "&gt; It's also rigorously (and trivially) derived from the above assumption.\n\nNo -- hashtable lookups can still be linear even assuming a constant hash computation because the elements per bucket pile up.\n\n&gt;In the end, is the added precision useful?\n\nIf it allows a single (scientific) model to derive equally-useful results, rather than a minefield of special-case gentleman's-agreements ... yes.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20156, "question": "&gt; It's also rigorously (and trivially) derived from the above assumption.\n\nNo -- hashtable lookups can still be linear even assuming a constant hash computation because the elements per bucket pile up.\n\n&gt;In the end, is the added precision useful?\n\nIf it allows a single (scientific) model to derive equally-useful results, rather than a minefield of special-case gentleman's-agreements ... yes.", "aSentId": 20157, "answer": "&gt; No -- hashtable lookups can still be linear even assuming a constant hash computation because the elements per bucket pile up.\n\nSure, yeah. I wasn't sure if we were assuming perfect hashing or talking about average case or what.\n\n&gt; If it allows a single (scientific) model to derive equally-useful results, rather than a minefield of special-case gentleman's-agreements ... yes.\n\nThat model might come in handy for complexity research and Turing machines, but the average hash table user wants their average O(1) complexity bound, the n = 2^64 case be damned.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20158, "question": "&gt; No -- hashtable lookups can still be linear even assuming a constant hash computation because the elements per bucket pile up.\n\nSure, yeah. I wasn't sure if we were assuming perfect hashing or talking about average case or what.\n\n&gt; If it allows a single (scientific) model to derive equally-useful results, rather than a minefield of special-case gentleman's-agreements ... yes.\n\nThat model might come in handy for complexity research and Turing machines, but the average hash table user wants their average O(1) complexity bound, the n = 2^64 case be damned.", "aSentId": 20159, "answer": "&gt; Sure, yeah. I wasn't sure if we were assuming perfect hashing or talking about average case or what.\n\nDoesn't matter; it's still linear.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20161, "question": "Remember that big O has no implicit unit.  It's not (necessarily) CPU cycles or clock ticks. When talking about sorting, the unit is usually comparisons, and we ignore the complexity of a comparison.  In hashing, it's hash function calls.\n\nThis works because big O is used as a shorthand to compare the relative efficiencies of algorithms.  Resolving collisions in hashing, for example--both/all of the algorithms will use the same hash function, so we factor that out and just count hashes.", "aSentId": 20162, "answer": "But if you use the same hash function throughout, you still have to soft through a linear number of collisions. Still not constant ops per lookup. (I already was aware of the points in your first paragraph and they don't affect my point.)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20163, "question": "But if you use the same hash function throughout, you still have to soft through a linear number of collisions. Still not constant ops per lookup. (I already was aware of the points in your first paragraph and they don't affect my point.)", "aSentId": 20164, "answer": "Now you're talking about a specific hash collision algorithm, but I'm not sure exactly what it is.  It's certainly possible to make hashing worse than O(1) with a poor collision algorithn. (Always chaining and never rehashing, for example, gives you O(n) on insert!)\n\nIf that's what you're getting at then, yes, hashing is only O(1) if you resolve conflicts efficiently.\n\nYou could resolve conflicts by always growing and rehashing.  This means each time you run into a conflict you execute O(n) hashes for the n items.  So overall your insertion is bounded by O(c(n) * n) where c(n) is the number of collisions for the set.  In practice, c(n) is very small relative to n, so is effectively a constant.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20165, "question": "Now you're talking about a specific hash collision algorithm, but I'm not sure exactly what it is.  It's certainly possible to make hashing worse than O(1) with a poor collision algorithn. (Always chaining and never rehashing, for example, gives you O(n) on insert!)\n\nIf that's what you're getting at then, yes, hashing is only O(1) if you resolve conflicts efficiently.\n\nYou could resolve conflicts by always growing and rehashing.  This means each time you run into a conflict you execute O(n) hashes for the n items.  So overall your insertion is bounded by O(c(n) * n) where c(n) is the number of collisions for the set.  In practice, c(n) is very small relative to n, so is effectively a constant.", "aSentId": 20166, "answer": "&gt; Now you're talking about a specific hash collision algorithm, but I'm not sure exactly what it is. It's certainly possible to make hashing worse than O(1) with a poor collision algorithn. (Always chaining and never rehashing, for example, gives you O(n) on insert!)\n\nNo -- with a perfect hash function (per the first case in my origin post) and arbitrarily large n, you have a linear number of objects per bucket.  Specifically (by counting considerations) there are ( 1/2^k )n objects per bucket, where k is the size of the hash output.  Still linear.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20167, "question": "&gt; Now you're talking about a specific hash collision algorithm, but I'm not sure exactly what it is. It's certainly possible to make hashing worse than O(1) with a poor collision algorithn. (Always chaining and never rehashing, for example, gives you O(n) on insert!)\n\nNo -- with a perfect hash function (per the first case in my origin post) and arbitrarily large n, you have a linear number of objects per bucket.  Specifically (by counting considerations) there are ( 1/2^k )n objects per bucket, where k is the size of the hash output.  Still linear.", "aSentId": 20168, "answer": "You say no but the rest of your comment agrees with what I wrote ... looks like miscommunication?\n\nOh, I think I see your concern: you're supposing a hash function that has fewer unique 'keys' than n.  In practice that's easy to avoid: a 1024 bit hash key will let you hash every atom in the universe without collisions, so there's a constant upper bound. :-)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20169, "question": "You say no but the rest of your comment agrees with what I wrote ... looks like miscommunication?\n\nOh, I think I see your concern: you're supposing a hash function that has fewer unique 'keys' than n.  In practice that's easy to avoid: a 1024 bit hash key will let you hash every atom in the universe without collisions, so there's a constant upper bound. :-)", "aSentId": 20170, "answer": "Then I don't know what your position is.  If there are k bits in the hash, and you have more than 2^k elements, lookup is linear.  Do you disagree?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20171, "question": "Then I don't know what your position is.  If there are k bits in the hash, and you have more than 2^k elements, lookup is linear.  Do you disagree?", "aSentId": 20172, "answer": "No, of course you're right.  And I said as much too -- but again it's easy to choose a 'k' that's large enough to effectively be a constant for any meaningful comparison of hash algorithms.  As others have posted on the thread, we rarely worry about exceeding the size of the keys/numbers in the algorithms, and making such an issue of this is a bit ... pedantic?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20173, "question": "No, of course you're right.  And I said as much too -- but again it's easy to choose a 'k' that's large enough to effectively be a constant for any meaningful comparison of hash algorithms.  As others have posted on the thread, we rarely worry about exceeding the size of the keys/numbers in the algorithms, and making such an issue of this is a bit ... pedantic?", "aSentId": 20174, "answer": "Hm, I always get lectures about computer science being all theory \"and has nothing to do with programming\", and yet when I take big-O to be it's pure theoretical meaning where you don't get to assume bounds on n, then I kinda feel let down...\n\nSeriously though, when the derivation of one big-O bound is completely different from the others, and by gentleman's agreement, I want to confirm that that's the case and I'm not just going crazy.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20175, "question": "Hm, I always get lectures about computer science being all theory \"and has nothing to do with programming\", and yet when I take big-O to be it's pure theoretical meaning where you don't get to assume bounds on n, then I kinda feel let down...\n\nSeriously though, when the derivation of one big-O bound is completely different from the others, and by gentleman's agreement, I want to confirm that that's the case and I'm not just going crazy.", "aSentId": 20176, "answer": "I think you're missing a certain sense of perspective.  A few posts ago I waved away your 2^k element concern by pointing out that it's very easy to set a fixed upper bound for k, and used atoms in the universe as an example, but that didn't phase you a bit.  (I was very much over-engineering the problem too: 2^1024 is a couple of hundred *orders of magnitude* larger than the number of atoms in the observable universe!)\n\nSuggest you reflect on the minute size of the hairs you're splitting, compared to how coarse and loosely defined big O is in practice, and ask yourself if you're really using your brain to think about the most important thing here.  ...cause there really are a ton of much more interesting problems to worry about than this one.\n\nBig O is a great tool for comparing one algorithm to another when both alternatives are analyzed with the same scoping and with the same definition of an operation.  But it's not a benchmark, and there's no absolute right answer for scoping an operation.  You can get 99.99% of the utility of a big O analysis without worrying about things like number size.  You're right that this isn't *precisely* correct, but more often than not it's more than good enough to pick an algorithm, and then other factors like complexity of implementation or data storage size will matter more.\n\nLearning to pick the right tools for the task at hand is probably more important than refining the big O tool to this degree.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20059, "question": "It's not unreasonable or uncommon to neglect the asymptotic cost of dealing with big numbers. We do this all the time when indexing arrays, for instance. You could argue that traversing from one array cell to the next is not O(1) because incrementing the index becomes more expensive as the number of digits in the index grows. (That darn carry from 999...999 to 1000...000.) Ultimately, we sacrifice some pedantry for the sake of simplicity, whether it's a uniform memory model or an arbitrarily big hash function.", "aSentId": 20178, "answer": "This is called the Word RAM model, and yes it's a bit of a stretch in pure physical terms.  \n\nObviously O(1) array access is one result, but it's also possible to get other nominally log n operations to work in O(1).  The fact that one can operate on log n bits in one step opens a lot of possibilities.  ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20180, "question": "Dude. Why is even array access considered `O(1)`? At least within our universe, as long as we don't find anything faster than light to use as a signal, it doesn't matter how efficiently we pack data on a 3d volume. It will still take at least `O(n^(1/3))` time for the signal to propagate from a point to another.", "aSentId": 20181, "answer": "When you talk about Big O running times, you have to say what the n represents. In your example, I'm assuming the n is the volume of the data storage, but that's not really a useful n to talk about since it a) doesn't vary and b) isn't an input to the function. \n\nIn many algorithms, the n is the length of the list and the `O()` represents number of comparisons, which is why array access is `O(1)`. You don't need to make any comparisons. That doesn't mean that it's instantaneous, but it does mean that it's not going to change with the number of elements in the array. Even if the array is so big that it takes longer to index into random parts due to physical drive constraints, it still doesn't take any comparisons.  \n\nIf you're talking about factoring or determining if a number is prime, the n is going to be the number of digits, not the size of the number. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20182, "question": "When you talk about Big O running times, you have to say what the n represents. In your example, I'm assuming the n is the volume of the data storage, but that's not really a useful n to talk about since it a) doesn't vary and b) isn't an input to the function. \n\nIn many algorithms, the n is the length of the list and the `O()` represents number of comparisons, which is why array access is `O(1)`. You don't need to make any comparisons. That doesn't mean that it's instantaneous, but it does mean that it's not going to change with the number of elements in the array. Even if the array is so big that it takes longer to index into random parts due to physical drive constraints, it still doesn't take any comparisons.  \n\nIf you're talking about factoring or determining if a number is prime, the n is going to be the number of digits, not the size of the number. ", "aSentId": 20183, "answer": "No. That is the point: array access time **increases** linearly in function of the size of the array. It is a physical limitation. The only reason we say it is O(1) is an engineering accident about how our computers are made: we have a very low bound on the maximum length of the array (the amount of memory on a system) and we purposely make every random access take the worst possible time by synchronizing operations in clocks. So, yea, practically it is O(1), but that is as true as saying that `sort` is O(1) because we have specialized hardware with a `sort` primitive operation. Yea, on that computer it is - but that just means such machine has a size limit and doesn't change the fact sorting arbitrary lists isn't possible in constant time. \n\nMany aren't interested in the practical complexity \"given our current computer architecture\", since that can change. We are worried about the complexity of an algorithm given the physical limitations of our universe. When analysing the true complexity of an algorithm you must take in account the storage medium and the host universe. Or else you could very well say that we live in an infini-dimensional universe and suddenly radix tree insertion becomes O(1). Or in an universe without speed limits and then **everything** is O(1).\n\nThe true complexity of array access is `O(n^(1/3))` in our universe.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20184, "question": "No. That is the point: array access time **increases** linearly in function of the size of the array. It is a physical limitation. The only reason we say it is O(1) is an engineering accident about how our computers are made: we have a very low bound on the maximum length of the array (the amount of memory on a system) and we purposely make every random access take the worst possible time by synchronizing operations in clocks. So, yea, practically it is O(1), but that is as true as saying that `sort` is O(1) because we have specialized hardware with a `sort` primitive operation. Yea, on that computer it is - but that just means such machine has a size limit and doesn't change the fact sorting arbitrary lists isn't possible in constant time. \n\nMany aren't interested in the practical complexity \"given our current computer architecture\", since that can change. We are worried about the complexity of an algorithm given the physical limitations of our universe. When analysing the true complexity of an algorithm you must take in account the storage medium and the host universe. Or else you could very well say that we live in an infini-dimensional universe and suddenly radix tree insertion becomes O(1). Or in an universe without speed limits and then **everything** is O(1).\n\nThe true complexity of array access is `O(n^(1/3))` in our universe.", "aSentId": 20185, "answer": "This is a really interesting point. Memory is typically the bottleneck in large scale simulations and it is probably because we assume it's constant and don't deal with the problem  algorithmically. \n\nStill, how do you come up with O(n^(1/3))? ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20186, "question": "This is a really interesting point. Memory is typically the bottleneck in large scale simulations and it is probably because we assume it's constant and don't deal with the problem  algorithmically. \n\nStill, how do you come up with O(n^(1/3))? ", "aSentId": 20187, "answer": "Optimally arranged storage would be in a sphere surrounding some access point. The worst case access speed of data stored within this volume will be primarily effected by the distance between the the access point and the edge of the sphere (the radius).\n\nThe radius of the sphere is proportional to V^(1/3) (with V being the volume of the sphere).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20188, "question": "Optimally arranged storage would be in a sphere surrounding some access point. The worst case access speed of data stored within this volume will be primarily effected by the distance between the the access point and the edge of the sphere (the radius).\n\nThe radius of the sphere is proportional to V^(1/3) (with V being the volume of the sphere).", "aSentId": 20189, "answer": "actually you can't even do that, because of heat.  if the heat generated is proportional to the number of components, to maintain a steady temperature, you would need surface area proportional to the number of components.  arranging in a sphere will be too dense to do that.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20190, "question": "actually you can't even do that, because of heat.  if the heat generated is proportional to the number of components, to maintain a steady temperature, you would need surface area proportional to the number of components.  arranging in a sphere will be too dense to do that.", "aSentId": 20191, "answer": "Also, a significantly large sphere would turn into a black hole. I have also heard that there is a physical limit to how large of an object can be held together.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20186, "question": "This is a really interesting point. Memory is typically the bottleneck in large scale simulations and it is probably because we assume it's constant and don't deal with the problem  algorithmically. \n\nStill, how do you come up with O(n^(1/3))? ", "aSentId": 20193, "answer": "&gt; Still, how do you come up with O(n^(1/3))? \n\nIt comes from assuming it's a cube.\n\nImagine we have a million items to store in a warehouse, and we have to drive a forklift to fetch one of them:\n\n* We can lay them all out in a line (1D) of a million items on the floor. On average, we have to drive a distance of 500,000 to items to get to the one we want (worst case, 1,000,000).\n* We can lay them out in a 2D grid on the floor. The size of the grid is 1000 \u00d7 1000. In the average case, we drive 500 items in the _x_ direction and 500 in the _y_ direction.\n* We can lay them out in a 3D grid by using shelves. The size of the grid is 100 \u00d7 100 \u00d7 100. In the average case, we drive 50 items in the _x_ direction and 50 in the _y_ direction and then raise the forklift to get 50 in the _z_ direction.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20184, "question": "No. That is the point: array access time **increases** linearly in function of the size of the array. It is a physical limitation. The only reason we say it is O(1) is an engineering accident about how our computers are made: we have a very low bound on the maximum length of the array (the amount of memory on a system) and we purposely make every random access take the worst possible time by synchronizing operations in clocks. So, yea, practically it is O(1), but that is as true as saying that `sort` is O(1) because we have specialized hardware with a `sort` primitive operation. Yea, on that computer it is - but that just means such machine has a size limit and doesn't change the fact sorting arbitrary lists isn't possible in constant time. \n\nMany aren't interested in the practical complexity \"given our current computer architecture\", since that can change. We are worried about the complexity of an algorithm given the physical limitations of our universe. When analysing the true complexity of an algorithm you must take in account the storage medium and the host universe. Or else you could very well say that we live in an infini-dimensional universe and suddenly radix tree insertion becomes O(1). Or in an universe without speed limits and then **everything** is O(1).\n\nThe true complexity of array access is `O(n^(1/3))` in our universe.", "aSentId": 20195, "answer": "Big oh notation is not a performance test. Its doesnt measure time but quantities of things. Array access is O (1) in the number of primitive operations needed, this is what people mean when they say time complexity. It is assumed that these operations are free. Big oh, in general, is not concerned with physical limitation (though it can be as long as you make your assumptions explicit)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20184, "question": "No. That is the point: array access time **increases** linearly in function of the size of the array. It is a physical limitation. The only reason we say it is O(1) is an engineering accident about how our computers are made: we have a very low bound on the maximum length of the array (the amount of memory on a system) and we purposely make every random access take the worst possible time by synchronizing operations in clocks. So, yea, practically it is O(1), but that is as true as saying that `sort` is O(1) because we have specialized hardware with a `sort` primitive operation. Yea, on that computer it is - but that just means such machine has a size limit and doesn't change the fact sorting arbitrary lists isn't possible in constant time. \n\nMany aren't interested in the practical complexity \"given our current computer architecture\", since that can change. We are worried about the complexity of an algorithm given the physical limitations of our universe. When analysing the true complexity of an algorithm you must take in account the storage medium and the host universe. Or else you could very well say that we live in an infini-dimensional universe and suddenly radix tree insertion becomes O(1). Or in an universe without speed limits and then **everything** is O(1).\n\nThe true complexity of array access is `O(n^(1/3))` in our universe.", "aSentId": 20197, "answer": "What practical application does this have? If I have an int[] and a pointer at index 100000000, what is the cost of accessing 100000001.  \n  \nWhile I do find this fascinating, my concern would be that including this into any computational complexity would be non-value added. Certainly assuming array access is O(1) is a simplification but what are the ramifications of not using this assumption? Is there a case where this assumption leads to a an analysis of a large n problem where an algorithm with a worse O-notation representation out performs the 'array-based' algorithm as a result of this limitation?  \n  \nIt seems this might come into play with O(1) data-structures (obviously we area talking about hash maps) and loglogn ones. Any more information would be great.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20198, "question": "What practical application does this have? If I have an int[] and a pointer at index 100000000, what is the cost of accessing 100000001.  \n  \nWhile I do find this fascinating, my concern would be that including this into any computational complexity would be non-value added. Certainly assuming array access is O(1) is a simplification but what are the ramifications of not using this assumption? Is there a case where this assumption leads to a an analysis of a large n problem where an algorithm with a worse O-notation representation out performs the 'array-based' algorithm as a result of this limitation?  \n  \nIt seems this might come into play with O(1) data-structures (obviously we area talking about hash maps) and loglogn ones. Any more information would be great.", "aSentId": 20199, "answer": "It is completely useless if you are talking about implementing algorithms to run on von-neumann architectures. It is useful if you are thinking about designing new, better architectures.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20182, "question": "When you talk about Big O running times, you have to say what the n represents. In your example, I'm assuming the n is the volume of the data storage, but that's not really a useful n to talk about since it a) doesn't vary and b) isn't an input to the function. \n\nIn many algorithms, the n is the length of the list and the `O()` represents number of comparisons, which is why array access is `O(1)`. You don't need to make any comparisons. That doesn't mean that it's instantaneous, but it does mean that it's not going to change with the number of elements in the array. Even if the array is so big that it takes longer to index into random parts due to physical drive constraints, it still doesn't take any comparisons.  \n\nIf you're talking about factoring or determining if a number is prime, the n is going to be the number of digits, not the size of the number. ", "aSentId": 20201, "answer": "&gt; O() represents number of comparisons\n\nthis is incorrect, most people would say \"array lookup is O(1) time\", meaning it takes a constant amount of time (not comparisons). If your quote were true, it would also be the case that looking up an element in a linked list is O(1) because you also don't need to make any comparisons.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20202, "question": "&gt; O() represents number of comparisons\n\nthis is incorrect, most people would say \"array lookup is O(1) time\", meaning it takes a constant amount of time (not comparisons). If your quote were true, it would also be the case that looking up an element in a linked list is O(1) because you also don't need to make any comparisons.", "aSentId": 20203, "answer": "Fair, it's not just comparisons, but the sum of primitive operations. In an array, element access and comparisons are the two primitive operations. Usually the big oh running time is not literally running time. \n\nAnd if you are looking strictly at comparisons, then it would be true that indexing a linked list (not lookup) would be O(1), however this isn't really a useful point to make. You could just as easily say that a sorting algorithm is constant time with respect to the number of cats in the room.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20180, "question": "Dude. Why is even array access considered `O(1)`? At least within our universe, as long as we don't find anything faster than light to use as a signal, it doesn't matter how efficiently we pack data on a 3d volume. It will still take at least `O(n^(1/3))` time for the signal to propagate from a point to another.", "aSentId": 20205, "answer": "One issue with this analogy is that it assumes that the items we pack are the same size. Even if we just use a 2D volume, we can keep the same access speed and pack in 4\u00d7 as much data if we shrink the size of everything by a factor of two.\n\nAnd that's what Moore's law keeps doing\u2014memories get bigger but they don't get slower because they've also become smaller.\n\nObviously, Moore's law can't go on forever, so your point still stands. Also, we if shrink feature sizes and _don't_ increase memory size, then things get _faster_, which is a positive way of viewing this property.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20180, "question": "Dude. Why is even array access considered `O(1)`? At least within our universe, as long as we don't find anything faster than light to use as a signal, it doesn't matter how efficiently we pack data on a 3d volume. It will still take at least `O(n^(1/3))` time for the signal to propagate from a point to another.", "aSentId": 20207, "answer": "There are self-consistent computation models in which memory access ignores speed of light limitations, even if physically unrealizable. \n\nThere are no self-consistent models in which you can assume n both is and is not bounded. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20209, "question": "&gt; hashtable lookup is constant (amortized)\n\nwell, the word \"amortized\" means something. Look it up.\n\nFor 1 operation, you are right, it's O(logn), or O(n). But for n operations, does it mean you may require up to O(n logn)? No, you still require O(n).\n\nFor example dynamic array: \"Insertion or removal of elements at the end - **amortized** constant O(1)\". For 1 insertion it may trigger reallocation of the whole array, so yes it's O(n) for 1 insertion. But for n insertions it still requires O(n), not O(n^2 ). Assume you insert  9 elements into an empty array, when the array is full, you reallocate it, double its size. So the cost are: \n\n1. 1 (1/1)\n1. 2 (realloc 1 element) (2/2)\n1. 3 (realloc 2 element) (3/4)\n1. 1 (4/4)\n1. 5 (realloc 4 elements) (5/8)\n1. 1 (6/8)\n1. 1 (7/8)\n1. 1 (8/8)\n1. 9 (realloc 8 elements) (9/16)\n\nThe total cost for 9 insertions is 9 + (1 + 2 + 4 + 8) = 24.\n\nYou may generalize it for n insertions, where n = 2^k + 1:\n\nn + (1 + 2 + 4 + ... + 2^k )  \n= n + (2^k+1 - 1) / (2 - 1)  \n= n + 2 * 2^k - 1.  \nBut n = 2^k + 1, so 2^k = n - 1, so our total cost are  \nn + 2(n - 1) - 1  \n= 3n - 3.  \nSo for n insertions it requires only O(n), not O(n^2 ). Therefore it is O(1) **amortized**", "aSentId": 20210, "answer": "I understand how amortization works.  I assumed away the costs of re-sizing the list and showed that, even with this (favorable) assumption, it's not O(1) amortized.  For the first case (no-resizing), n insertions are still O(n^2 ) and for the second case, they're O(n log n).\n\nAgain, if you can restrict n to being &lt; 2^k (or less than any constant), then yes each lookup averages to O(1) ... my whole point is that you don't get to assume this by the definition of big O.\n\nFor arbitrarily large n, you have to keep increasing the hash output size, which necessarily requires logarithmically increasing operations per insertion.  Computing a 5-bit hash requires 5c operations (and gives 2^5 buckets).  8-bit hash requires 8c operations (and gives 2^8 buckets).  And so on.\n\nEdit for clarity: my whole point was to dispute the values of your table.  It would be more like:\n\n1. 1c (hash requires 1c op)\n2. 2c (hash requires 2c ops)\n3. 3c (hash requires 3c ops)\n4. 3c\n5. 4c (hash requires 4c ops)\n6. 4c (6/8)\n7. 4c (7/8)\n8. 4c (8/8)\n9. 5c (hash requires 5c ops)\n\n(Again, ignoring resizing costs). Average over the first 2 is (3/2)c; the first 4 is (9/4)c.  Average over first 8 is (25/8)c.\n\nRecall:\n\n&gt;But getting more buckets means using a hash function with a larger output ... increasing the number of buckets exponentially requires a linear increase in the computation time of the hash function.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20211, "question": "I understand how amortization works.  I assumed away the costs of re-sizing the list and showed that, even with this (favorable) assumption, it's not O(1) amortized.  For the first case (no-resizing), n insertions are still O(n^2 ) and for the second case, they're O(n log n).\n\nAgain, if you can restrict n to being &lt; 2^k (or less than any constant), then yes each lookup averages to O(1) ... my whole point is that you don't get to assume this by the definition of big O.\n\nFor arbitrarily large n, you have to keep increasing the hash output size, which necessarily requires logarithmically increasing operations per insertion.  Computing a 5-bit hash requires 5c operations (and gives 2^5 buckets).  8-bit hash requires 8c operations (and gives 2^8 buckets).  And so on.\n\nEdit for clarity: my whole point was to dispute the values of your table.  It would be more like:\n\n1. 1c (hash requires 1c op)\n2. 2c (hash requires 2c ops)\n3. 3c (hash requires 3c ops)\n4. 3c\n5. 4c (hash requires 4c ops)\n6. 4c (6/8)\n7. 4c (7/8)\n8. 4c (8/8)\n9. 5c (hash requires 5c ops)\n\n(Again, ignoring resizing costs). Average over the first 2 is (3/2)c; the first 4 is (9/4)c.  Average over first 8 is (25/8)c.\n\nRecall:\n\n&gt;But getting more buckets means using a hash function with a larger output ... increasing the number of buckets exponentially requires a linear increase in the computation time of the hash function.", "aSentId": 20212, "answer": "I believe they calculate 32-bit hash or 64-bit hash value, and use logical &amp; operator to extract 4-bit or 5-bit, so the hashing step is about constant. For string it may depend on the length L of the string but not the size n of the table so you may assume hash function is constant O(1). It may require 10 ops, 20 ops, 100 ops, but it does not depend on the size n of the table, so it is constant. It only depends on the key, which is, very small compared to n and can be considered constant. If n is small then you may consider switch to tree table, since the cost of hashing may become big compared to n.\n\nThe time complexity I guess if you are given M buckets and N objects, M &lt; N, so yes there must be some buckets that contain more than 1 object. But you don't worry about which bucket has the most objects. You worry about how many empty buckets X there are. Good hash function will distribute evenly so X ~ 0. Therefore you can say the cost for each insertion is about N/M ops amortized, and if N ~ cM, c is some constant  then each insertion is about c ops, which is O(1) amortized.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20213, "question": "I believe they calculate 32-bit hash or 64-bit hash value, and use logical &amp; operator to extract 4-bit or 5-bit, so the hashing step is about constant. For string it may depend on the length L of the string but not the size n of the table so you may assume hash function is constant O(1). It may require 10 ops, 20 ops, 100 ops, but it does not depend on the size n of the table, so it is constant. It only depends on the key, which is, very small compared to n and can be considered constant. If n is small then you may consider switch to tree table, since the cost of hashing may become big compared to n.\n\nThe time complexity I guess if you are given M buckets and N objects, M &lt; N, so yes there must be some buckets that contain more than 1 object. But you don't worry about which bucket has the most objects. You worry about how many empty buckets X there are. Good hash function will distribute evenly so X ~ 0. Therefore you can say the cost for each insertion is about N/M ops amortized, and if N ~ cM, c is some constant  then each insertion is about c ops, which is O(1) amortized.", "aSentId": 20214, "answer": "&gt; I believe they calculate 32-bit hash or 64-bit hash value, and use logical &amp; operator to extract 4-bit or 5-bit, so the hashing step is about constant. ... It may require 10 ops, 20 ops, 100 ops, but it does not depend on the size n of the table, so it is constant.\n\nBut it *does* depend on the size of the table -- my whole point is that clipping off a piece of the output doesn't work anymore once there are more than 2^64 elements.  In order to get more buckets, you have to use a hash function that requires more steps -- specifically, O(log n) steps.\n\nThis is why I said (and repeated again):\n\n&gt;But getting more buckets means using a hash function with a larger output ... increasing the number of buckets exponentially requires a linear increase in the computation time of the hash function.\n\n... which (again), is not a practical consideration, but does violate the meaning of big-O, which is \"we don't allow limits on n\".\n\n&gt;The time complexity I guess if you are given M buckets and N objects, M &lt; N, so yes there must be some buckets that contain more than 1 object. But you don't worry about which bucket has the most objects. You worry about how many empty buckets X there are. Good hash function will distribute evenly so X ~ 0. Therefore you can say the cost for each insertion is about N/M ops amortized, and if N ~ cM, c is some constant then each insertion is about c ops, which is O(1) amortized.\n\nIncorrect; as I said above, that's still O(n).\n\n&gt;If so, then as n gets arbitrarily large, each bucket (hash value) has `(1/2^k)n` elements that you search through as a linked list (in conventional accounts). That's still O(n) -- albeit with a coefficient logarithmic in k -- not O(1).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20215, "question": "&gt; I believe they calculate 32-bit hash or 64-bit hash value, and use logical &amp; operator to extract 4-bit or 5-bit, so the hashing step is about constant. ... It may require 10 ops, 20 ops, 100 ops, but it does not depend on the size n of the table, so it is constant.\n\nBut it *does* depend on the size of the table -- my whole point is that clipping off a piece of the output doesn't work anymore once there are more than 2^64 elements.  In order to get more buckets, you have to use a hash function that requires more steps -- specifically, O(log n) steps.\n\nThis is why I said (and repeated again):\n\n&gt;But getting more buckets means using a hash function with a larger output ... increasing the number of buckets exponentially requires a linear increase in the computation time of the hash function.\n\n... which (again), is not a practical consideration, but does violate the meaning of big-O, which is \"we don't allow limits on n\".\n\n&gt;The time complexity I guess if you are given M buckets and N objects, M &lt; N, so yes there must be some buckets that contain more than 1 object. But you don't worry about which bucket has the most objects. You worry about how many empty buckets X there are. Good hash function will distribute evenly so X ~ 0. Therefore you can say the cost for each insertion is about N/M ops amortized, and if N ~ cM, c is some constant then each insertion is about c ops, which is O(1) amortized.\n\nIncorrect; as I said above, that's still O(n).\n\n&gt;If so, then as n gets arbitrarily large, each bucket (hash value) has `(1/2^k)n` elements that you search through as a linked list (in conventional accounts). That's still O(n) -- albeit with a coefficient logarithmic in k -- not O(1).", "aSentId": 20216, "answer": "First of all, when working with big-O, it's always a theoretical conversation.  You aren't talking about running on actual hardware, rather just trying to get a general idea of how the complexity of your algorithm grows with the input size.  The standard convention is to define arithmetic operations as taking O(1) time (remember, not talking about actual hardware so the number of bits required to represent the arithmetic operands, or the processor word width don't matter.  In fact, they don't even exist for purposes of that particular discussion).  Therefore, this\n&gt;\"But getting more buckets means using a hash function with a larger output ... increasing the number of buckets exponentially requires a linear increase in the computation time of the hash function.\"\n\nisn't really the case when theoretically discussing big-O as it might be on real hardware.  We are not placing a limit on n as you suggest, rather just ignoring real-world hardware concerns (which are likely negligible in most situations anyways, especially when compared to the dropped constants) while discussing theoretical complexity.\n\nIf you don't like this, I promise you it's really not any different than all the other quirks of hardware that you're already ignoring to perform big-O calculations.  Think about memory access times (with the cache hierarchy), the difference between integer and floating point ops, operating system overhead, etc.  All of this is ignored when working with big-O because it's a theoretical calculation only meant to give a rough estimate of the algorithm's complexity.\n\nTherefore on our imaginary theoretical hardware, by convention we define that arithmetic is always O(1) because this is both simple and reasonably accurate in most cases.  Similarly, since arithmetic is assumed O(1), we assume that we will be able to build a hash function from some finite number of arithmetic ops, and that the hash function will also be O(1).\n\nThus, the above poster is correct\n&gt;\"The time complexity I guess if you are given M buckets and N objects, M &lt; N, so yes there must be some buckets that contain more than 1 object. But you don't worry about which bucket has the most objects. You worry about how many empty buckets X there are. Good hash function will distribute evenly so X ~ 0. Therefore you can say the cost for each insertion is about N/M ops amortized, and if N ~ cM, c is some constant then each insertion is about c ops, which is O(1) amortized.\"\n\nWe really only care about the average time, not the worst case (hence the amortized).  So as he correctly mentioned, there must be some buckets that contain multiple objects but is assumed that there are enough buckets that this is a reasonably small number (or that we'll add more buckets otherwise, which will also be an amortized cost).  Since your average access time is N/M, we are assuming that the hash table keeps M relatively close in magnitude to N, we get some constant value.  Therefore, accessing an element in a hash table is O(1) with a fairly standard set of assumptions.\n", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20217, "question": "First of all, when working with big-O, it's always a theoretical conversation.  You aren't talking about running on actual hardware, rather just trying to get a general idea of how the complexity of your algorithm grows with the input size.  The standard convention is to define arithmetic operations as taking O(1) time (remember, not talking about actual hardware so the number of bits required to represent the arithmetic operands, or the processor word width don't matter.  In fact, they don't even exist for purposes of that particular discussion).  Therefore, this\n&gt;\"But getting more buckets means using a hash function with a larger output ... increasing the number of buckets exponentially requires a linear increase in the computation time of the hash function.\"\n\nisn't really the case when theoretically discussing big-O as it might be on real hardware.  We are not placing a limit on n as you suggest, rather just ignoring real-world hardware concerns (which are likely negligible in most situations anyways, especially when compared to the dropped constants) while discussing theoretical complexity.\n\nIf you don't like this, I promise you it's really not any different than all the other quirks of hardware that you're already ignoring to perform big-O calculations.  Think about memory access times (with the cache hierarchy), the difference between integer and floating point ops, operating system overhead, etc.  All of this is ignored when working with big-O because it's a theoretical calculation only meant to give a rough estimate of the algorithm's complexity.\n\nTherefore on our imaginary theoretical hardware, by convention we define that arithmetic is always O(1) because this is both simple and reasonably accurate in most cases.  Similarly, since arithmetic is assumed O(1), we assume that we will be able to build a hash function from some finite number of arithmetic ops, and that the hash function will also be O(1).\n\nThus, the above poster is correct\n&gt;\"The time complexity I guess if you are given M buckets and N objects, M &lt; N, so yes there must be some buckets that contain more than 1 object. But you don't worry about which bucket has the most objects. You worry about how many empty buckets X there are. Good hash function will distribute evenly so X ~ 0. Therefore you can say the cost for each insertion is about N/M ops amortized, and if N ~ cM, c is some constant then each insertion is about c ops, which is O(1) amortized.\"\n\nWe really only care about the average time, not the worst case (hence the amortized).  So as he correctly mentioned, there must be some buckets that contain multiple objects but is assumed that there are enough buckets that this is a reasonably small number (or that we'll add more buckets otherwise, which will also be an amortized cost).  Since your average access time is N/M, we are assuming that the hash table keeps M relatively close in magnitude to N, we get some constant value.  Therefore, accessing an element in a hash table is O(1) with a fairly standard set of assumptions.\n", "aSentId": 20218, "answer": "Keeping M close to N requires a larger hashspace, which requires a larger hash output, which requires logarithmically more operations per hash computation, even if all arithmetic operations are constant. This is what I addressed as the second case, where you can resize the table. \n\nGetting a 5 bit hash output requires (linearly) more steps than a 4 bit hash output. This remains true even if all mod H operations are O(1) (H being size of hashspace), because you have to adapt the hash function to give larger output too. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20219, "question": "Keeping M close to N requires a larger hashspace, which requires a larger hash output, which requires logarithmically more operations per hash computation, even if all arithmetic operations are constant. This is what I addressed as the second case, where you can resize the table. \n\nGetting a 5 bit hash output requires (linearly) more steps than a 4 bit hash output. This remains true even if all mod H operations are O(1) (H being size of hashspace), because you have to adapt the hash function to give larger output too. ", "aSentId": 20220, "answer": "When you do big-O, you make som asumptions. Some of them is that memory access and any arithmetic operation is O(1) - this is (in the big-O) true for any integer size. You can try to do this with real hardware constraints, but in that case you'll likely just assume some fixed size of the hash, and then the assumptions will hold anyway.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20221, "question": "When you do big-O, you make som asumptions. Some of them is that memory access and any arithmetic operation is O(1) - this is (in the big-O) true for any integer size. You can try to do this with real hardware constraints, but in that case you'll likely just assume some fixed size of the hash, and then the assumptions will hold anyway.", "aSentId": 20222, "answer": "Even with constant arithmetic operations, you need more such operations as the hash output size increases. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20223, "question": "Even with constant arithmetic operations, you need more such operations as the hash output size increases. ", "aSentId": 20224, "answer": "The proof is not on an arbitrarily large hash output, but rather for some fixed size. If you need larger than this, the proof is exactly the same, the constants are just bigger.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20225, "question": "The proof is not on an arbitrarily large hash output, but rather for some fixed size. If you need larger than this, the proof is exactly the same, the constants are just bigger.", "aSentId": 20226, "answer": "But big O is an asymptotic bound so you have to allow for arbitrarily large tables. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20227, "question": "But big O is an asymptotic bound so you have to allow for arbitrarily large tables. ", "aSentId": 20228, "answer": "That depends on what your parameter is. But in any case, since memory operations are constant speed, it doesn't matter if the output increases or not. 16 bits are no faster to read then 256 bits (in the big O abstraction).", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20219, "question": "Keeping M close to N requires a larger hashspace, which requires a larger hash output, which requires logarithmically more operations per hash computation, even if all arithmetic operations are constant. This is what I addressed as the second case, where you can resize the table. \n\nGetting a 5 bit hash output requires (linearly) more steps than a 4 bit hash output. This remains true even if all mod H operations are O(1) (H being size of hashspace), because you have to adapt the hash function to give larger output too. ", "aSentId": 20230, "answer": "Most real-world hash functions have either 32-bit or 64-bit outputs, depending on the word side of the machine.  For a small table, you're just truncating the output, so it doesn't get any faster.  If the table exceeds the capacity of the hash function, then it also exceeds the address space of the machine, so there's no reason to make the hash output any larger.\n\nO(1) is a useful way to understand hash tables in practice, even if they wouldn't actually work that way on a theoretical computer with infinite memory.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20219, "question": "Keeping M close to N requires a larger hashspace, which requires a larger hash output, which requires logarithmically more operations per hash computation, even if all arithmetic operations are constant. This is what I addressed as the second case, where you can resize the table. \n\nGetting a 5 bit hash output requires (linearly) more steps than a 4 bit hash output. This remains true even if all mod H operations are O(1) (H being size of hashspace), because you have to adapt the hash function to give larger output too. ", "aSentId": 20232, "answer": "&gt; Getting a 5 bit hash output requires (linearly) more steps than a 4 bit hash output.\n\nI'm not really sure why you think that, but it's not true for all cases.  The modulo hash function you mention, which is about as simple as a hash function can get, is a great counter example to your point.  Going from four to five bit hash function output in this case is as simple as changing your modulo from 2^4 to 2^5.  That certainly does not involve a linear increase in the complexity of your hash function. They both require precisely one op, which is clearly constant time.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20233, "question": "&gt; Getting a 5 bit hash output requires (linearly) more steps than a 4 bit hash output.\n\nI'm not really sure why you think that, but it's not true for all cases.  The modulo hash function you mention, which is about as simple as a hash function can get, is a great counter example to your point.  Going from four to five bit hash function output in this case is as simple as changing your modulo from 2^4 to 2^5.  That certainly does not involve a linear increase in the complexity of your hash function. They both require precisely one op, which is clearly constant time.", "aSentId": 20234, "answer": "As I said several times now, that trick stops working when n increases to more than the hashspace :-P\n\nEdit: why is this downvoted? Everyone here seems to think that using a larger modulus increases the hash space, even when the modulus itself is larger than the hash space ... Which is trivially false. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20235, "question": "As I said several times now, that trick stops working when n increases to more than the hashspace :-P\n\nEdit: why is this downvoted? Everyone here seems to think that using a larger modulus increases the hash space, even when the modulus itself is larger than the hash space ... Which is trivially false. ", "aSentId": 20236, "answer": "You can't have a modulus larger than the hash space unless elements aren't distinct, in which case it doesn't actually matter.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20209, "question": "&gt; hashtable lookup is constant (amortized)\n\nwell, the word \"amortized\" means something. Look it up.\n\nFor 1 operation, you are right, it's O(logn), or O(n). But for n operations, does it mean you may require up to O(n logn)? No, you still require O(n).\n\nFor example dynamic array: \"Insertion or removal of elements at the end - **amortized** constant O(1)\". For 1 insertion it may trigger reallocation of the whole array, so yes it's O(n) for 1 insertion. But for n insertions it still requires O(n), not O(n^2 ). Assume you insert  9 elements into an empty array, when the array is full, you reallocate it, double its size. So the cost are: \n\n1. 1 (1/1)\n1. 2 (realloc 1 element) (2/2)\n1. 3 (realloc 2 element) (3/4)\n1. 1 (4/4)\n1. 5 (realloc 4 elements) (5/8)\n1. 1 (6/8)\n1. 1 (7/8)\n1. 1 (8/8)\n1. 9 (realloc 8 elements) (9/16)\n\nThe total cost for 9 insertions is 9 + (1 + 2 + 4 + 8) = 24.\n\nYou may generalize it for n insertions, where n = 2^k + 1:\n\nn + (1 + 2 + 4 + ... + 2^k )  \n= n + (2^k+1 - 1) / (2 - 1)  \n= n + 2 * 2^k - 1.  \nBut n = 2^k + 1, so 2^k = n - 1, so our total cost are  \nn + 2(n - 1) - 1  \n= 3n - 3.  \nSo for n insertions it requires only O(n), not O(n^2 ). Therefore it is O(1) **amortized**", "aSentId": 20238, "answer": "This is the correct answer.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20240, "question": "&gt; With that said, you do need extremely high values of n for this to be an issue, which I understand. But big-O means arbitrarily large, and it doesn't seem like you can actually make it O(1) across arbitrarily large tables.\n\nName me a single nontrivial O(1) algorithm then.", "aSentId": 20241, "answer": "Parity. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20242, "question": "Parity. ", "aSentId": 20243, "answer": "How do you get each input value? You gotta get and discard all the bits except the least significant, that's not free.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20244, "question": "How do you get each input value? You gotta get and discard all the bits except the least significant, that's not free.", "aSentId": 20245, "answer": "Parity of the nearest bit that your representation has access to. (Eg if you start from LSB, parity of that bit.)", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20246, "question": "Parity of the nearest bit that your representation has access to. (Eg if you start from LSB, parity of that bit.)", "aSentId": 20247, "answer": "But how do you get and discard the rest of the number? Do you have a magic \"fetch me the next number\" function that can fetch you a multi-gigabyte number in O(1), or position you at the next such number, or whatever?", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20249, "question": "Also why would you asset the hash function takes more time? We're not bound to 64 bit architecture. If we were working on a8 bit system n would be much smaller before that case of long running hash functions.", "aSentId": 20250, "answer": "Because more values in the output requires linear time in the length of the output. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20255, "question": "I think what's happening in this particular case is that hash functions today have huge outputs, so we do not encounter cases in which you'd need to replace them. This means hash function output size is assumed constant on n.\n\nWhile you are technically right, we do make assumptions like this all the time, like considering addition and multiplication to be O(1), when they are actually dependent on the number of bits used to represent the input.", "aSentId": 20256, "answer": "I don't think that's a valid comparison. The assumption that you can only operate on k-bit numbers only adds constant-factor overhead, which doesn't change the big-O. Eg you might require 2n operations where unrestricted would take n, but you wouldnt require n^2 .   ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20257, "question": "I don't think that's a valid comparison. The assumption that you can only operate on k-bit numbers only adds constant-factor overhead, which doesn't change the big-O. Eg you might require 2n operations where unrestricted would take n, but you wouldnt require n^2 .   ", "aSentId": 20258, "answer": "Technically, addition takes O(log k) time, and O(k) total work on CLAs. Multiplication is even more work, and a linear lower bound has been proven.\n\nEdit: you're treating k as a constant, just like other people are treating hash function output size as a constant.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20259, "question": "Technically, addition takes O(log k) time, and O(k) total work on CLAs. Multiplication is even more work, and a linear lower bound has been proven.\n\nEdit: you're treating k as a constant, just like other people are treating hash function output size as a constant.", "aSentId": 20260, "answer": "But hash function output size has to increase with n. This differs from the case of addition in which we can have arbitrarily many addition operations while also bounding the size of the numbers we operate in. The addend size is independent of the number of addends. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20267, "question": "It feels like your issue comes from imagining a hash table, then performing n insertions on it. That's not what big o is doing. There is no initial state where you don't know n ahead of time. Furthermore the state of the date does not preserve after calls.\n\n\nBig O notation isn't applied to continually running the problem. The values are fixed. That is, say for n = 100. The time it takes to insert 100 elements into the hash table is O(h(1)) where h is the hash function. You pick any n, and you're looking at inserting that many elements into a prepared and empty table. There is no initial state. As n gets large, you STILL have a large and empty enough table. *There is no shared state between running the operation as n increases.*", "aSentId": 20268, "answer": "For a given fixed table size, lookup is constant within that size. I agree with you there. By the larger the fixed size, the larger that constant. ", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20271, "question": "Type Theory in 15 Minutes", "aSentId": 20272, "answer": "While cool and pretty well done, I think the bigger implication of Type Theory is not just implying basic boolean logic, but the concept of types directly produces ALL predicate logic, which this blog stops just short of proving.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20273, "question": "While cool and pretty well done, I think the bigger implication of Type Theory is not just implying basic boolean logic, but the concept of types directly produces ALL predicate logic, which this blog stops just short of proving.", "aSentId": 20274, "answer": "Yup! I've been meaning to write a followup going into that since I first wrote this. I figured the post was long enough (\"15 minutes\" was already ambitious). Hopefully, I'll have the other one up today or tomorrow...", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20275, "question": "Yup! I've been meaning to write a followup going into that since I first wrote this. I figured the post was long enough (\"15 minutes\" was already ambitious). Hopefully, I'll have the other one up today or tomorrow...", "aSentId": 20276, "answer": "Probably a good choice on the length then. I think the types -&gt; predicate logic is the cooler part of the implication. The idea that the axiom of \"data can have a type\" implies predicate logic is just a really cool and impressive idea I think.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20271, "question": "Type Theory in 15 Minutes", "aSentId": 20278, "answer": "Thanks for the interest (and the corrections)! I've cleaned up the typos (hopefully I got all of them) and some formatting. Working on a followup post on dependent types now.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20271, "question": "Type Theory in 15 Minutes", "aSentId": 20280, "answer": "A *barrack* is where soldiers sleep. Barack Obama is the President of the United States.", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20281, "question": "A *barrack* is where soldiers sleep. Barack Obama is the President of the United States.", "aSentId": 20282, "answer": "Another *type error* ha ha ha\n\n&gt; what it\u2019s canonical elements are\n\nits*", "corpus": "reddit"},{"docID": "t5_2qhmr", "qSentId": 20283, "question": "Another *type error* ha ha ha\n\n&gt; what it\u2019s canonical elements are\n\nits*", "aSentId": 20284, "answer": "&gt; This fact is capture by\n\ncaptured*", "corpus": "reddit"}]