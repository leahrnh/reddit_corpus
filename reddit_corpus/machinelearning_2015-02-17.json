[{"docID": "t5_2r3gv","qSentId": 53901,"question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST","aSentId": 53902,"answer": "You again!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53901,"question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST","aSentId": 53904,"answer": "All I can say is _wow_! This shall be interesting.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53901,"question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST","aSentId": 53906,"answer": "You are a god.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53911,"question": "\"DRAW: A Recurrent Neural Network For Image Generation\" from DeepMind","aSentId": 53912,"answer": "While I enjoyed this paper greatly, I think that one needs to remain skeptical of visualizations like Figures 9,10, and 12 from this paper which show samples from the model alongisde the closest images from the training set.  The reason is that the L2 distance isn't a very strong measure of image similarity.  A generative model that just reproduces images from the training set but with random changes in brightness would cause the corresponding image from the training set to not show up in such a visualization.  \n\nPerhaps such a visualization would better if one showed the closest images in the hidden representation from a convnet / autoencoder trained on the dataset instead?  \n\nThe paper has other evaluations (like log-likelihood on held out data), so I'm pretty sure that's not happening here.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53911,"question": "\"DRAW: A Recurrent Neural Network For Image Generation\" from DeepMind","aSentId": 53914,"answer": "Maybe this is a weird thing to say, but it's downright spooky to see algorithms generate new lifelike samples out of thin air. Makes you wonder what kind of things technology like this will be used for in the future.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53911,"question": "\"DRAW: A Recurrent Neural Network For Image Generation\" from DeepMind","aSentId": 53916,"answer": "Generative art is coming. Someone will train this alongside a language model next and then create images from text samples like twitter. \n\nJust the idea of a neural network producing art from art sample as an input is exciting.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53918,"question": "The Genetic Algorithm - Explained","aSentId": 53919,"answer": "Shameless plug for /r/genetic_algorithms","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53918,"question": "The Genetic Algorithm - Explained","aSentId": 53921,"answer": "Nice, very clear explanation. Wish I had found an article like this when I was learning to implement one.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53922,"question": "Nice, very clear explanation. Wish I had found an article like this when I was learning to implement one.","aSentId": 53923,"answer": "Okay, I'm still baffled as to how could you use this sort of thing?!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53924,"question": "Okay, I'm still baffled as to how could you use this sort of thing?!","aSentId": 53925,"answer": "&gt;Generally used in problems where linear/brute-force searches are not viable in terms of time, such as \u2013 Travelling Salesman Problem, Timetable Scheduling, Finding Neural Network Weights, Sudoku, Trees(data-structure) etc..\n\nUsing a GA can give you a 'good enough' solution for the stuff above that doesn't have an exact solution. And of course more directly mimicking evolution link in the link below. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53924,"question": "Okay, I'm still baffled as to how could you use this sort of thing?!","aSentId": 53927,"answer": "They're great when you know how to score a solution but don't have a clue how to come up with an algorithm to solve it.  They're kind of a heuristic A* search for ginormous search spaces and well suited when there is a lot of interacting parts.  They're often really easy to implement too.\n\nBut honestly the reason I usually implement them is they're a ton of fun to watch.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53928,"question": "They're great when you know how to score a solution but don't have a clue how to come up with an algorithm to solve it.  They're kind of a heuristic A* search for ginormous search spaces and well suited when there is a lot of interacting parts.  They're often really easy to implement too.\n\nBut honestly the reason I usually implement them is they're a ton of fun to watch.","aSentId": 53929,"answer": "Where did you start? I want to learn more, but just don't know where to start :)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53930,"question": "Where did you start? I want to learn more, but just don't know where to start :)","aSentId": 53931,"answer": "I think I started in '97 after reading an article on genetic algorithms.  Thanks for asking!  Today I would start with the Wikipedia page and google.  A little googling and [this came up](http://www.ai-junkie.com/ga/intro/gat1.html).  Just keep in mind you don't really need to use bits to encode the DNA.\n\nI would recommend keeping the top 50% of each run.  Have at least 100 guys.  As a speed up, each offspring gets at least one mutation if it ends up being the same as one of the parents.  There's a lot of parameters to tweak like mutation rate, population size, what percent of the population to keep, etc.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53918,"question": "The Genetic Algorithm - Explained","aSentId": 53933,"answer": "Fantastic!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53935,"question": "How many different subs did you post this to?  You should mark it as a cross-post so the comments aren't spread all over and diluted. ","aSentId": 53936,"answer": "That's what x-posting does??","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53937,"question": "That's what x-posting does??","aSentId": 53938,"answer": "I think that's the point.  You post something in the most relevant place.  Then go to other subs that could also be relevant and post a x-post to the original.  That tends to gather the comments and discussion to the original posting.\n\nAt least that's how I understand how it works.  I could be totally wrong, and it wouldn't be the first time.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53941,"question": "Why word2vec works","aSentId": 53942,"answer": "Seems like this technique could scale inductively to vectorize entire classes of words (documents, web page, etc).","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53944,"question": "Unsupervised Learning of Video Representations using LSTMs","aSentId": 53945,"answer": "Very nice work.  This is something that I've wanted to see for a while.  I have two criticisms: \n\n1.  The input and the output are both structured as images.  The model does use convolutional features in the input, but not in the output.  As far as I can tell, the output layer is fully connected, which seems suboptimal.  Perhaps Zeiler's deconvolutional network would be better.  \n\n2.  They use the squared error as their loss function.  This corresponds to estimating the mean pixel intensity (or pixel intensity for each color).  This is worse than having a generative model that finds a joint distribution over the intensity of the pixels.  This also explains why the model blurs.  If there's a 50% chance a ball will fall right and a 50% chance a ball will fall left, then the mean pixel intensity corresponds to an image containing a ball falling left and a ball falling right, both with 50% intensity.  Note that the output here actually has 0% probability on a sensible generative model.  \n\nInstead they should use a generative model on the output.  One option is R-NADE.  Another is a mixture of gaussians.  Yet another is generative adversarial networks.  I'm not really happy with any of these solutions.  (R-NADE is slow and doesn't exploit spatial structure, MOG is hard to optimize and limits the number of modes, GAN is very hard to optimize).  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53947,"question": "FunkyYak: autodiff raw numpy+Python code","aSentId": 53948,"answer": "This is the autodiff library mentioned in the (very cool) recent paper, \"[Gradient-based Hyperparameter Optimization through Reversible Learning](http://arxiv.org/abs/1502.03492)\". ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53949,"question": "This is the autodiff library mentioned in the (very cool) recent paper, \"[Gradient-based Hyperparameter Optimization through Reversible Learning](http://arxiv.org/abs/1502.03492)\". ","aSentId": 53950,"answer": "If I understand RMD correctly, is this paper treating the entire training process as some ridiculously long feedforward function and then taking the gradient of that feedforward function w.r.t to the hyperparameter?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53951,"question": "If I understand RMD correctly, is this paper treating the entire training process as some ridiculously long feedforward function and then taking the gradient of that feedforward function w.r.t to the hyperparameter?","aSentId": 53952,"answer": "Yes, exactly.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53953,"question": "Yes, exactly.","aSentId": 53954,"answer": "Well, that's certainly a form of gradient descent! Is the extra cost even worth it?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53951,"question": "If I understand RMD correctly, is this paper treating the entire training process as some ridiculously long feedforward function and then taking the gradient of that feedforward function w.r.t to the hyperparameter?","aSentId": 53956,"answer": "Yes, but the gradient of the training process is computed on the validation set if I understood the paper correctly.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53958,"question": "Ask ML: has anyone tried to reproduce ADASECANT?","aSentId": 53959,"answer": "Thanks for this, this is the first implementation of this algorithm I've found anywhere.\n\nI don't know what the problem is yet, but it seems the function starts diverging much earlier than 100 steps. It pretty much starts going off the rails as soon as you \"turn off\" normal SGD (step 10), as eta jumps up from 0.001 up to 1 and stays there. This suggests that there's might be some fundamental mistake / typo in the code. I'll try fine-reading the article again and see what I can figure out.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53958,"question": "Ask ML: has anyone tried to reproduce ADASECANT?","aSentId": 53961,"answer": "I see a third version was posted at the end of january. is that what you're working from? I haven't checked what differences there are, but I remember in the earlier versions there was some uncertainty about how things were defined.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53958,"question": "Ask ML: has anyone tried to reproduce ADASECANT?","aSentId": 53963,"answer": "Optimization algorithms can be difficult to debug. Use usual test functions from the literature such as sphere, rosenbrock and ellipse. Report the plots and look at convergence rate etc... Compare to SGD. My 2cents ;)\n\nEdit: you are already on sphere, bad sign...\n\nEdit2: remove the randomness or use exact gradient.\n\nEdit3: tips, run SGD in parallel and compare values","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53964,"question": "Optimization algorithms can be difficult to debug. Use usual test functions from the literature such as sphere, rosenbrock and ellipse. Report the plots and look at convergence rate etc... Compare to SGD. My 2cents ;)\n\nEdit: you are already on sphere, bad sign...\n\nEdit2: remove the randomness or use exact gradient.\n\nEdit3: tips, run SGD in parallel and compare values","aSentId": 53965,"answer": "Lawyer up, delete facebook, hit the gym? I've already done all the above!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53966,"question": "Lawyer up, delete facebook, hit the gym? I've already done all the above!","aSentId": 53967,"answer": "Haha right, reddit remains ;)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53970,"question": "Back-propagation output layer error confusion -- wondering if anyone can clear up this quick question?","aSentId": 53971,"answer": "In the formula (yi-ai)*f'(zi), the f(zi) is the activation function for the output layer. So ai = f(zi), meaning f'(zi) = d(ai)/d(zi). It's been a while since I worked through this but I think Ng is using linear activations for the top-most layer here, which means f'(x) = 1. If you use a non-linear output layer function (like softmax) then this is no longer true.\n\n(In fact for softmax you get non-diagonal terms where d(ai)/d(zj) != 0 when i != j, so you have to be extra careful in that case.)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53976,"question": "Net#: Introducing Microsoft's Neural Nets Library","aSentId": 53977,"answer": "Unfortunate that the name Net# is ungoogleable (and even unbingable).","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53978,"question": "Unfortunate that the name Net# is ungoogleable (and even unbingable).","aSentId": 53979,"answer": "Completely typical though! ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53978,"question": "Unfortunate that the name Net# is ungoogleable (and even unbingable).","aSentId": 53981,"answer": "Luckily it's based off their .net technologies so they can local in and give you a hand at any time.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53976,"question": "Net#: Introducing Microsoft's Neural Nets Library","aSentId": 53983,"answer": "What are the benefits of using this over, say scikit-learn package?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53984,"question": "What are the benefits of using this over, say scikit-learn package?","aSentId": 53985,"answer": "for one thing, sklearn doesn't have neural networks...","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53986,"question": "for one thing, sklearn doesn't have neural networks...","aSentId": 53987,"answer": "whats the comparison wit PyGenes or other Python libraries for machine learning?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53984,"question": "What are the benefits of using this over, say scikit-learn package?","aSentId": 53989,"answer": "I'd assume better/easier integration for .NET developers. Seems rather intuitive to me, as they have considerable motivation for ensuring that .NET developers are able to pull off the type of stuff that people are doing in other dev environments.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53984,"question": "What are the benefits of using this over, say scikit-learn package?","aSentId": 53991,"answer": "If one replaces scikit-learn with Theano or Torch, I have the same question.  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53992,"question": "If one replaces scikit-learn with Theano or Torch, I have the same question.  ","aSentId": 53993,"answer": "looks like cuda-convnet/caffe model definitions that run in the \"cloud\". Unsure if they do stuff in parallel or its run on a single server.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53992,"question": "If one replaces scikit-learn with Theano or Torch, I have the same question.  ","aSentId": 53995,"answer": "I was thinking the advantage would be Windows support, but I don't see any evidence of that. It seems to be some kind of cloud thing.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53996,"question": "I was thinking the advantage would be Windows support, but I don't see any evidence of that. It seems to be some kind of cloud thing.","aSentId": 53997,"answer": "Yes, it's part of their Azure cloud computing service.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 53998,"question": "Yes, it's part of their Azure cloud computing service.","aSentId": 53999,"answer": "I was hoping for some good C# NN source code implementations. Seems instead that this isn't a \"library\" at all, it's a programming language, used to interface with their web service as you say.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54000,"question": "I was hoping for some good C# NN source code implementations. Seems instead that this isn't a \"library\" at all, it's a programming language, used to interface with their web service as you say.","aSentId": 54001,"answer": "How about accord.net?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54004,"question": "Sentence Labelling in Torch7","aSentId": 54005,"answer": "I don't know about your question but I just want to mention that there is just typo in the name of the author. His name is Jason Weston.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54006,"question": "I don't know about your question but I just want to mention that there is just typo in the name of the author. His name is Jason Weston.","aSentId": 54007,"answer": "thanks you fixed the typo","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54009,"question": "Random Walks... (training 1000-layer networks)","aSentId": 54010,"answer": "Seems like an interesting direction of research.  But I don't understand why does the author report exclusively training performance but not classification performance (on testing set).  The author just says that making it work well on classification tasks is subject of further research; but it should be trivial to measure and report the performance. Maybe it brutally overfits and the performance is abysmal, but learning that would still leave me with much better taste than not reporting it at all. If he already has the networks, I'd ask the author for this even as a reviewer of a workshop paper.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54011,"question": "Seems like an interesting direction of research.  But I don't understand why does the author report exclusively training performance but not classification performance (on testing set).  The author just says that making it work well on classification tasks is subject of further research; but it should be trivial to measure and report the performance. Maybe it brutally overfits and the performance is abysmal, but learning that would still leave me with much better taste than not reporting it at all. If he already has the networks, I'd ask the author for this even as a reviewer of a workshop paper.","aSentId": 54012,"answer": "Test set classification performance isn't the point of the paper. The paper is presenting a new training scheme to train extremely deep networks.\n","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54013,"question": "Test set classification performance isn't the point of the paper. The paper is presenting a new training scheme to train extremely deep networks.\n","aSentId": 54014,"answer": "I agree. The point seems to be to prove some sort of convergence here, instead of exploding / vanishing gradients. I would expect as /u/paskie says that a 1000-layer network overfits horribly on MNIST, so the test results likely aren't that interesting.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54015,"question": "I agree. The point seems to be to prove some sort of convergence here, instead of exploding / vanishing gradients. I would expect as /u/paskie says that a 1000-layer network overfits horribly on MNIST, so the test results likely aren't that interesting.","aSentId": 54016,"answer": "Well, anytime I read a paper and expect some basic corrolaries that I'm not sure about and the paper doesn't answer it, I dislike that paper for it. If it overfits, they should just say so and throw in a number.\n\nIt also helps a lot when reproducing the work, by comparing both numbers others can be much more sure they reached the same result.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54013,"question": "Test set classification performance isn't the point of the paper. The paper is presenting a new training scheme to train extremely deep networks.\n","aSentId": 54018,"answer": "While true, I think its fair to ask for some sort of motivation and subsequent result - classification is the obvious application.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54011,"question": "Seems like an interesting direction of research.  But I don't understand why does the author report exclusively training performance but not classification performance (on testing set).  The author just says that making it work well on classification tasks is subject of further research; but it should be trivial to measure and report the performance. Maybe it brutally overfits and the performance is abysmal, but learning that would still leave me with much better taste than not reporting it at all. If he already has the networks, I'd ask the author for this even as a reviewer of a workshop paper.","aSentId": 54020,"answer": "I think its important to avoid ranking the importance of new ML work on how well it classifies some data. By revealing this work at this stage others may be inspired to try something new. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54021,"question": "I think its important to avoid ranking the importance of new ML work on how well it classifies some data. By revealing this work at this stage others may be inspired to try something new. ","aSentId": 54022,"answer": "Of course. As I said, this is interesting research. (Though I didn't check / think hard about their math.)\n\nBut the point is not ranking their work by their performance. The point is, when writing a paper, thinking about the immediate questions readers would have, and equally importantly the data others will need to be sure they replicated the work successfuly, and trying to briefly cover these.\n\nAlso, okay, this is a nice starting point, but self-admitted base for further work in the exact direction in which they don't give any starting data. Others may propose some different scheme to do this, and they will want to compare to other work in the field. If they are a bit more practically minded, they will want to compare both training and classification performance.  Well, if they don't completely reproduce the scheme proposed here, they have nothing to compare to!  You yourself as an author aren't sure if you are improving the work of others, and that's also terribly annoying.\n\nNeed I go on about the reasons why I think this is an important omission?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54009,"question": "Random Walks... (training 1000-layer networks)","aSentId": 54024,"answer": "At least one review on CMT for ICLR2015 is pretty negative about it, questioning the mathematical justification.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54025,"question": "At least one review on CMT for ICLR2015 is pretty negative about it, questioning the mathematical justification.","aSentId": 54026,"answer": "I don't see it (0 reviews)","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54029,"question": "The Face Detection Algorithm Set To Revolutionise Image Search | MIT Technology Review","aSentId": 54030,"answer": "Haar Cascades aren't anywhere near the state of the art in terms of accuracy.  Their advantage is that they're extremely fast.  \n\nI wonder how they do face detection with convnets.  Do they randomly generate candidate patches?  Do they feed the whole image as an input for the network?  ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54031,"question": "Haar Cascades aren't anywhere near the state of the art in terms of accuracy.  Their advantage is that they're extremely fast.  \n\nI wonder how they do face detection with convnets.  Do they randomly generate candidate patches?  Do they feed the whole image as an input for the network?  ","aSentId": 54032,"answer": "They apparently used sliding window approach and convnet was fine tuned version of AlexNet....","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54029,"question": "The Face Detection Algorithm Set To Revolutionise Image Search | MIT Technology Review","aSentId": 54034,"answer": "The Face Detection Algorithm Set To Revolutionise Image Search","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54037,"question": "How Does NASA Use Machine Learning? - Forbes","aSentId": 54038,"answer": "A lot of ways! Here are some that I know about off the top of my head:","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54040,"question": "Applied Machine Learning - Reporting results in a publication","aSentId": 54041,"answer": "Using a simpler method like Chi^2 as /u/hellkin4u suggests is a good idea. Reviewers will ask why you didn't.\n\nIf the effect you're seeing only shows up in a ml model you can construct a permutation test of feature contribution by running the model many times with shuffled versions of one of the features (only shuffle one per run) and seeing how many times the shuffled version does better/worse then the unshuffled version for each. This will allow you to report a p-value of any machine learning model without needing to make any of the assumptions of a parametric model/specific distribution needed for most statistical tests. (Ie the p value is the chance a permutated feature does better or worse then the real feature depending on the null hypothesis)\n\nThe downside is that it requires a lot of computation to get a good p-value.\n\n\nAlso you need to be careful. Z not adding anything to X and Y's performance doesn't imply Z has no predictive value. It might just have redundant information with X and Y.\n\nEdit: also if you aren't familiar with p testing consult with a statistician if you can. It can be really tricky to get right when dealing with multiple features because of issues of multiple testing and 1 or 2 sided p-tests (which is right depends on when in the process you thought the features might be a good predictor and you really should have decided which to use before you did the experiment).\n\nYou can publish results without a p-value but a p-value of &lt;.05 is the easiest way to say something is statistically significant (which is why so many people conveniently ignore multiple testing etc) and just actually doing the analysis a bunch (hundreds if not thousands) with different permutations is the easiest/lowest assumption way to get one. It should be embarrassingly parallel so if you have access to a compute cluster use that.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54040,"question": "Applied Machine Learning - Reporting results in a publication","aSentId": 54043,"answer": "I prefer reporting the chi^2 values also\n\nPretty easy to do in sklearn\n\nhttp://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54046,"question": "A small list of highly technical reference pages.","aSentId": 54047,"answer": "This is an excellent list with lots of inspiring topics which are not as frequent here.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54049,"question": "Come learn about AI in the health space this Thursday at IDEO (Pier 28) at 7pm","aSentId": 54050,"answer": "Looks awesome! RSVP'd","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54051,"question": "Looks awesome! RSVP'd","aSentId": 54052,"answer": "Can't wait to see you there!","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54054,"question": "How to start learning natural language processing","aSentId": 54055,"answer": "There's another more specific subreddit, /r/languagetechnology that you might want to check out. /u/dustintran pointed out the Jurafsky and Manning course, and I am a big fan of the Jurafksy &amp; Martin book 'Speech and Language Processing'. ","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54056,"question": "There's another more specific subreddit, /r/languagetechnology that you might want to check out. /u/dustintran pointed out the Jurafsky and Manning course, and I am a big fan of the Jurafksy &amp; Martin book 'Speech and Language Processing'. ","aSentId": 54057,"answer": "Awesome! Thanks for point out that subreddit. I didn't know it existed.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54056,"question": "There's another more specific subreddit, /r/languagetechnology that you might want to check out. /u/dustintran pointed out the Jurafsky and Manning course, and I am a big fan of the Jurafksy &amp; Martin book 'Speech and Language Processing'. ","aSentId": 54059,"answer": "The book is awesome. Jurafsky states it's the leading texbook nowadays, and I believe him. I have long followed Manning's track, and it's very good also. But Jurafsky kind of compiles everything that's been said before on the subject (this includes Manning's Statsistical NLP and Intro to IR), adds speech, and bundles it in this tome.\n\nI plan to go through all the algorithms described there in my blog.","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54060,"question": "The book is awesome. Jurafsky states it's the leading texbook nowadays, and I believe him. I have long followed Manning's track, and it's very good also. But Jurafsky kind of compiles everything that's been said before on the subject (this includes Manning's Statsistical NLP and Intro to IR), adds speech, and bundles it in this tome.\n\nI plan to go through all the algorithms described there in my blog.","aSentId": 54061,"answer": "Could you provide the link to your blog?","corpus": "reddit"},{"docID": "t5_2r3gv","qSentId": 54062,"question": "Could you provide the link to your blog?","aSentId": 54063,"answer": "Of course: http://ai-maker.com/","corpus": "reddit"}]