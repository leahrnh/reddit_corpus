[{"docID": "t5_2r3gv", "qSentId": 41905, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 41906, "answer": "You again!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41905, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 41908, "answer": "All I can say is _wow_! This shall be interesting.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41905, "question": "Juergen Schmidhuber will be doing an AMA in /r/MachineLearning on March 4 10AM EST", "aSentId": 41910, "answer": "In case I don't make it: does the French guy get his bottle of exquisite French wine? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41917, "question": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "aSentId": 41918, "answer": "Since examples are generated and essentially unlimited, I wonder what happens if we train network only on examples it did not get? \n\nAt first model should learn simple tasks, and will see only harder ones. But the moment it start to forget something it would see examples from a given task again.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41920, "question": "My Attempt at Outperforming Deepmind's Atari Results - UPDATE 12", "aSentId": 41921, "answer": "What is this, some version of Conway's Game of Life? I don't quite understand.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41922, "question": "What is this, some version of Conway's Game of Life? I don't quite understand.", "aSentId": 41923, "answer": "It's an attempt at modeling the neocortex and using it for reinforcement learning. The reason it looks like that is the use of sparse distributed representations. If you have ever dealt with sparse autoencoders, it's basically that, but with local connections and lateral inhibition instead of a sparsity penalty.\n\nOnce a SDR is formed, one can efficiently predict which SDR will occur next by learning connections between SDR \"on\" bits. This ability to predict ahead of time is then used for action generation.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41928, "question": "Coursera Free course on Linear Algebra. Coding The Matrix: Linear Algebra Through Computer Science Applications by Philip Klein", "aSentId": 41929, "answer": "anyone care to weigh in on this course?  I'm looking to learn LA but I heard this course is more on its utilities than getting a grip of the subject.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41930, "question": "anyone care to weigh in on this course?  I'm looking to learn LA but I heard this course is more on its utilities than getting a grip of the subject.", "aSentId": 41931, "answer": "I believe the Gilbert Strang course is considered the best resource for learning LA. (MIT Open Courseware)\n\nBut I think your idea of the coursera course is a little off. The goal isn't jsut to learn LA utilities, it's to understand the concepts through applications.\n\nEither route you go, send me a message if you're looking for a study partner.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41932, "question": "I believe the Gilbert Strang course is considered the best resource for learning LA. (MIT Open Courseware)\n\nBut I think your idea of the coursera course is a little off. The goal isn't jsut to learn LA utilities, it's to understand the concepts through applications.\n\nEither route you go, send me a message if you're looking for a study partner.", "aSentId": 41933, "answer": "thanks, I'm leaning towards Strang because everything MIT I've seen has been outstanding.  But curiosity will likely get the best of me and I'll start both.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41932, "question": "I believe the Gilbert Strang course is considered the best resource for learning LA. (MIT Open Courseware)\n\nBut I think your idea of the coursera course is a little off. The goal isn't jsut to learn LA utilities, it's to understand the concepts through applications.\n\nEither route you go, send me a message if you're looking for a study partner.", "aSentId": 41935, "answer": "Thanks for the recommendation!  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41930, "question": "anyone care to weigh in on this course?  I'm looking to learn LA but I heard this course is more on its utilities than getting a grip of the subject.", "aSentId": 41937, "answer": "I'm doing this course at the moment. I've done some LA before at undergrad for simultaneous equations etc., but this is more rigorous as well as being really good to see how mathematical concepts map to programming implementations. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41930, "question": "anyone care to weigh in on this course?  I'm looking to learn LA but I heard this course is more on its utilities than getting a grip of the subject.", "aSentId": 41939, "answer": "I did this course a year and a half ago. I can imagine that for people with a higher level of mathematical maturity, this can be somewhat tedious. You basically code up all the matrix operations yourself; while that's certainly not a bad way to learn, if linear algebra happens to make sense to you, it's a lot of technical fiddling for no good reason. I found it useful and have retained a good chunk of what I learned (though naturally not everything). I am probably due for a linear algebra refresher come to think of it!", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41928, "question": "Coursera Free course on Linear Algebra. Coding The Matrix: Linear Algebra Through Computer Science Applications by Philip Klein", "aSentId": 41942, "answer": "The app store is telling me I can't download in my country. I'm in the USA, on android. Anyone else having an issue?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41944, "question": "[Discuss] The elephant in the room of machine learning research", "aSentId": 41945, "answer": "I largely agree with you, which is why I'm planning on leaving academia after my PhD. The publish or perish model pushes a lot these types of papers to print. I can't tell you how many papers (particularly deep learning papers) that state that the proposed method is state of the art, but the results of only one experiment trial are reported. This is wildly unscientific, but because there's so much scrutiny on ML in academia and because most people value the length of your CV rather than the quality, we're going to continue to be inundated with papers that are most likely just reporting random noise as innovation. I've seen this with my own experiments, I can beat state of the art by initializing my network weights with a particular seed. \n\nUnlike bio or physics, CS should be immune to this problem, because it's so easy to exactly reproduce experiments - all you need to do is ship your code and data to other researchers. But it seems researchers are more and more hesitant (again, particularly deep learning researchers) to release their code because Google is paying big money for the IP.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41946, "question": "I largely agree with you, which is why I'm planning on leaving academia after my PhD. The publish or perish model pushes a lot these types of papers to print. I can't tell you how many papers (particularly deep learning papers) that state that the proposed method is state of the art, but the results of only one experiment trial are reported. This is wildly unscientific, but because there's so much scrutiny on ML in academia and because most people value the length of your CV rather than the quality, we're going to continue to be inundated with papers that are most likely just reporting random noise as innovation. I've seen this with my own experiments, I can beat state of the art by initializing my network weights with a particular seed. \n\nUnlike bio or physics, CS should be immune to this problem, because it's so easy to exactly reproduce experiments - all you need to do is ship your code and data to other researchers. But it seems researchers are more and more hesitant (again, particularly deep learning researchers) to release their code because Google is paying big money for the IP.", "aSentId": 41947, "answer": "I left academia roughly 6-7 months ago, after many years of work in machine learning and control theory. If you think the publish or perish model is bad, wait till you write your first 5,000 proposals. One thinks touting the highly questionable superiority of your pet method over all others is bad until one starts writing about how your new framework will revolutionize the WHOLE field, and will cure cancer and enrich the local economies of struggling nation-states etc etc etc.\n\nI've found that applying machine learning methods to industrial problems is extremely interesting as well, and you really get to play with data that will completely *wreck* the so-called state of the art methods. In the real world, simple methods tend to work a lot better, due to the robustness properties associated with the restrictions imposed upon the function class: the fragility of the state-of-the-art becomes painfully apparent here.\n\nFurthermore, some jobs will allow you to keep publishing as well. I recently submitted a paper to ICML after years of disinterest and you know what? I couldn't care less if it gets accepted or not because I like the idea, and I know it will find a good home sooner or later. Disconnecting from the generic academic model allowed me to explore territory that was extremely different  from my normal fare, and I was grateful to have the opportunity.\n\nEDIT: grammar. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41948, "question": "I left academia roughly 6-7 months ago, after many years of work in machine learning and control theory. If you think the publish or perish model is bad, wait till you write your first 5,000 proposals. One thinks touting the highly questionable superiority of your pet method over all others is bad until one starts writing about how your new framework will revolutionize the WHOLE field, and will cure cancer and enrich the local economies of struggling nation-states etc etc etc.\n\nI've found that applying machine learning methods to industrial problems is extremely interesting as well, and you really get to play with data that will completely *wreck* the so-called state of the art methods. In the real world, simple methods tend to work a lot better, due to the robustness properties associated with the restrictions imposed upon the function class: the fragility of the state-of-the-art becomes painfully apparent here.\n\nFurthermore, some jobs will allow you to keep publishing as well. I recently submitted a paper to ICML after years of disinterest and you know what? I couldn't care less if it gets accepted or not because I like the idea, and I know it will find a good home sooner or later. Disconnecting from the generic academic model allowed me to explore territory that was extremely different  from my normal fare, and I was grateful to have the opportunity.\n\nEDIT: grammar. ", "aSentId": 41949, "answer": "&gt; If you think the publish or perish model is bad, wait till you write your first 5,000 proposals.\n\nI can only imagine. I don't think I could handle the pressure of running my own lab, given the current state of things.\n\n&gt;I've found that applying machine learning methods for industrial problems is extremely interesting as well, and you really get to play with data that will completely wreck the so-called state of the art methods. In the real world, simple methods tend to work a lot better\n\nI've only done a research internship or two, but I definitely agree. Implementing something like a deep model at a company requires enormous resources that only a few places have, and oftentimes complicated methods don't even beat logistic regression. I think working on problems with financial implications keeps you much more honest about what is actually going on with your models. In academia oftentimes you find data that works with your method, but in industry you have to find methods that work with your data, which is a completely different paradigm.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41950, "question": "&gt; If you think the publish or perish model is bad, wait till you write your first 5,000 proposals.\n\nI can only imagine. I don't think I could handle the pressure of running my own lab, given the current state of things.\n\n&gt;I've found that applying machine learning methods for industrial problems is extremely interesting as well, and you really get to play with data that will completely wreck the so-called state of the art methods. In the real world, simple methods tend to work a lot better\n\nI've only done a research internship or two, but I definitely agree. Implementing something like a deep model at a company requires enormous resources that only a few places have, and oftentimes complicated methods don't even beat logistic regression. I think working on problems with financial implications keeps you much more honest about what is actually going on with your models. In academia oftentimes you find data that works with your method, but in industry you have to find methods that work with your data, which is a completely different paradigm.", "aSentId": 41951, "answer": "&gt; I think working on problems with financial implications keeps you much more honest about what is actually going on with your models.\n\nCouldn't agree more. For the data I've looked at for example, the training process associated with stacked autoencoders is so brittle (as in you need a wealth of experience in coding and training them) that I've found it's just easier to go with an SVM. Even conventional wisdom, like the fact that using Gaussian RBFs in high dimensions is bad practice, fails, because it turns out that the RBFs provide the best model for the least investment in terms of training and optimization time. \n\nAnother thing you learn very quickly in industry is that more often than not, what matters is your set of features, not the model class you choose from. Features with very low SNR will result in garbage models no matter how sophisticated the method you employ, and ultimately, the goal is to make the model work for you, no matter what process you utilize to get there. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41950, "question": "&gt; If you think the publish or perish model is bad, wait till you write your first 5,000 proposals.\n\nI can only imagine. I don't think I could handle the pressure of running my own lab, given the current state of things.\n\n&gt;I've found that applying machine learning methods for industrial problems is extremely interesting as well, and you really get to play with data that will completely wreck the so-called state of the art methods. In the real world, simple methods tend to work a lot better\n\nI've only done a research internship or two, but I definitely agree. Implementing something like a deep model at a company requires enormous resources that only a few places have, and oftentimes complicated methods don't even beat logistic regression. I think working on problems with financial implications keeps you much more honest about what is actually going on with your models. In academia oftentimes you find data that works with your method, but in industry you have to find methods that work with your data, which is a completely different paradigm.", "aSentId": 41953, "answer": "&gt;  Implementing something like a deep model at a company requires enormous resources that only a few places have, and oftentimes complicated methods don't even beat logistic regression\n\nNow try convincing your VP to let you spend the time/money to play around with the new fangled method. They haven't even done a regression in 15 years and love the phrase \"big data\" and \"state of the art\".", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41946, "question": "I largely agree with you, which is why I'm planning on leaving academia after my PhD. The publish or perish model pushes a lot these types of papers to print. I can't tell you how many papers (particularly deep learning papers) that state that the proposed method is state of the art, but the results of only one experiment trial are reported. This is wildly unscientific, but because there's so much scrutiny on ML in academia and because most people value the length of your CV rather than the quality, we're going to continue to be inundated with papers that are most likely just reporting random noise as innovation. I've seen this with my own experiments, I can beat state of the art by initializing my network weights with a particular seed. \n\nUnlike bio or physics, CS should be immune to this problem, because it's so easy to exactly reproduce experiments - all you need to do is ship your code and data to other researchers. But it seems researchers are more and more hesitant (again, particularly deep learning researchers) to release their code because Google is paying big money for the IP.", "aSentId": 41955, "answer": "&gt; Unlike bio or physics, CS should be immune to this problem, because it's so easy to exactly reproduce experiments - all you need to do is ship your code and data to other researchers.\n\nAs a practitioner, this is one of the most frustrating aspects of reading ML papers for me. It's even worse when important details needed to replicate the results are unclear or missing altogether.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41956, "question": "&gt; Unlike bio or physics, CS should be immune to this problem, because it's so easy to exactly reproduce experiments - all you need to do is ship your code and data to other researchers.\n\nAs a practitioner, this is one of the most frustrating aspects of reading ML papers for me. It's even worse when important details needed to replicate the results are unclear or missing altogether.", "aSentId": 41957, "answer": "I know what you mean, but compared to say bio or chem the ML literature is incredibly refreshing.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41958, "question": "I know what you mean, but compared to say bio or chem the ML literature is incredibly refreshing.", "aSentId": 41959, "answer": "You're probably right, although I don't read much bio or chem literature. I guess what I mean is that it's harder than it needs to be. Like the parent to my comment pointed out, in CS there's no (technical) reason why reproducing a paper shouldn't be as easy as downloading the code, data and settings for a paper and running it. But I admit that maybe I'm just spoiled.\n\nEdit: I initially read your comment as *\"I don't know what you mean, compared to say bio or chem the ML literature is incredibly refreshing.*\"\n\nSo in addition to being spoiled, I guess I can't read either. :P", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41946, "question": "I largely agree with you, which is why I'm planning on leaving academia after my PhD. The publish or perish model pushes a lot these types of papers to print. I can't tell you how many papers (particularly deep learning papers) that state that the proposed method is state of the art, but the results of only one experiment trial are reported. This is wildly unscientific, but because there's so much scrutiny on ML in academia and because most people value the length of your CV rather than the quality, we're going to continue to be inundated with papers that are most likely just reporting random noise as innovation. I've seen this with my own experiments, I can beat state of the art by initializing my network weights with a particular seed. \n\nUnlike bio or physics, CS should be immune to this problem, because it's so easy to exactly reproduce experiments - all you need to do is ship your code and data to other researchers. But it seems researchers are more and more hesitant (again, particularly deep learning researchers) to release their code because Google is paying big money for the IP.", "aSentId": 41961, "answer": "I'm also leaving in about 6months (when PhD is all wrapped up). For me 'data science' will have to pay the bills. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41963, "question": "Sorry for the off-topic comment, but can I ask what your general plan is for when you graduate? I'm currently getting a PhD and am considering doing the same.", "aSentId": 41964, "answer": "I'm lucky to go to school in an area with a bunch of Very Large Tech Companies, so I'll probably end up working in one of their research departments. I've already done a research internship at one of them and it was a lot of fun. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41944, "question": "[Discuss] The elephant in the room of machine learning research", "aSentId": 41966, "answer": "It depends whether you think the only worthwhile reason for a paper to be accepted is that it blows away some benchmark. I wouldn't say that it is. I'd say things are worthy of publication if they suggest a very interesting or different way of doing things that at least doesn't do significantly worse. If it doesn't do better and just adds complexity, like a certain paper mentioned in other comments here, then it's of questionable value. Unfortunately evaluating qualitative contributions is an awful lot harder, and I'd hope to see quantitative analysis of the inner workings of the method in addition to benchmark results, especially when the benchmark results are weak. Of course, when a paper completely demolishes the competition on a useful benchmark then it's almost certainly worth understanding what's going on in it.\n\nRe: deep learning, I overheard an interesting conversation by several notable old-timers at NIPS, musing about the fact that this game of demanding benchmark wins was essentially brought to the machine learning community by the kernel machine crowd back in the 90s, and used to shut out methods that they didn't respect (there was a time not too long ago when having \"neural\" in your NIPS submission was anti-correlated with your chance of acceptance). Now, of course, they're being dragged over the coals and beaten at their own game; however, in my opinion, it's also led to a rather unhealthy situation where interesting directions don't get exposure at good conferences unless they can meet some standard that has more to do with the amount of compute time they can devote to hyperparameter tuning than anything else. arXiv helps here a little bit, but it's definitely worth re-examining the game being played.\n\nNow, actually *claiming* that you \"outperform\" some other method is a different story. There's an unhealthy culture that leads people to claim it when it isn't, statistically speaking, true, but that isn't an excuse. A pet peeve of mine is when people make a big deal of tiny improvements on MNIST.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41944, "question": "[Discuss] The elephant in the room of machine learning research", "aSentId": 41968, "answer": "I don't agree. Improvements are becoming \"marginal\" - and I'm assuming you're talking mainly about ImageNet top-5 here -  because we're getting close to the asymptote. Frankly, a 1% improvement over a 95% accuracy baseline on a benchmark as well-known as ImageNet is more impressive than a 10% improvement at 50%. \n\nEven then, the papers I've seen posted here recently that report such results have been PReLU and batch-normalization, both of which are extremely interesting from a conceptual POV and neither of which are anything to do with hyperparameters.\n\n&gt;  that OBVIOUSLY lie within the variance caused by the hyperparameters\n\nWhy this is obvious? And what exactly is the \"variance of the hyperparameters\"? It's a weird thing to say when most authors will conduct an extensive search over those hyperparams before publishing their paper.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41969, "question": "I don't agree. Improvements are becoming \"marginal\" - and I'm assuming you're talking mainly about ImageNet top-5 here -  because we're getting close to the asymptote. Frankly, a 1% improvement over a 95% accuracy baseline on a benchmark as well-known as ImageNet is more impressive than a 10% improvement at 50%. \n\nEven then, the papers I've seen posted here recently that report such results have been PReLU and batch-normalization, both of which are extremely interesting from a conceptual POV and neither of which are anything to do with hyperparameters.\n\n&gt;  that OBVIOUSLY lie within the variance caused by the hyperparameters\n\nWhy this is obvious? And what exactly is the \"variance of the hyperparameters\"? It's a weird thing to say when most authors will conduct an extensive search over those hyperparams before publishing their paper.", "aSentId": 41970, "answer": "Improvements are becoming marginal, but the problem is that we're seldom bothering with significance tests. and that we're sticking with the same old benchmarks for ages.\n\nMarginal improvements would be fine, if you can show that they're significant. But within the Deep Learning community this done almost never.  Oftentimes it is even very obvious that people report a handpicked \"best random initialization parameter\" result in their paper. Or think back on DropConnect, where after being questioned, the author had to admit to the ICML-audience that, well, actually, to be fully honest, no, he was not able to really convincingly show that he's significantly better than Dropout at all. I know other examples just like this, and I'm sorry to be singling out the DC paper ^1. I know much worse examples. But for some reason, we as a community often are okay with this happening, and I don't think we should be. If you're going to write a paper about a method, and you state that it's better, it *should* be convincingly better. Yet -- especially in the DL realm -- people often don't show significant improvements, they just that under some random condition and with one very lucky random initialization of the PRNG they are a bit better than SOTA.\n\nOn the other extreme, we do sometimes stick with outdated benchmarks for way too long. For years and years now people have bothered with reporting \"new best results on MNIST*   (*well at least on permutation-invariant MNIST without dataset augmentation and no ensembles and only comparing to other methods whose name has an odd number of letters)\".... which is just silly. Having a few benchmarks is nice, but always relying on the same, old (and frankly: solved) datasets is not useful.\n\n\n^1  I still think the paper shows well enough that DC performs as well as or potentially better than Dropout. It's just that the statement about *significantly* outperforming Dropout (which I think the paper implicitly tries to make) doesn't hold up too well.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41971, "question": "Improvements are becoming marginal, but the problem is that we're seldom bothering with significance tests. and that we're sticking with the same old benchmarks for ages.\n\nMarginal improvements would be fine, if you can show that they're significant. But within the Deep Learning community this done almost never.  Oftentimes it is even very obvious that people report a handpicked \"best random initialization parameter\" result in their paper. Or think back on DropConnect, where after being questioned, the author had to admit to the ICML-audience that, well, actually, to be fully honest, no, he was not able to really convincingly show that he's significantly better than Dropout at all. I know other examples just like this, and I'm sorry to be singling out the DC paper ^1. I know much worse examples. But for some reason, we as a community often are okay with this happening, and I don't think we should be. If you're going to write a paper about a method, and you state that it's better, it *should* be convincingly better. Yet -- especially in the DL realm -- people often don't show significant improvements, they just that under some random condition and with one very lucky random initialization of the PRNG they are a bit better than SOTA.\n\nOn the other extreme, we do sometimes stick with outdated benchmarks for way too long. For years and years now people have bothered with reporting \"new best results on MNIST*   (*well at least on permutation-invariant MNIST without dataset augmentation and no ensembles and only comparing to other methods whose name has an odd number of letters)\".... which is just silly. Having a few benchmarks is nice, but always relying on the same, old (and frankly: solved) datasets is not useful.\n\n\n^1  I still think the paper shows well enough that DC performs as well as or potentially better than Dropout. It's just that the statement about *significantly* outperforming Dropout (which I think the paper implicitly tries to make) doesn't hold up too well.", "aSentId": 41972, "answer": "Interesting! Didn't know this about DropConnect. I'm assuming that the paper didn't get retracted or anything of that sort? ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41973, "question": "Interesting! Didn't know this about DropConnect. I'm assuming that the paper didn't get retracted or anything of that sort? ", "aSentId": 41974, "answer": "I've overblown it a bit. A retraction certainly isn't necessary. Dropconnect still performs as well as or better as Dropout. It was just the numbers in the paper didn't always show that clearly (if you looked closely enough).\n\nI wasn't there myself, I was just told the story (I am also sure someone told it here on the subreddit, but the search function turns up nothing). Apparently someone called him out on the fact that the confidence intervals between dropout/dropconnect do often overlap, and that he mostly focuses on ensemble performance. The author then somewhat admitted that he wasn't able to show a clearer difference.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41975, "question": "I've overblown it a bit. A retraction certainly isn't necessary. Dropconnect still performs as well as or better as Dropout. It was just the numbers in the paper didn't always show that clearly (if you looked closely enough).\n\nI wasn't there myself, I was just told the story (I am also sure someone told it here on the subreddit, but the search function turns up nothing). Apparently someone called him out on the fact that the confidence intervals between dropout/dropconnect do often overlap, and that he mostly focuses on ensemble performance. The author then somewhat admitted that he wasn't able to show a clearer difference.\n", "aSentId": 41976, "answer": "If I understand the approach correctly, DC, unlike DO, requires a draw from the Bermoulli distribution for each floating point multiplication. This must be pretty expensive. I doubt that doing this is justifiable unless the improvement is substantial.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41977, "question": "If I understand the approach correctly, DC, unlike DO, requires a draw from the Bermoulli distribution for each floating point multiplication. This must be pretty expensive. I doubt that doing this is justifiable unless the improvement is substantial.", "aSentId": 41978, "answer": "Implementing DC is much more complex than DO. The problematic thing is that there is a whole dropout-mask (of same size as the weight-matrix) *for each training sample* that you will need to temporarily store. So you can't really do (mini)-batch-learning.  IIRC the paper has a whole section dedicated on the tricks needed to implement this somewhat performantly, but it was hell of complex, and I think in the end you're still stuck with doing matrix-vector multiplications mostly (i.e., it's hard to take advantage of fast matrix-multiplies on gpus).\n\nIn contrast, with most activation functions (sigmoids, relus) you don't even need additional storage for DO, and minibatches are an absolute no-brainer.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41971, "question": "Improvements are becoming marginal, but the problem is that we're seldom bothering with significance tests. and that we're sticking with the same old benchmarks for ages.\n\nMarginal improvements would be fine, if you can show that they're significant. But within the Deep Learning community this done almost never.  Oftentimes it is even very obvious that people report a handpicked \"best random initialization parameter\" result in their paper. Or think back on DropConnect, where after being questioned, the author had to admit to the ICML-audience that, well, actually, to be fully honest, no, he was not able to really convincingly show that he's significantly better than Dropout at all. I know other examples just like this, and I'm sorry to be singling out the DC paper ^1. I know much worse examples. But for some reason, we as a community often are okay with this happening, and I don't think we should be. If you're going to write a paper about a method, and you state that it's better, it *should* be convincingly better. Yet -- especially in the DL realm -- people often don't show significant improvements, they just that under some random condition and with one very lucky random initialization of the PRNG they are a bit better than SOTA.\n\nOn the other extreme, we do sometimes stick with outdated benchmarks for way too long. For years and years now people have bothered with reporting \"new best results on MNIST*   (*well at least on permutation-invariant MNIST without dataset augmentation and no ensembles and only comparing to other methods whose name has an odd number of letters)\".... which is just silly. Having a few benchmarks is nice, but always relying on the same, old (and frankly: solved) datasets is not useful.\n\n\n^1  I still think the paper shows well enough that DC performs as well as or potentially better than Dropout. It's just that the statement about *significantly* outperforming Dropout (which I think the paper implicitly tries to make) doesn't hold up too well.", "aSentId": 41980, "answer": "I'm still new in the field.  Can you clarify what you mean here?\n\n&gt; ... only comparing to other methods whose name has an odd number of letters)\"....\n\nI feel like the FNG who doesn't get all the in-jokes.\n\n&gt; Having a few benchmarks is nice, but always relying on the same, old (and frankly: solved) datasets is not useful.\n\nAre there benchmarks and measures you would particularly recommend?  I suspect they would have to be task-specific.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41981, "question": "I'm still new in the field.  Can you clarify what you mean here?\n\n&gt; ... only comparing to other methods whose name has an odd number of letters)\"....\n\nI feel like the FNG who doesn't get all the in-jokes.\n\n&gt; Having a few benchmarks is nice, but always relying on the same, old (and frankly: solved) datasets is not useful.\n\nAre there benchmarks and measures you would particularly recommend?  I suspect they would have to be task-specific.\n", "aSentId": 41982, "answer": "The in-joke here was that MNIST has been over-used so much, you can't realistically list all published results in your work. You will have to choose which ones you take in and which one's you don't. This is often done arbitrarily (\"I'm going to compare against method X, Y and Z, but not W or V, and I'm fact I'm not even gonna mention that W or V exist\"), and most often, you pick and choose who to compare to, in order to look favorable.\n\n&gt; Are there benchmarks and measures you would particularly recommend? \n\nMeasures: sure, always use the \"standard\" measure (which will most often be accuracy or AUC, depending on task) and be vary when someone uses something non-standard without a good reason (though good reasons do sometimes exist). As for benchmarks, it is **really** tricky. Deep Learning folks love MNIST, CIFAR and ImageNet, and maybe TIMIT and NORB. And they're all nice datasets, and I like to use them myself. But at the same time, they're boring and been used to death (okay, ImageNet is still somewhat fresh). But I wouldn't know what else to recommend, either.  Just make sure you use a lot of them, and maybe even come up with a few on your own or something. (Sorry, I know this is very generic advice. It depends on what exactly your work is about etc... but in general: compare on multiple datasets and try your best to be rigorous)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41983, "question": "The in-joke here was that MNIST has been over-used so much, you can't realistically list all published results in your work. You will have to choose which ones you take in and which one's you don't. This is often done arbitrarily (\"I'm going to compare against method X, Y and Z, but not W or V, and I'm fact I'm not even gonna mention that W or V exist\"), and most often, you pick and choose who to compare to, in order to look favorable.\n\n&gt; Are there benchmarks and measures you would particularly recommend? \n\nMeasures: sure, always use the \"standard\" measure (which will most often be accuracy or AUC, depending on task) and be vary when someone uses something non-standard without a good reason (though good reasons do sometimes exist). As for benchmarks, it is **really** tricky. Deep Learning folks love MNIST, CIFAR and ImageNet, and maybe TIMIT and NORB. And they're all nice datasets, and I like to use them myself. But at the same time, they're boring and been used to death (okay, ImageNet is still somewhat fresh). But I wouldn't know what else to recommend, either.  Just make sure you use a lot of them, and maybe even come up with a few on your own or something. (Sorry, I know this is very generic advice. It depends on what exactly your work is about etc... but in general: compare on multiple datasets and try your best to be rigorous)", "aSentId": 41984, "answer": "&gt; You will have to choose which ones you take in and which one's you don't. This is often done arbitrarily (\"I'm going to compare against method X, Y and Z, but not W or V, and I'm fact I'm not even gonna mention that W or V exist\"), and most often, you pick and choose who to compare to, in order to look favorable\n\nThank you!  It would seem to me that to follow good experimental design, the benchmarks and measures should be established ahead of time, including the cut-offs are for what will be substantial improvement.  I imagine that gets harder to do without fore-knowledge on these established data sets. \n\nI really do appreciate your answer.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41983, "question": "The in-joke here was that MNIST has been over-used so much, you can't realistically list all published results in your work. You will have to choose which ones you take in and which one's you don't. This is often done arbitrarily (\"I'm going to compare against method X, Y and Z, but not W or V, and I'm fact I'm not even gonna mention that W or V exist\"), and most often, you pick and choose who to compare to, in order to look favorable.\n\n&gt; Are there benchmarks and measures you would particularly recommend? \n\nMeasures: sure, always use the \"standard\" measure (which will most often be accuracy or AUC, depending on task) and be vary when someone uses something non-standard without a good reason (though good reasons do sometimes exist). As for benchmarks, it is **really** tricky. Deep Learning folks love MNIST, CIFAR and ImageNet, and maybe TIMIT and NORB. And they're all nice datasets, and I like to use them myself. But at the same time, they're boring and been used to death (okay, ImageNet is still somewhat fresh). But I wouldn't know what else to recommend, either.  Just make sure you use a lot of them, and maybe even come up with a few on your own or something. (Sorry, I know this is very generic advice. It depends on what exactly your work is about etc... but in general: compare on multiple datasets and try your best to be rigorous)", "aSentId": 41986, "answer": "The reason that's done is because MNIST is image data. Methods that take advantage of that, like convolutions or distorting the training set, can improve results quite a bit. So it's not fair to compare those results to more general methods.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41989, "question": "I don't totally agree. I see many interesting papers which results can be verified very quickly and which dont seem like cheating. Like this one http://arxiv.org/pdf/1502.01852v1.pdf. It presents 2 ideas: new weight initialization and parametric ReLU. Both are easy to implement (Theano with auto diff was a blessing here!) and are indeed useful in my experiments. You as a practitioner must filter this kind of papers from the others.", "aSentId": 41990, "answer": "Yea getting PRelu up in theano was painless. Have you done [batch normalization](http://arxiv.org/abs/1502.03167) yet? I've been super impressed with it so far. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41991, "question": "Yea getting PRelu up in theano was painless. Have you done [batch normalization](http://arxiv.org/abs/1502.03167) yet? I've been super impressed with it so far. ", "aSentId": 41992, "answer": "Did you get it to work with theano? Could you share? [My attempt](http://pastebin.com/ZcmkvmHK) using torch fails when I try it on MNIST, but it is very likely due to my guess about how they calculated the input variance/expectation and/or bugs on gradient calculation (it got really ugly trying to avoid allocations).", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41993, "question": "Did you get it to work with theano? Could you share? [My attempt](http://pastebin.com/ZcmkvmHK) using torch fails when I try it on MNIST, but it is very likely due to my guess about how they calculated the input variance/expectation and/or bugs on gradient calculation (it got really ugly trying to avoid allocations).", "aSentId": 41994, "answer": "I have gotten it working in theano, both for a fully connected and a convolutional net, and have seen some significant speedup when testing on CIFAR100. I don't have any code hosted that shows it, but it is pretty straightforward. Just a heads up, they propose a scale/shift step after you normalize with extra learned parameters, I haven't added that since I use non-saturating activations.\n\n[This](http://deeplearning.net/tutorial/code/mlp.py) is the code for a theano MLP, you can make the following modification to the hidden layer's lin_output to do batch normalization.\n\n\n    lin_output = T.dot(input, self.W) + self.b\n    #Linear output of a fully-connected layer\n    batch_mean = T.mean(lin_output, axis=0)\n    batch_var = T.var(lin_output, axis=0)\n    norm_output = (lin_output - batch_mean)/T.sqrt(batch_var + 1e-7)\n    #Norm output now passed to nonlinearity. \n\nDuring test time I use a moving window of the batch means/vars, in theano this is tracked in a shared variable. \n\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41995, "question": "I have gotten it working in theano, both for a fully connected and a convolutional net, and have seen some significant speedup when testing on CIFAR100. I don't have any code hosted that shows it, but it is pretty straightforward. Just a heads up, they propose a scale/shift step after you normalize with extra learned parameters, I haven't added that since I use non-saturating activations.\n\n[This](http://deeplearning.net/tutorial/code/mlp.py) is the code for a theano MLP, you can make the following modification to the hidden layer's lin_output to do batch normalization.\n\n\n    lin_output = T.dot(input, self.W) + self.b\n    #Linear output of a fully-connected layer\n    batch_mean = T.mean(lin_output, axis=0)\n    batch_var = T.var(lin_output, axis=0)\n    norm_output = (lin_output - batch_mean)/T.sqrt(batch_var + 1e-7)\n    #Norm output now passed to nonlinearity. \n\nDuring test time I use a moving window of the batch means/vars, in theano this is tracked in a shared variable. \n\n\n", "aSentId": 41996, "answer": "Thanks. Very useful. Shouldn't you freeze the means and vars at test time so they are deterministic?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41997, "question": "Thanks. Very useful. Shouldn't you freeze the means and vars at test time so they are deterministic?", "aSentId": 41998, "answer": "Yea so I track the mean/var with two shared variables, updated like:\n\n    mean_upd = window_mean * .95 + (1 -.95) * batch_mean\n    var_upd = window_var * .95 + (1 - .95) * batch_var\n\nand then I use an iflese statement to select between a normalization produced with the batch stats or the window stats. Neither the testing or validation function applies updates to any shared variables, so I take the window normalized output.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41995, "question": "I have gotten it working in theano, both for a fully connected and a convolutional net, and have seen some significant speedup when testing on CIFAR100. I don't have any code hosted that shows it, but it is pretty straightforward. Just a heads up, they propose a scale/shift step after you normalize with extra learned parameters, I haven't added that since I use non-saturating activations.\n\n[This](http://deeplearning.net/tutorial/code/mlp.py) is the code for a theano MLP, you can make the following modification to the hidden layer's lin_output to do batch normalization.\n\n\n    lin_output = T.dot(input, self.W) + self.b\n    #Linear output of a fully-connected layer\n    batch_mean = T.mean(lin_output, axis=0)\n    batch_var = T.var(lin_output, axis=0)\n    norm_output = (lin_output - batch_mean)/T.sqrt(batch_var + 1e-7)\n    #Norm output now passed to nonlinearity. \n\nDuring test time I use a moving window of the batch means/vars, in theano this is tracked in a shared variable. \n\n\n", "aSentId": 42000, "answer": "TY. Am I correct to understand that you use a moving average for E[x] and Var[x] during training and then you freeze them for prediction on test/validation batches? I think I'll also try to drop the scaling factors for now if it works without them.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42001, "question": "TY. Am I correct to understand that you use a moving average for E[x] and Var[x] during training and then you freeze them for prediction on test/validation batches? I think I'll also try to drop the scaling factors for now if it works without them.", "aSentId": 42002, "answer": "Yes that is correct. From the paper it sounded like they added the scaling factors to help deal with sigmoid units, I plan to add them in at some point but so far I've had improved performance on ReLU units even without them. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41991, "question": "Yea getting PRelu up in theano was painless. Have you done [batch normalization](http://arxiv.org/abs/1502.03167) yet? I've been super impressed with it so far. ", "aSentId": 42004, "answer": "I will try. Thanks for the tip.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 41989, "question": "I don't totally agree. I see many interesting papers which results can be verified very quickly and which dont seem like cheating. Like this one http://arxiv.org/pdf/1502.01852v1.pdf. It presents 2 ideas: new weight initialization and parametric ReLU. Both are easy to implement (Theano with auto diff was a blessing here!) and are indeed useful in my experiments. You as a practitioner must filter this kind of papers from the others.", "aSentId": 42006, "answer": "&gt; You as a practitioner must filter this kind of papers from the others.\n\nAgree. I like to read the Arxiv (stat.ml) each day to see the new papers posted, and it's often quite clear when authors are trying to stretch the performance of an algorithm. It's often quite easy as well, whenever there is manual hyperparameter tuning of more than one method there's likely some bias. Science should be objective, but when you come up with a neat new algorithm you *really* want it to do well, so you end up trying to find scenarios where it does. As a result, it's actually pretty easy to show that one method beats another if that is your goal. \n\nA proper experimental methodology is a good solution to this problem. Hyperparameters should be selected automatically, you should use several datasets of varying characteristics, etc. Another good test of an algorithm is to give it to others (who are unbiased) to test out. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42010, "question": "Here are some other interesting papers that discuss this:\n\n[Hand's \"Classifier Technology and the Illusion of Progress\"](http://projecteuclid.org/download/pdfview_1/euclid.ss/1149600839)\n\nand\n\n[Wagstaff's \"Machine Learning that Matters\"](http://arxiv.org/ftp/arxiv/papers/1206/1206.4656.pdf)", "aSentId": 42011, "answer": "Hand's paper is an all-time classic.  Hand himself is great at explaining which parts of extremely subtle topics actually matter in practice.  Always worth noting.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42010, "question": "Here are some other interesting papers that discuss this:\n\n[Hand's \"Classifier Technology and the Illusion of Progress\"](http://projecteuclid.org/download/pdfview_1/euclid.ss/1149600839)\n\nand\n\n[Wagstaff's \"Machine Learning that Matters\"](http://arxiv.org/ftp/arxiv/papers/1206/1206.4656.pdf)", "aSentId": 42013, "answer": "I actually worked on a paper a long time ago that showed how practically this is a huge problem for the small datasets that academia uses [here.](http://www3.nd.edu/~dial/papers/ICDM10.pdf) This whole topic is why I think academic research is really not particularly useful practically until otherwise demonstrated.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42014, "question": "I actually worked on a paper a long time ago that showed how practically this is a huge problem for the small datasets that academia uses [here.](http://www3.nd.edu/~dial/papers/ICDM10.pdf) This whole topic is why I think academic research is really not particularly useful practically until otherwise demonstrated.", "aSentId": 42015, "answer": "Thank you for the link! I'm currently doing my PhD on evaluation metrics for Data Mining so this looks perfect.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42016, "question": "Thank you for the link! I'm currently doing my PhD on evaluation metrics for Data Mining so this looks perfect.", "aSentId": 42017, "answer": "No problem, hope it helps :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42020, "question": "Undergrad intending to start a masters degree in statistics next year.  Time to say \"we told you so.\"", "aSentId": 42021, "answer": "Why is that?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42023, "question": "It only goes to show that DNNs have pretty much reached the top of their performance. What is needed are true breakthroughs in machine learning. How about an unsupervised learning model that does more than just recognize chairs but actually understands what a chair is? Now, that would be impressive.", "aSentId": 42024, "answer": "Mind being more specific? What kind of approach would that even be?\n\nIt's easy to dismiss something as bad, and restate the objective. It's much more difficult to make a claim regarding what kind of steps to take to reach the (difficult) objective.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42025, "question": "Mind being more specific? What kind of approach would that even be?\n\nIt's easy to dismiss something as bad, and restate the objective. It's much more difficult to make a claim regarding what kind of steps to take to reach the (difficult) objective.", "aSentId": 42026, "answer": "Well, in my opinion, it will require at least 3 paradigm shifts. The first is to realize that intelligence is all about timing, the only supervisor needed for effective learning. Timing is the common glue that links all knowledge together. The second shift is to reject the mainstream notion that events in the world are inherently probabilistic and that the job of an intelligent system is to compute the probabilities. The third paradigm shift is to accept the opposite, that events in the world are perfectly consistent and that the job of the intelligent system is to discover this perfection.\n\nObviously, Reddit is not the proper forum to explain all this (it gets involved). But do keep your ears and eyes open. This field is progressing much faster than many suppose.\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42027, "question": "Well, in my opinion, it will require at least 3 paradigm shifts. The first is to realize that intelligence is all about timing, the only supervisor needed for effective learning. Timing is the common glue that links all knowledge together. The second shift is to reject the mainstream notion that events in the world are inherently probabilistic and that the job of an intelligent system is to compute the probabilities. The third paradigm shift is to accept the opposite, that events in the world are perfectly consistent and that the job of the intelligent system is to discover this perfection.\n\nObviously, Reddit is not the proper forum to explain all this (it gets involved). But do keep your ears and eyes open. This field is progressing much faster than many suppose.\n", "aSentId": 42028, "answer": "Your comment is absurdly vague for someone claiming such detailed knowledge of major breakthroughs. I'll base this just on what you said.\n\nTiming is just a constraint of a task, not a model. Saying you need timing is not any more useful than saying \"you need data\" or \"you need memory\".\n\nThe world definitely is uncertain and probabalistic. If we could perfectly predict everything this would all be pointless.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42029, "question": "Your comment is absurdly vague for someone claiming such detailed knowledge of major breakthroughs. I'll base this just on what you said.\n\nTiming is just a constraint of a task, not a model. Saying you need timing is not any more useful than saying \"you need data\" or \"you need memory\".\n\nThe world definitely is uncertain and probabalistic. If we could perfectly predict everything this would all be pointless.", "aSentId": 42030, "answer": "First off, why put words in my mouth? Why the hostility? I said that timing was the supervisor for perceptual learning. I did not say that it was a model. Sensors emit precisely timed pulses and the temporal relationships between the pulses can be learned by the cortex: pulses are either concurrent or sequential. This tells us that memory represents all knowledge in terms of patterns and sequences.\n\nThere certainly are events in the world that are probabilistic but the brain cannot process them. If the brain processed probabilities directly, we would not need the field of statistics. By and large, the world is deterministic. For example, when you walk in the kitchen, you find the sink and the fridge exactly where you left them. As you walk, objects in your field of vision move exactly and precisely as they should. We are surprised when things are not exactly the way we expect them to be. Probability has nothing to do with it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42031, "question": "First off, why put words in my mouth? Why the hostility? I said that timing was the supervisor for perceptual learning. I did not say that it was a model. Sensors emit precisely timed pulses and the temporal relationships between the pulses can be learned by the cortex: pulses are either concurrent or sequential. This tells us that memory represents all knowledge in terms of patterns and sequences.\n\nThere certainly are events in the world that are probabilistic but the brain cannot process them. If the brain processed probabilities directly, we would not need the field of statistics. By and large, the world is deterministic. For example, when you walk in the kitchen, you find the sink and the fridge exactly where you left them. As you walk, objects in your field of vision move exactly and precisely as they should. We are surprised when things are not exactly the way we expect them to be. Probability has nothing to do with it.", "aSentId": 42032, "answer": "Time is just another dimension of input. And no one is claiming we won't need recurrency.\n\nThere is uncertainty in everything. When you go into the kitchen you don't know exactly where the mayonnaise is in the fridge. You can't possibly predict where every item is or everything that is going to happen. Your vision certainly can't predict every pixel and every object you see as you move.\n\n", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42033, "question": "Time is just another dimension of input. And no one is claiming we won't need recurrency.\n\nThere is uncertainty in everything. When you go into the kitchen you don't know exactly where the mayonnaise is in the fridge. You can't possibly predict where every item is or everything that is going to happen. Your vision certainly can't predict every pixel and every object you see as you move.\n\n", "aSentId": 42034, "answer": "Your visual cortex certainly notices when things do not behave as expected and creates a new scenario that explains the anomaly. But hey, to each his own. I'll be seeing you around.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42039, "question": "RMSProp and equilibrated adaptive learning rates for non-convex optimization", "aSentId": 42040, "answer": "I have a general basic question about second-order methods for neural networks. Popular activation functions nowadays, such as ReLU, are not even once differentiable. Their second derivative, where it exists, is always zero. A smooth activation function like softplus might seem friendlier, and near the origin it is, but far from the origin, its second derivative is far too close to zero to provide appropriate scaling information (because sigmoid(x) / (sigmoid(x) * (1-sigmoid(x))) = 1 + e^x ).\n\nWhy don't the bad second derivatives of activation functions cause problems for second-order methods? I get that there are other second-order effects going on in the whole network because weights from successive layers multiply together, and possibly also from the loss function. Is that all that's happening?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42041, "question": "I have a general basic question about second-order methods for neural networks. Popular activation functions nowadays, such as ReLU, are not even once differentiable. Their second derivative, where it exists, is always zero. A smooth activation function like softplus might seem friendlier, and near the origin it is, but far from the origin, its second derivative is far too close to zero to provide appropriate scaling information (because sigmoid(x) / (sigmoid(x) * (1-sigmoid(x))) = 1 + e^x ).\n\nWhy don't the bad second derivatives of activation functions cause problems for second-order methods? I get that there are other second-order effects going on in the whole network because weights from successive layers multiply together, and possibly also from the loss function. Is that all that's happening?", "aSentId": 42042, "answer": "I think there is also some confusion about what a second order method is.\n\nI think the purest definition is that a second order method uses the curvature information. There are two schools of it.\n\nOne is that of quasi newton methods. One prominent example is LBFGS. All of them are approximations to Newton's method by having an explicit approximation to the Hessian.\n\nThe other is truncated newton, or Hessian free. The latter name is misleading, as the Hessian is used but it is never explicitly stored (or an approximation to it). Instead, efficient ways to calculate the dot product of the Hessian with a vector are used. Nonlinear conjugate gradient falls into that category.\n\n\nTo the best of my knowledge, the adaptive optimisers proposed in the last years (adagrad, rmsprop, adadelta, radagrad, adam, no more pesky learning rates (?), ...) are *not* second order methods in the classical sense. Instead, they use quantities available during the learning process to adapt learning rates. To what extent these are related to curvature I don't know, but they do not explicity claim to do this.\n\nThat being said, the Hessian has (imho) best been summarised in the ICML2010 paper \"a fast natural newton method\":\n\n&gt; Remember what the Hessian is: a measure of the change in gradient when we move in parameter space. In other words, the Hessian helps to answer the question: if I were at a slightly different position in parameter space, how different would the gradient be? It is a quantity defined for any (twice differentiable) function.\n\nNow, relus make the net not twice differentiable. However, you can easily see that approximations to the Hessian that carry sensible information can exist, because slight changes in parameters can still change the gradient.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42043, "question": "I think there is also some confusion about what a second order method is.\n\nI think the purest definition is that a second order method uses the curvature information. There are two schools of it.\n\nOne is that of quasi newton methods. One prominent example is LBFGS. All of them are approximations to Newton's method by having an explicit approximation to the Hessian.\n\nThe other is truncated newton, or Hessian free. The latter name is misleading, as the Hessian is used but it is never explicitly stored (or an approximation to it). Instead, efficient ways to calculate the dot product of the Hessian with a vector are used. Nonlinear conjugate gradient falls into that category.\n\n\nTo the best of my knowledge, the adaptive optimisers proposed in the last years (adagrad, rmsprop, adadelta, radagrad, adam, no more pesky learning rates (?), ...) are *not* second order methods in the classical sense. Instead, they use quantities available during the learning process to adapt learning rates. To what extent these are related to curvature I don't know, but they do not explicity claim to do this.\n\nThat being said, the Hessian has (imho) best been summarised in the ICML2010 paper \"a fast natural newton method\":\n\n&gt; Remember what the Hessian is: a measure of the change in gradient when we move in parameter space. In other words, the Hessian helps to answer the question: if I were at a slightly different position in parameter space, how different would the gradient be? It is a quantity defined for any (twice differentiable) function.\n\nNow, relus make the net not twice differentiable. However, you can easily see that approximations to the Hessian that carry sensible information can exist, because slight changes in parameters can still change the gradient.", "aSentId": 42044, "answer": "&gt;To what extent these are related to curvature I don't know, but they do not explicity claim to do this.\n\nThis paper makes that link explicit. It connects Rmsprop to a diagonal approximation of the absolute Hessian.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42045, "question": "&gt;To what extent these are related to curvature I don't know, but they do not explicity claim to do this.\n\nThis paper makes that link explicit. It connects Rmsprop to a diagonal approximation of the absolute Hessian.", "aSentId": 42046, "answer": "Thanks! :) Maybe I should have read it beforehand. :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42041, "question": "I have a general basic question about second-order methods for neural networks. Popular activation functions nowadays, such as ReLU, are not even once differentiable. Their second derivative, where it exists, is always zero. A smooth activation function like softplus might seem friendlier, and near the origin it is, but far from the origin, its second derivative is far too close to zero to provide appropriate scaling information (because sigmoid(x) / (sigmoid(x) * (1-sigmoid(x))) = 1 + e^x ).\n\nWhy don't the bad second derivatives of activation functions cause problems for second-order methods? I get that there are other second-order effects going on in the whole network because weights from successive layers multiply together, and possibly also from the loss function. Is that all that's happening?", "aSentId": 42048, "answer": "&gt;Why don't the bad second derivatives of activation functions \n&gt;cause problems for second-order methods? \n\nTLDR: they approximate the 2nd order components over multiple facets, not any facet in particular.\n\nThe discontinuous derivatives will cause \"faceting\" (piecewise-continuous) of the objective function.\n\nlike you point out, the non-linear components of the objective could give the objective function a non-zero Hessian on any particular facet.\n\nBut many of these approximate second order methods rely on evaluating the objective function multiple times with different parameters. \n\nWhat matters is that the gradient changes, not that it's continuous.\n\nSo even if the objective function is completely  piecewise-linear (faceted) these sort of methods are like a Taylor series approximating the points on the facets they hit, not any facet in particular:\n\nEach edge of the faceting is caused by a single data-point's error contribution crossing a single non-differentiable boundary. It'll be rare that you update the model parameters without having a single data-point cross a single non-differentiable boundary.\n\nAnd that's without any noise contributed by SGD.\n\n(this is mostly just a rephrasing of the last paragraph from /u/sieisteinmodel )", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42050, "question": "Word2Vec used to summarize content at medium.com", "aSentId": 42051, "answer": "Details for the lazy: http://genopharmix.com/biomimetic-cognition/in_silico_cognitive_biomimicry.html", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42057, "question": "Deeplearning4j - Open-source, distributed deep learning for the JVM", "aSentId": 42058, "answer": "The examples work fine, but I wouldn't consider this a solid base for researching neural network algorithms. The builder APIs can easily break or do the wrong thing. There's no means of handling trivial configuration changes, like doing regression instead of classification. I can't monitor the performance of a model on held-out dataset as a function of iterations.\n\nThe underlying array library, Nd4j, is promising though, once you get past the awkward semantics of column-major arrays, slicing and indexing.\n\nThe development model also moves from repl -&gt; tracing debugger. Perhaps there are design decisions that would make this work better.\n\nOverall this is subpar compared to libraries offered on top of torch and theano, which are developed and used by LeCunn and Bengio. Torchnn, Pylearn2 and Lasagne are probably better options for most people. If you need to integrate them with Java code, you can do so with an API.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42059, "question": "The examples work fine, but I wouldn't consider this a solid base for researching neural network algorithms. The builder APIs can easily break or do the wrong thing. There's no means of handling trivial configuration changes, like doing regression instead of classification. I can't monitor the performance of a model on held-out dataset as a function of iterations.\n\nThe underlying array library, Nd4j, is promising though, once you get past the awkward semantics of column-major arrays, slicing and indexing.\n\nThe development model also moves from repl -&gt; tracing debugger. Perhaps there are design decisions that would make this work better.\n\nOverall this is subpar compared to libraries offered on top of torch and theano, which are developed and used by LeCunn and Bengio. Torchnn, Pylearn2 and Lasagne are probably better options for most people. If you need to integrate them with Java code, you can do so with an API.", "aSentId": 42060, "answer": "Thanks a lot for this comment and for the indications. I'll take a look to the libraries you suggested.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42057, "question": "Deeplearning4j - Open-source, distributed deep learning for the JVM", "aSentId": 42062, "answer": "I don't think any deep learning experts use this library and the documentation doesn't seem to indicate that the people developing this know a lot about this technology. That said, I haven't tried to use it personally so I don't know if it is any good.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42057, "question": "Deeplearning4j - Open-source, distributed deep learning for the JVM", "aSentId": 42064, "answer": "I have a lot to say about DeepLearning4j. I've been in the field on and off for a few years.  It's certainly significantly easier to set up on Windows than Theano or Caffe, but I felt the structure of a project was very stinted.  When I wanted to do some things with NLP, I probably rewrote my vectorizing code several times in several places because it wasn't clear of I should subclass the vectorize, add a data set fetcher, or put it elsewhere.  Documentation, thinking back, was a bit sparse.  Building models was particularly error prone and tricky.  DefaultModelFactory, I think it was, would cause problems if you made a convolutional network layer. (You have to make a convolutional factory or the weights end up null.)\n\nIt certainly has potential, but there's a lot of work to be done.  I'll try to write more when I'm on a real computer.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42066, "question": "Scalable Bayesian Optimization Using Deep Neural Networks", "aSentId": 42067, "answer": "I like these hyperparameter optimization papers mainly because it exposes something endemic to machine learning research. The obsession with what I call the **'marginally state-of-the-art'**. It's become particularly bad with deep learning because of all the hyperparameters available to tune.\n\nAs a practitioner, this is extremely frustrating. Papers pushing complicated augmentations to standard methods keep using the word 'outperform' for results that OBVIOUSLY lie within the variance caused by the hyperparameters. This is both dishonest and a disservice to the larger machine learning community. **And it's getting worse if you look at the neural network papers submitted to NIPS, ICML, ICLR.** If you look at the reviews of ICLR, at best this issue is being completely ignored and at worst this sort of misleading progress is encouraged. \n\nDo no misunderstand what I say, I believe classification performance and other measures are extremely important, but not when the increase is so marginal. Researchers should be simplifying their methods and getting **competitive** performance. This is where real progress happens.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42068, "question": "I like these hyperparameter optimization papers mainly because it exposes something endemic to machine learning research. The obsession with what I call the **'marginally state-of-the-art'**. It's become particularly bad with deep learning because of all the hyperparameters available to tune.\n\nAs a practitioner, this is extremely frustrating. Papers pushing complicated augmentations to standard methods keep using the word 'outperform' for results that OBVIOUSLY lie within the variance caused by the hyperparameters. This is both dishonest and a disservice to the larger machine learning community. **And it's getting worse if you look at the neural network papers submitted to NIPS, ICML, ICLR.** If you look at the reviews of ICLR, at best this issue is being completely ignored and at worst this sort of misleading progress is encouraged. \n\nDo no misunderstand what I say, I believe classification performance and other measures are extremely important, but not when the increase is so marginal. Researchers should be simplifying their methods and getting **competitive** performance. This is where real progress happens.", "aSentId": 42069, "answer": "I agree. After a while one just wants a simple, reliable method producing good results, not another flavour-of-the-week SoTA crap.\n\nThat said, the problem the paper claims to solve is dealing with many (order of thousands) function evaluations in hyperparam optimization. It's a problem relatively few people have, anyway.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42070, "question": "I agree. After a while one just wants a simple, reliable method producing good results, not another flavour-of-the-week SoTA crap.\n\nThat said, the problem the paper claims to solve is dealing with many (order of thousands) function evaluations in hyperparam optimization. It's a problem relatively few people have, anyway.", "aSentId": 42071, "answer": "This was confusing for me. If your objective function is so expensive, how are you evaluating it so damn much that you run into the cubic scaling problem? These guys must be dealing simultaneously with massive problems and massive resources.\n\nIf the idea is just to replace the GP with some ANN (which in some cases is asymptotically equivalent to a GP, but scales better if you do it right) that seems like a dull-but-effective idea. As they mention in the conclusion, one could similarly try a sparse GP, or whatever else people do to deal with cubic scaling. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42068, "question": "I like these hyperparameter optimization papers mainly because it exposes something endemic to machine learning research. The obsession with what I call the **'marginally state-of-the-art'**. It's become particularly bad with deep learning because of all the hyperparameters available to tune.\n\nAs a practitioner, this is extremely frustrating. Papers pushing complicated augmentations to standard methods keep using the word 'outperform' for results that OBVIOUSLY lie within the variance caused by the hyperparameters. This is both dishonest and a disservice to the larger machine learning community. **And it's getting worse if you look at the neural network papers submitted to NIPS, ICML, ICLR.** If you look at the reviews of ICLR, at best this issue is being completely ignored and at worst this sort of misleading progress is encouraged. \n\nDo no misunderstand what I say, I believe classification performance and other measures are extremely important, but not when the increase is so marginal. Researchers should be simplifying their methods and getting **competitive** performance. This is where real progress happens.", "aSentId": 42073, "answer": "It's interesting that you mentioned this issue while commenting on this paper, since the experimental results seem quite unconvincing. On both CIFAR-10 and CIFAR-100, they use\n\n- more data augmentation techniques than others (How much gain in performance is due to these? If they don't affect much, why were they used?)\n- bigger/deeper networks (How much gain in performance is due to these?)\n- a different and more complex strategy at test time: \"averaging\nits log-probability predictions on 100 samples drawn from the input corruption distribution, with masks drawn from the unit dropout distribution\"\n\nThe results do not isolate the effect of the proposed approach, which should be more important that showing better results than everyone.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42066, "question": "Scalable Bayesian Optimization Using Deep Neural Networks", "aSentId": 42075, "answer": "New state of the art on CIFAR10/100 and image label generation. Reiterates how important hyperparameter optimization can be.\n\nThey did this back in 2012 with [Practical Bayesian Optimization of Machine Learning Algorithms](http://arxiv.org/abs/1206.2944) and this time the results are even more impressive. The only worry is the seeming requirement of a large cluster to do this in an efficent manner.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42076, "question": "New state of the art on CIFAR10/100 and image label generation. Reiterates how important hyperparameter optimization can be.\n\nThey did this back in 2012 with [Practical Bayesian Optimization of Machine Learning Algorithms](http://arxiv.org/abs/1206.2944) and this time the results are even more impressive. The only worry is the seeming requirement of a large cluster to do this in an efficent manner.", "aSentId": 42077, "answer": "how is this state of the art? Benjamin Graham's [spatially sparse cnn](http://arxiv.org/abs/1409.6070) does slightly better on both CIFAR10/100.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42078, "question": "how is this state of the art? Benjamin Graham's [spatially sparse cnn](http://arxiv.org/abs/1409.6070) does slightly better on both CIFAR10/100.", "aSentId": 42079, "answer": "Graham's work has been largely ignored by the broader research community. \n\nI don't know why, it may simply be ignorance, for instance, this paper doesn't list \"all conv\" results which are a bit better than deeply supervised results. This has happened before with several not well known MNIST papers all claiming state of the art on permutation invariant in the 0.8-0.9% range and usually none of them cite any of the others.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42080, "question": "Graham's work has been largely ignored by the broader research community. \n\nI don't know why, it may simply be ignorance, for instance, this paper doesn't list \"all conv\" results which are a bit better than deeply supervised results. This has happened before with several not well known MNIST papers all claiming state of the art on permutation invariant in the 0.8-0.9% range and usually none of them cite any of the others.", "aSentId": 42081, "answer": "that's too bad, I think his stuff is neat and I like that he's released working code.\n\nIt seems really strange to claim SOTA when the kaggle CIFAR10 competition had half a dozen people with better scores.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42082, "question": "that's too bad, I think his stuff is neat and I like that he's released working code.\n\nIt seems really strange to claim SOTA when the kaggle CIFAR10 competition had half a dozen people with better scores.", "aSentId": 42083, "answer": "I think one of the reasons could be that it's still not clear (not adequately explained in his paper) why his results are so good. Is it because he's using much larger networks, more data augmentation, or a different test time strategy? Additionally, his technique appears to be well motivated for small images (and so is appropriate for offline handwriting), but what about more realistic image sizes? These issues will (hopefully) be ironed out before the paper appears in a peer-reviewed conference/journal.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42080, "question": "Graham's work has been largely ignored by the broader research community. \n\nI don't know why, it may simply be ignorance, for instance, this paper doesn't list \"all conv\" results which are a bit better than deeply supervised results. This has happened before with several not well known MNIST papers all claiming state of the art on permutation invariant in the 0.8-0.9% range and usually none of them cite any of the others.", "aSentId": 42085, "answer": "Do you have links to those mnist papers?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42080, "question": "Graham's work has been largely ignored by the broader research community. \n\nI don't know why, it may simply be ignorance, for instance, this paper doesn't list \"all conv\" results which are a bit better than deeply supervised results. This has happened before with several not well known MNIST papers all claiming state of the art on permutation invariant in the 0.8-0.9% range and usually none of them cite any of the others.", "aSentId": 42087, "answer": "I'll quickly hijack this comment.\nIndeed somehow Graham's work went unnoticed for a while (I for one only heard / read of it after we submitted the All-CNN paper). \nIt really is a shame that the community tends to not do a good job of correctly citing the SOTA. On the other hand so many papers are currently published in parallel that it is sometimes hard to keep track.\nHopefully this problem will dissolve once progress settles down a bit. On the note of SOTA results, we did run a few more experiments using networks closer to Graham's work for the All-CNN paper and will update the results there in the coming week (for those interested).\nIt is definitely true that the results from the paper OP linked are not really state of the art anymore, especially considering the fact that they use unknown quantities of data augmentation and do not correctly account for their influence. \n\nEDIT: Language ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42088, "question": "I'll quickly hijack this comment.\nIndeed somehow Graham's work went unnoticed for a while (I for one only heard / read of it after we submitted the All-CNN paper). \nIt really is a shame that the community tends to not do a good job of correctly citing the SOTA. On the other hand so many papers are currently published in parallel that it is sometimes hard to keep track.\nHopefully this problem will dissolve once progress settles down a bit. On the note of SOTA results, we did run a few more experiments using networks closer to Graham's work for the All-CNN paper and will update the results there in the coming week (for those interested).\nIt is definitely true that the results from the paper OP linked are not really state of the art anymore, especially considering the fact that they use unknown quantities of data augmentation and do not correctly account for their influence. \n\nEDIT: Language ", "aSentId": 42089, "answer": "Yes, it's surprising that this paper does such a bad job with comparative results. There are hardly any takeaways from the experimental results section.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42066, "question": "Scalable Bayesian Optimization Using Deep Neural Networks", "aSentId": 42092, "answer": "**Question:**\n\nTheir *meta-model* has 3 hidden layers, with 50 units each, so it must have over 5000 weights. So how do they train so many weights in the beginning, when there are few observations (especially if they don't use DropOut for regularization, and their weight decay is modest, as they say) ?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42093, "question": "**Question:**\n\nTheir *meta-model* has 3 hidden layers, with 50 units each, so it must have over 5000 weights. So how do they train so many weights in the beginning, when there are few observations (especially if they don't use DropOut for regularization, and their weight decay is modest, as they say) ?", "aSentId": 42094, "answer": "You forgot that they use Bayesian linear regression as a top layer. Its predictive distribution is pretty broad for few samples.\n\nProbably they do not even have to tune the net for the first 10 samples. :)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42095, "question": "You forgot that they use Bayesian linear regression as a top layer. Its predictive distribution is pretty broad for few samples.\n\nProbably they do not even have to tune the net for the first 10 samples. :)", "aSentId": 42096, "answer": "It would probably make sense to train a simpler model while there are few samples, or maybe use random weights, but as I understand it, they train the same NN in the same way, regardless of the number of samples.\n\nI don't have a good intuition about how quickly the overtraining should disappear vs how quickly the distribution should get narrower. I wish the paper addressed this somehow.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42097, "question": "It would probably make sense to train a simpler model while there are few samples, or maybe use random weights, but as I understand it, they train the same NN in the same way, regardless of the number of samples.\n\nI don't have a good intuition about how quickly the overtraining should disappear vs how quickly the distribution should get narrower. I wish the paper addressed this somehow.", "aSentId": 42098, "answer": "Yes, totaly with you there. It would be nice if one could judge how good this approach does for few samples, and if we lose a lot if we chose this for experiments with &lt; 100 trials. \n\nThat could make a pretty cool plot, actually.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42103, "question": "Machine Learning Software Design", "aSentId": 42104, "answer": "I really like the syntax of scikit-learn. I would just suggest reading the source code and rewriting the parts that are most interesting to you.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42106, "question": "CN24: complete semantic segmentation framework using fully convolutional networks, supports OpenCL, MKL, AMCL...", "aSentId": 42107, "answer": "openCL support is pretty cool. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42110, "question": "Navigating the Machine Learning job market", "aSentId": 42111, "answer": "In my experience, most companies with some \"data\" want a \"data scientist\" that specializes in ML, they just dont know why.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42112, "question": "In my experience, most companies with some \"data\" want a \"data scientist\" that specializes in ML, they just dont know why.", "aSentId": 42113, "answer": "a long time ago, i was headhunted by Taco Bell for one such position. They wanted to model their data in visual basic and excel. \n\nI've never seen anything like that before that or since then. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42114, "question": "a long time ago, i was headhunted by Taco Bell for one such position. They wanted to model their data in visual basic and excel. \n\nI've never seen anything like that before that or since then. ", "aSentId": 42115, "answer": "Excel is used by a lot of finance people to do financial models, and if you want to script Excel, you're probably doing it in VBA. (That said there's a firm providing the ability to script excel with python - which is just brilliant, not that I'd encourage the use of excel.)", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42116, "question": "Excel is used by a lot of finance people to do financial models, and if you want to script Excel, you're probably doing it in VBA. (That said there's a firm providing the ability to script excel with python - which is just brilliant, not that I'd encourage the use of excel.)", "aSentId": 42117, "answer": "i get that. kind of surprised i have never seen that kind of stuff in this line of work at all except that one time.... and since that kind of a position requires skills very different from someone with my profile, i was surprised they actually tried headhunting me. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42116, "question": "Excel is used by a lot of finance people to do financial models, and if you want to script Excel, you're probably doing it in VBA. (That said there's a firm providing the ability to script excel with python - which is just brilliant, not that I'd encourage the use of excel.)", "aSentId": 42119, "answer": "Because of people writing even games in excel:\n\nhttp://mashable.com/2014/08/27/microsoft-excel-games/\n\ndoen't mean this is an adequate plarform for doing it (I don't see Valve switching to Excel anytime soon...).\n\nThat said, I support Python for this, and I'm lately moving to Lua.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42114, "question": "a long time ago, i was headhunted by Taco Bell for one such position. They wanted to model their data in visual basic and excel. \n\nI've never seen anything like that before that or since then. ", "aSentId": 42121, "answer": "Our lead data scientist does a *lot* of exploratory work in Excel. It's weird. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42114, "question": "a long time ago, i was headhunted by Taco Bell for one such position. They wanted to model their data in visual basic and excel. \n\nI've never seen anything like that before that or since then. ", "aSentId": 42123, "answer": "I worked for everyone's favorite electronics retailer for a while, and they wanted me to do forecasting and predictive modeling for them.  The tools made available.....\n\nExcel and Access.\n\nDidn't take me long to get a SQL certification to prove I wouldn't break their SQL systems, and setup an R instance on my laptop.  ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42112, "question": "In my experience, most companies with some \"data\" want a \"data scientist\" that specializes in ML, they just dont know why.", "aSentId": 42125, "answer": "In that case, a major skill is not actually ML skill, but how to convince people who don't know ML that you know ML.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42110, "question": "Navigating the Machine Learning job market", "aSentId": 42127, "answer": "I guess the tl;dr of the article is if you want a serious ML job (actual research and not just implementing other people's work) do a PhD and hopefully get into a top lab", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42128, "question": "I guess the tl;dr of the article is if you want a serious ML job (actual research and not just implementing other people's work) do a PhD and hopefully get into a top lab", "aSentId": 42129, "answer": "(or start a business)\n\nActually it's a shame that entrepreneurship is not discussed more as an option. It's just not taught well enough anywhere I guess. Machine learning, automation etc. is probably *the* hottest field to start a business in right now.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42130, "question": "(or start a business)\n\nActually it's a shame that entrepreneurship is not discussed more as an option. It's just not taught well enough anywhere I guess. Machine learning, automation etc. is probably *the* hottest field to start a business in right now.", "aSentId": 42131, "answer": "I don't know; I think there is an over-saturation to be honest. Lots of pixie dust \"data science\" companies and questionable Deep Learning outfits.  \n\nSearching for the problem itself is always the first issue. Big companies already have the capital to implement and improve core tech, finding the *use case* is more interesting.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42132, "question": "I don't know; I think there is an over-saturation to be honest. Lots of pixie dust \"data science\" companies and questionable Deep Learning outfits.  \n\nSearching for the problem itself is always the first issue. Big companies already have the capital to implement and improve core tech, finding the *use case* is more interesting.", "aSentId": 42133, "answer": "I'd agree a lot with this. There is a huge overabundance of companies whose sole business plan is to get acquired, and even more that don't seem to have any concept of a business plan beyond \"get investment money\". ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42134, "question": "I'd agree a lot with this. There is a huge overabundance of companies whose sole business plan is to get acquired, and even more that don't seem to have any concept of a business plan beyond \"get investment money\". ", "aSentId": 42135, "answer": "*cough* Clarafai *cough*", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42132, "question": "I don't know; I think there is an over-saturation to be honest. Lots of pixie dust \"data science\" companies and questionable Deep Learning outfits.  \n\nSearching for the problem itself is always the first issue. Big companies already have the capital to implement and improve core tech, finding the *use case* is more interesting.", "aSentId": 42137, "answer": "I think the biggest problem with startups in the Data Science/ML world right now, is a lot are trying to be everything for everyone.  The key is finding a specific problem, and addressing it.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42130, "question": "(or start a business)\n\nActually it's a shame that entrepreneurship is not discussed more as an option. It's just not taught well enough anywhere I guess. Machine learning, automation etc. is probably *the* hottest field to start a business in right now.", "aSentId": 42139, "answer": "Any examples of such fields that can greatly benefit from ML for entrepreneurship?", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42128, "question": "I guess the tl;dr of the article is if you want a serious ML job (actual research and not just implementing other people's work) do a PhD and hopefully get into a top lab", "aSentId": 42141, "answer": "With ML people as scarce as they are, I've found it's overwhelmingly about what you've done. The PhD is a good idea because it gives you several years to construct a body of work that can convince people you know what you're doing. Doing the same from a professional position is harder but not impossible. The main barrier is that most of what you do will be NDA'd.", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42142, "question": "With ML people as scarce as they are, I've found it's overwhelmingly about what you've done. The PhD is a good idea because it gives you several years to construct a body of work that can convince people you know what you're doing. Doing the same from a professional position is harder but not impossible. The main barrier is that most of what you do will be NDA'd.", "aSentId": 42143, "answer": "Agreed.  \n\nA masters is really helpful to build up the skillset, but then once you're out, small companies are often looking for seasoned pros, and big companies are scooping up PhDs, or just looking for software engineers.  \n\nI dropped out of PhD due to hating it etc, but I got really lucky and have been doing ML R&amp;D in a small company, hopefully building up my resume enough for a real track-record by the time I move on. ", "corpus": "reddit"},{"docID": "t5_2r3gv", "qSentId": 42128, "question": "I guess the tl;dr of the article is if you want a serious ML job (actual research and not just implementing other people's work) do a PhD and hopefully get into a top lab", "aSentId": 42145, "answer": "i wonder if those sorts of research jobs exist outside of university environments. Microsoft Research and IBM Research used to be places like that, but now they are firing people and/or restructuring labs to be more integrated with engineering. \n\nEven within universities, i increasingly find problems to be engineering-driven more than research-driven. i'd like to know if that is the case or if it's just my tunnel vision. ", "corpus": "reddit"}]